<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mysql on Classic</title>
    <link>/categories/mysql/</link>
    <description>Recent content in mysql on Classic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 25 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="/categories/mysql/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MySQL常用脚本_审计检查</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E5%AE%A1%E8%AE%A1%E6%A3%80%E6%9F%A5/</link>
      <pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E5%AE%A1%E8%AE%A1%E6%A3%80%E6%9F%A5/</guid>
      <description>最近做等保，反复的有评测机构联系要做MySQL审计，会扔过来一些脚本，要看执行结果。
  可以写一个这样的脚本，将他们要求执行的语句扔进去。
  节约点时间
  vim run_mysql_queries.sh
  #!/bin/bash # --- 配置 --- # 默认数据库名，如果需要也可以作为参数传入 DB_NAME=&amp;quot;dbtest01&amp;quot; # MySQL 客户端命令，根据你的实际情况修改,如果用密码登录，也在这里改 MYSQL_CMD=&amp;quot;mysql --login-path=dba&amp;quot; # ---------------- # 检查是否传入了 IP 地址和端口 if [ -z &amp;quot;$1&amp;quot; ] || [ -z &amp;quot;$2&amp;quot; ]; then echo &amp;quot;用法: $0 &amp;lt;IP地址&amp;gt; &amp;lt;端口&amp;gt;&amp;quot; echo &amp;quot;例如: $0 10.10.0.0.1 3306&amp;quot; exit 1 fi TARGET_IP=&amp;quot;$1&amp;quot; TARGET_PORT=&amp;quot;$2&amp;quot; # 根据 IP 和端口动态生成日志文件名 LOG_FILE=&amp;quot;output_${TARGET_IP}_${TARGET_PORT}.log&amp;quot; # 初始化日志文件 echo &amp;quot;当前时间是: $(date)&amp;quot; &amp;gt; &amp;quot;$LOG_FILE&amp;quot; echo &amp;quot;执行实例：${TARGET_IP}:${TARGET_PORT}&amp;quot; &amp;gt;&amp;gt; &amp;quot;$LOG_FILE&amp;quot; echo &amp;quot;&amp;quot; &amp;gt;&amp;gt; &amp;quot;$LOG_FILE&amp;quot; # 添加一个空行，使格式更美观 # 定义执行 SQL 并记录日志的函数 # 参数1: SQL 查询语句 execute_sql_and_log() { local sql_query=&amp;quot;$1&amp;quot; echo &amp;quot;&amp;quot; &amp;gt;&amp;gt; &amp;quot;$LOG_FILE&amp;quot; echo &amp;quot;&amp;quot; &amp;gt;&amp;gt; &amp;quot;$LOG_FILE&amp;quot; echo &amp;quot;执行SQL：&amp;quot; &amp;gt;&amp;gt; &amp;quot;$LOG_FILE&amp;quot; echo &amp;quot;$sql_query&amp;quot; &amp;gt;&amp;gt; &amp;quot;$LOG_FILE&amp;quot; echo &amp;quot;-------------------------------------------------------------------------------------&amp;quot; &amp;gt;&amp;gt; &amp;quot;$LOG_FILE&amp;quot; # 执行 mysql 命令并将输出追加到日志文件 # 注意：这里假设 mysqlw 客户端连接成功不会输出到 stderr，错误会输出到 stderr # 如果需要更健壮的错误处理，可以检查 $?</description>
    </item>
    
    <item>
      <title>做了一个去O的工具：异构数据验证对比</title>
      <link>/oracle/%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%8E%BBo%E7%9A%84%E5%B7%A5%E5%85%B7%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E8%A1%8C%E7%BA%A7%E9%AA%8C%E8%AF%81%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>/oracle/%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%8E%BBo%E7%9A%84%E5%B7%A5%E5%85%B7%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E8%A1%8C%E7%BA%A7%E9%AA%8C%E8%AF%81%E5%AF%B9%E6%AF%94/</guid>
      <description>工具有什么用  当我们用一些第三方同步工具同步数据时，同步过程中可能会出现：数据不一致 这时候需要一些数据对比，来验证数据同步是否按预期进行 很早以前我写过类似的功能。但是一直没有做异构数据同步支持 异构数据是指源数据库和目标数据库不是同一种数据库 例如，从Oralce实时同步数据到MySQL 这是一个常见的异构数据同步场景 如何来验证和修复这个数据同步 于是我开发了这个工具，用来解决这个问题  支持异构数据源对比  支持MySQL&amp;lt;&amp;mdash;-&amp;gt;MySQL 支持Oracle&amp;lt;&amp;mdash;-&amp;gt;MySQL 支持Oracle&amp;lt;&amp;mdash;-&amp;gt;Oracle 支持MySQL&amp;lt;&amp;mdash;-&amp;gt;Oracle  双向数据对比  能比较出源库有没有的数据(生成Detelte语句) 能比较出源库没有，但是目标库有的数据(生成Detelte语句) 能比较出主键相同，但是其他列有差异的数据(生成Update语句)  双向生成修复SQL  可以生成目标库的redo SQL 也可以生成源库的Undo SQL  可配置的时间精度对比  对不同数据的不同时间精度都用同一维度对比(默认精确到分钟:YYYY-MM-DD HH:MM) 对不同精度的小数格式化支持(会去掉0.6000后面的000 )  列默认值支持  通常用于一边是Null，一边是Not Null的默认值  白名单支持  支持对列级别的白名单（这一列不参与对比） 支持对值级别的白名单 (包含)  支持表结构变形后的对比  支持源表和目标表表结构不同（要求主键是唯一的，其他变形在sql可控范围内）  对比速度  可按表级别并发执行，单表对比速度约：1万-3万行/秒  </description>
    </item>
    
    <item>
      <title>MySQL多实例部署报错：io_setup failed with EAGAIN</title>
      <link>/mysql/mysql%E5%A4%9A%E5%AE%9E%E4%BE%8B%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99io_setup-failed-with-eagain/</link>
      <pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%A4%9A%E5%AE%9E%E4%BE%8B%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99io_setup-failed-with-eagain/</guid>
      <description>多实例部署时遇到io_setup() failed with EAGAIN报错  2024-10-30T14:17:28.568852+08:00 0 [System] [MY-013169] [Server] /usr/local/mysql8/bin/mysqld (mysqld 8.0.22) initializing of server in progress as process 171586 2024-10-30T14:17:28.587654+08:00 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started. 2024-10-30T14:17:28.593140+08:00 1 [Warning] [MY-012582] [InnoDB] io_setup() failed with EAGAIN. Will make 5 attempts before giving up. 2024-10-30T14:17:28.593225+08:00 1 [Warning] [MY-012583] [InnoDB] io_setup() attempt 1. 2024-10-30T14:17:29.094632+08:00 1 [Warning] [MY-012583] [InnoDB] io_setup() attempt 2. 2024-10-30T14:17:29.595951+08:00 1 [Warning] [MY-012583] [InnoDB] io_setup() attempt 3.</description>
    </item>
    
    <item>
      <title>MySQL为了适应大规模bi拉取数据的参数调整</title>
      <link>/mysql/mysql%E4%B8%BA%E4%BA%86%E9%80%82%E5%BA%94%E5%A4%A7%E8%A7%84%E6%A8%A1bi%E6%8B%89%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/</link>
      <pubDate>Sat, 12 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E4%B8%BA%E4%BA%86%E9%80%82%E5%BA%94%E5%A4%A7%E8%A7%84%E6%A8%A1bi%E6%8B%89%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/</guid>
      <description>线上bi实例，专门用于bi拉数或开发定位问题用的，需要调整以下参数
  设置连接空闲的超时时间（秒）。可以通过增加这些参数的值来延长连接的存活时间，以便超长查询有充足的时间完成。  SET GLOBAL wait_timeout = 28800; -- 28800秒（8小时） SET GLOBAL interactive_timeout = 28800;  用于控制服务器等待客户端发送数据的时间（秒），对于较大的导出操作，增大这两个参数可以防止数据传输中断。  SET GLOBAL net_read_timeout = 600; -- 增大到10分钟，适合大查询 SET GLOBAL net_write_timeout = 600;  控制单个查询的最大执行时间（毫秒），  SET GLOBAL max_execution_time = 0; -- 设置为0，表示不限制查询时间  控制排序操作时使用的内存大小,默认值较小。对于导出大数据的查询，将其适当增大（如16M或32M）可以减少磁盘排序操作，提升查询效率。  SET GLOBAL sort_buffer_size = 167772160; -- 160M  控制全表扫描时每次读取的数据量，适合大查询时进行适度调高，通常可以设置为2M到16M。  SET GLOBAL read_buffer_size = 16777216;  控制InnoDB引擎在等待行锁的最大时间（秒）。对于长查询，适当增大可以避免锁等待超时错误。  SET GLOBAL innodb_lock_wait_timeout = 300;  控制最大通信数据包大小。对于导出大数据，可以增加此值避免错误。  SET GLOBAL max_allowed_packet = 1073741824; -脚本</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(五):SlowLog</title>
      <link>/mysql/mysql%E7%9A%84slowlog%E6%97%A5%E5%BF%97/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84slowlog%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续：
 MySQL的7种日志(一):概况 MySQL的7种日志(二):RedoLog MySQL的7种日志(三):UndoLog MySQL的7种日志(四):BinLog MySQL的7种日志(五):SlowLog  1.什么是Slowlog  数据库执行一个SQL时，如果超过了设定值(比如说500毫秒),数据库将此SQL和相关信息记录到日志中，这个日志就是SLowlog，我们也称为慢日志 slowLog的开启，是为了定位和发现慢SQL用的。这一点跟前几篇文章里的redbolog,undolog,binlog等日志不一样  SlowLog的超时时间long_query_time  这个需要特别注意，并不是我们通常理解的一个SQL从开始执行到执行完用了多长时间 事实上MySQL判断一个sql是否要被记到slowlog中，是这样的逻辑： 假设我们设置了超过500毫秒的SQL是慢SQl要记下来，MySQL会这样处理 实际SQL执行消耗的时间- 锁等待消耗时间 如果这个时间&amp;gt;=500毫秒，则记下SlowLog否则不记 这就相当于说开车起点到终点的时间如果超过30分钟就很慢了 但我们说的30分钟不包括路上堵车和等红绿灯的时间  # @long_query_time ：我们设置了慢日志记录时间 # sqltime ：mysql判断一个sql的执行用时 # cur_utime ：这条sql从开始执行到结束，实际消耗的时间 # utime_after_lock：锁等待消耗时间 sqltime = cur_utim- utime_after_lock if sqltime&amp;gt;=@long_query_time: recordIt() #写入慢日志 else: pass 2.MySQL慢日志的常用操作 开启  修改my.cnf  [mysqld] slow_query_log = 1 slow_query_log_file = /data/mysql3306/mysql-slow.log #指定了慢查询日志的输出文件路径； long_query_time = 1 # 超过多长时间（秒）的SQL 被记录 修改  慢日志的几个项都可以在线联机修改的  set global long_query_time=0.</description>
    </item>
    
    <item>
      <title>MySQL创建远程链接服务器LinkServer步骤</title>
      <link>/mysql/mysql%E5%88%9B%E5%BB%BA%E8%BF%9C%E7%A8%8B%E9%93%BE%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8linkserver%E6%AD%A5%E9%AA%A4/</link>
      <pubDate>Thu, 05 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%88%9B%E5%BB%BA%E8%BF%9C%E7%A8%8B%E9%93%BE%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8linkserver%E6%AD%A5%E9%AA%A4/</guid>
      <description>环境准备  执行show engines;   mysql&amp;gt; show engines; +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | Engine | Support | Comment | Transactions | XA | Savepoints | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL | | MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO | | InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES | | PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO | | MyISAM | YES | MyISAM storage engine | NO | NO | NO | | MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO | | BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO | | CSV | YES | CSV storage engine | NO | NO | NO | | ARCHIVE | YES | Archive storage engine | NO | NO | NO | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ 9 rows in set (0.</description>
    </item>
    
    <item>
      <title>MySQL9.0在centos7上安装部署DBA版</title>
      <link>/mysql/mysql9.0%E5%9C%A8centos7%E4%B8%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2dba%E7%89%88/</link>
      <pubDate>Thu, 29 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql9.0%E5%9C%A8centos7%E4%B8%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2dba%E7%89%88/</guid>
      <description>方法一：RPM安装方式 下载： wget https://dev.mysql.com/get/Downloads/MySQL-9.0/mysql-9.0.1-1.el7.x86_64.rpm-bundle.tar tar -xvf mysql-9.0.1-1.el7.x86_64.rpm-bundle.tar ll -rw-r--r-- 1 root root 1082173440 Jul 14 03:03 mysql-9.0.1-1.el7.x86_64.rpm-bundle.tar -rw-r--r-- 1 7155 31415 15319752 Jul 14 02:55 mysql-community-client-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 3628356 Jul 14 02:55 mysql-community-client-plugins-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 709720 Jul 14 02:55 mysql-community-common-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 595117664 Jul 14 02:55 mysql-community-debuginfo-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 2023960 Jul 14 02:55 mysql-community-devel-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 4219028 Jul 14 02:55 mysql-community-embedded-compat-9.</description>
    </item>
    
    <item>
      <title>MySQL8.0即时在线加字段instant-add-column</title>
      <link>/mysql/mysql8.0%E5%8D%B3%E6%97%B6%E5%9C%A8%E7%BA%BF%E5%8A%A0%E5%AD%97%E6%AE%B5instant-add-column/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql8.0%E5%8D%B3%E6%97%B6%E5%9C%A8%E7%BA%BF%E5%8A%A0%E5%AD%97%E6%AE%B5instant-add-column/</guid>
      <description>原文地址：https://mysqlserverteam.com/mysql-8-0-innodb-now-supports-instant-add-column/
 在MySQL 8.0中迁移到新的事务数据字典使我们的这项工作变得容易得多。在MySQL 8.0之前，元数据（数据字典）存储在称为.frm文件的平面文件中， .frm文件是一种不可思议的格式，已近过时很久了。
该即时加列补丁是由腾讯游戏数据库管理员团队提供的，我们要感谢并感谢腾讯游戏所做的重要而及时的贡献。
以前有什么问题  MySQL 5.6之前，执行DDL的唯一方法是逐行复制行 (copy) MySQL 5.6是第一个支持INPLACE DDL的版本。(inplace)  INPLACE DDL主要由InnoDB处理，而逐行COPY在服务器层处理    copy和inplace的存在的问题  对于大型表，可能要花费很长时间，尤其是在复制环境中。 磁盘空间需求将增加一倍以上，大小与现有表大致相同。 DDL操作占用资源，并且对CPU，内存和IO提出了很高的要求，这从用户事务中争夺资源。 如果涉及复制，slave要一直要等待到DDL的完成，才能开始同步。  新的instant加字段方式 出现的时间点  MySQL 8.0.12 由腾讯游戏数据库管理员团队提供的instant-add-column被官方集成认可 新的加字段语法,通过指定ALGORITHM=INSTANT来代替原来的加字段方式，SQL如下：  ALTER TABLE table_name [alter_specification], ALGORITHM=INSTANT;  MySQL 8.0.29开始，扩展了对ALTER TABLE … ALGORITHM=INSTANT的支持：用户可以在表的任何位置即时添加列、即时删除列、添加列时评估行大小限制。 MySQL 8.0.29开始，添加列时会检查行大小限制。如果超出限制，则会报错。  优势  INSTANT算法的优势在于，仅在数据字典中进行元数据更改。 更改期间无需获取元数据锁定，也不会修改表中的数据。 速度极快，秒速完成，对业务几乎没有影响 不会产生大量的binlog 不会影响主从同步 不会影响性能  原理  简单的说：只修改了表定义元数据，并没有修改真正的数据 翻译官方的原理是：  我们面临的问题是，在立即添加列后元数据发生更改后，如何解析页面上的物理记录？ 请注意，此处的物理记录是指存储在聚集索引的叶页中的记录。聚簇索引的现有二级索引甚至非叶页（B树的内部节点）都不会受到影响。 InnoDB有两种主要的行格式，即冗余行和紧凑行格式。行格式动态是compact的一个较小变体。压缩及其派生的行格式从冗余行格式中删除了一些元数据，以节省空间。 由于这种“节省空间”的更改，当我们必须对页面上物理行中的数据进行反序列化时，我们总是需要从内部元数据结构中查找元数据。 为了使即时添加列起作用，我们需要为页面上的DYNAMIC和COMPACT行格式的物理记录添加一些元数据。 REDUNDANT行格式不需要此附加元数据，因为列数已存储在物理记录中。 额外的信息与数据字典中的一些元数据一起保留在物理记录中。 这与基于相同腾讯补丁的一些下游黑客的做法非常不同，后者在表空间的模糊和未使用的部分存储类似的元数据。 我们认为，将元数据存储在适当的数据字典表中并使其在事务上保持一致将使其更健壮且更自然。此新的元数据存储在物理记录中。 这个新的元数据包括一个存储在info_bits中的标志。 info_bits中的此新信息用于跟踪是否在第一个即时ADD COLUMN之后创建记录。 我们还使用info_bits跟踪物理记录中的字段/列数。当表经历第一个即时ADD COLUMN时的列数以及新添加的列的所有默认值都存储在数据字典中。 这两条信息存储在数据字典表的se_private_data列中。 有了这些额外的信息，现在可以立即执行ADD COLUMN操作，而无需修改表中的任何行。如果没有即时的ADD COLUMN，则表中的所有行将采用与以前相同的格式。 即时发出ADD COLUMN后，对该表的任何更新都将以新格式写入行。从数据字典中查找默认值（如果有）。 在每个即时ADD COLUMN中，都会分别跟踪新添加的列的默认值。这些列的默认值可以随时更改。因此，在重建或截断表之后，可以丢弃即时列数和默认值，此外，可以像以前一样将表中的行更改为旧格式。 如果该表是分区表，则不同的分区可能具有不同数量的即时列，并且需要不同数量的默认值。 如果某些分区被重建，截断或重新创建，则分区中的行也可以像以前一样更改为旧格式。 使用限制  在了解原理之后，我们来看看 “立刻加列” 的使用限制，就很容易能理解其中的前两项： “instant加列” 的加列位置只能在表的最后，而不能加在其他列之间（MySQL8.</description>
    </item>
    
    <item>
      <title>8种常用于数据库的数据结构</title>
      <link>/dba/8%E7%A7%8D%E5%B8%B8%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</link>
      <pubDate>Tue, 02 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/8%E7%A7%8D%E5%B8%B8%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</guid>
      <description>1.跳跃表(skipList) 什么是skiplist  跳跃表（skiplist）是一种随机化的数据， 由 William Pugh 在论文《Skip lists: a probabilistic alternative to balanced trees》中提出， 跳跃表以有序的方式在层次化的链表中保存元素， 效率和平衡树媲美 —— 查找、删除、添加等操作都可以在对数期望时间下完成， 并且比起平衡树来说， 跳跃表的实现要简单直观得多。  图示    用途：  Redis  2.哈希索引（Hash Index） 什么是hash Index  基于哈希表实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 哈希索引可细分为静态哈希和动态哈希这两大类，  静态哈希  基于散列技术的文件组织使我们能够避免访问索引结构，同时也提供了一种构造索引的方法。在对散列的描述中，使用桶(bucket)来表示能存储一条或多条记录的一个存储单位。通常一个桶就是一个磁盘块，但也可能大于或者小于一个磁盘块。 散列索引将散列函数作用于搜索码以确定对应的桶， 然后将此搜索码以及对应的指针存入此桶(或溢出桶)中。 静态散列最大的缺点在于必须在实现系统时选择确定的散列函数。此后若被索引的文件变大或缩小，要想再改变散列函数就不容易了。因为散列函数 h 将搜索码值映射到桶地址的固定集合 B 上： 根据当前文件大小选择散列函数，这样的选择会使得性能随着数据库的增大而下降。换言之，初始时集合 B 太小，一个桶就会包含许多不同的搜索码值的记录，从而可能发生桶溢出。当文件变大时，性能就会受到影响。 根据将来某个时刻文件的预计大小选择散列函数。 尽管这样可以避免性能下降，但是初始时会造成相当大的空间浪费。  动态哈希  针对静态散列技术出现的问题，动态散列（dynamic hashing）技术允许散列函数动态改变，以适应数据库增大或缩小的需要 当数据库增大或缩小时，可扩充散列可以通过桶的分裂或合并来适应数据库大小的变化，这样可以保持空间的使用效率。此外，由于重组每次仅作用于一个桶，因此所带来的性能开销较低。  图示    3.ssTable 什么是ssTable  SSTable文件是memtable 数据到一定阈值写入文件形成的，由于内存容量总是有限的，将一定量数据写入磁盘可以存放更多数据，所以leveldb相比redis能存放更多数据。既然数据持久化到磁盘，那么还有必然涉及到从磁盘中查询数据，从磁盘中查询数据与从内存中查询数据的效率是不一样的，所以SSTable 数据组织方式必然与众不同，因为必须要提高查询效率，不能给一个key就去遍历所有SSTable。因此本文的另一个目的就是学习SSTable 文件如何组织key-value，提高查询效率。为了提高内存中数据查询效率 我们学习了各种数据结构如红黑树，散列表，那么SSTable是学习如何提高文件查询数据效率的一个很好例子。  图示    4.</description>
    </item>
    
    <item>
      <title>MySQL内置函数</title>
      <link>/mysql/mysql%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/</link>
      <pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/</guid>
      <description>MySQL字符串函数    函数 说明     ASCII(s) 返回字符串 s 的第一个字符的 ASCII 码。   CHAR_LENGTH(s) 返回字符串 s 的字符数   CHARACTER_LENGTH(s) 返回字符串 s 的字符数   CONCAT(s1,s2&amp;hellip;sn) 字符串 s1,s2 等多个字符串合并为一个字符串   CONCAT_WS(x, s1,s2&amp;hellip;sn) 同 CONCAT(s1,s2,&amp;hellip;) 函数，但是每个字符串直接要加上 x，x 可以是分隔符   FIELD(s,s1,s2&amp;hellip;) 返回第一个字符串 s 在字符串列表(s1,s2&amp;hellip;)中的位置   FIND_IN_SET(s1,s2) 返回在字符串s2中与s1匹配的字符串的位置   FORMAT(x,n) 函数可以将数字 x 进行格式化 &amp;ldquo;#,###.##&amp;rdquo;, 将 x 保留到小数点后 n 位，最后一位四舍五入。   INSERT(s1,x,len,s2) 字符串 s2 替换 s1 的 x 位置开始长度为 len 的字符串   LOCATE(s1,s) 从字符串 s 中获取 s1 的开始位置   LCASE(s) 将字符串 s 的所有字母变成小写字母   LEFT(s,n) 返回字符串 s 的前 n 个字符   LEFT(s,n) 返回字符串 s 的前 n 个字符   LENGTH(str) 返回字符的长度，一个汉字算三个字符，一个数字或字母算一个字符。   LOCATE(s1,s) 从字符串 s 中获取 s1 的开始位置   LOWER(s) 将字符串 s 的所有字母变成小写字母。   LPAD(s1,len,s2) 在字符串 s1 的开始处填充字符串 s2，使字符串长度达到 len   LTRIM(s) 去掉字符串 s 开始处的空格   MID(s,n,len) 从字符串 s 的 start 位置截取长度为 length 的子字符串，同 SUBSTRING(s,n,len)   POSITION(s1 IN s) 从字符串 s 中获取 s1 的开始位置   REPEAT(s,n) 将字符串 s 重复 n 次   REPLACE(str,from_str,to_str) 将字符串 str 中出现的字符串 from_str 替代成字符串 to_str，并返回替换后新字符串，该方法对大小写敏感。   REVERSE(s) 将字符串s的顺序反过来   RIGHT(s,n) 返回字符串 s 的后 n 个字符   RPAD(s1,len,s2) 在字符串 s1 的结尾处添加字符串 s2，使字符串的长度达到 len   RTRIM(s) 去掉字符串 s 结尾处的空格   SPACE(n) 返回 n 个空格   STRCMP(s1,s2) 比较字符串 s1 和 s2，如果 s1 与 s2 相等返回 0 ，如果 s1&amp;gt;s2 返回 1，如果 s1&amp;lt;s2 返回 -1   SUBSTR(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串   SUBSTRING(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串   SUBSTRING_INDEX(s, delimiter, number) 返回从字符串 s 的第 number 个出现的分隔符 delimiter 之后的子串。如果 number 是正数，返回第 number 个字符左边的字符串。如果 number 是负数，返回第(number 的绝对值(从右边数))个字符右边的字符串。   如果 number 是负数，返回第(number 的绝对值(从右边数))个字符右边的字符串。    TRIM(str) 去掉字符串 str 开始和结尾处的指定的符号，不指定默认为空格。   UCASE(s) 将字符串转换为大写   UPPER(s) 将字符串转换为大写    MySQL数字函数    函数 说明     ABS(x) 返回 x 的绝对值　   ACOS(x) 求 x 的反余弦值(参数是弧度)   ASIN(x) 求反正弦值(参数是弧度)   ATAN(x) 求反正切值(参数是弧度)   ATAN2(n, m) 求反正切值(参数是弧度)   AVG(expression) 返回一个表达式的平均值，expression 是一个字段   CEIL(x) 返回大于或等于 x 的最小整数　   CEILING(x) 返回大于或等于 x 的最小整数　   COS(x) 求余弦值(参数是弧度)   COT(x) 求余切值(参数是弧度)   COUNT(expression) 返回查询的记录总数，expression 参数是一个字段或者 * 号   DEGREES(x) 将弧度转换为角度　   n DIV m 整除，n 为被除数，m 为除数   EXP(x) 返回 e 的 x 次方　   FLOOR(x) 返回小于或等于 x 的最大整数　   GREATEST(expr1, expr2, expr3, &amp;hellip;) 返回列表中的最大值   LEAST(expr1, expr2, expr3, &amp;hellip;) 返回列表中的最小值   LN 返回数字的自然对数   LOG(x) 返回自然对数(以 e 为底的对数)　   LOG10(x) 返回以 10 为底的对数　   LOG2(x) 返回以 2 为底的对数   MAX(expression) 返回字段 expression 中的最大值   MIN(expression) 返回字段 expression 中的最小值   MOD(x,y) 返回 x 除以 y 以后的余数　   PI() 返回圆周率　   POW(x,y) 返回 x 的 y 次方　   POWER(x,y) 返回 x 的 y 次方　   RADIANS(x) 将角度转换为弧度　   RAND() 返回 0 到 1 的随机数　   ROUND(x) 返回离 x 最近的整数   SIGN(x) 返回 x 的符号，x 是负数、0、正数分别返回 -1、0 和 1　   SIN(x) 求正弦值(参数是弧度)　   SQRT(x) 返回x的平方根　   SUM(expression) 返回指定字段的总和   TAN(x) 求正切值(参数是弧度)   TRUNCATE(x,y) 返回数值 x 保留到小数点后 y 位的值（与 ROUND 最大的区别是不会进行四舍五入）    MySQL日期时间函数    函数 说明     ADDDATE(d,n) 计算起始日期 d 加上 n 天的日期   ADDTIME(t,n) 时间 t 加上 n 秒的时间   CURDATE() 返回当前日期   CURRENT_DATE() 返回当前日期   CURRENT_TIME 返回当前时间   CURRENT_TIMESTAMP() 返回当前日期和时间   CURTIME() 返回当前时间   DATE() 从日期或日期时间表达式中提取日期值   DATEDIFF(expr1, expr2) 计算日期 expr1 表达式与日期 expr2 之间相隔的天数。   DATE_ADD(d，INTERVAL expr type) 计算起始日期 d 加上一个时间段后的日期   DATE_FORMAT(d,f) 按表达式 f的要求显示日期 d   DATE_SUB(date, INTERVAL expr unit) 从指定日期减去指定的日期时间间隔。   DAY(d) 返回日期值 d 的日期部分   DAYNAME(d) 返回日期 d 是星期几，如 Monday,Tuesday   DAYOFMONTH(d) 计算日期 d 是本月的第几天   DAYOFWEEK(d) 日期 d 今天是星期几，1 星期日，2 星期一，以此类推   DAYOFYEAR(d) 计算日期 d 是本年的第几天   EXTRACT(type FROM d) 从日期 d 中获取指定的值，type 指定返回的值。type可取值为：,MICROSECOND,SECOND,MINUTE,HOUR,DAY,WEEK,MONTH,QUARTER,YEAR,SECOND_MICROSECOND,MINUTE_MICROSECOND,MINUTE_SECOND,HOUR_MICROSECOND,HOUR_SECOND,HOUR_MINUTE,DAY_MICROSECOND,DAY_SECOND,DAY_MINUTE,DAY_HOUR,YEAR_MONTH   FROM_DAYS(n) 计算从 0000 年 1 月 1 日开始 n 天后的日期   HOUR(t) 返回 t 中的小时值   LAST_DAY(d) 返回给给定日期的那一月份的最后一天   LOCALTIME() 返回当前日期和时间   LOCALTIMESTAMP() 返回当前日期和时间   MAKEDATE(year, day-of-year) 基于给定参数年份 year 和所在年中的天数序号 day-of-year 返回一个日期   MAKETIME(hour, minute, second) 组合时间，参数分别为小时、分钟、秒   MICROSECOND(date) 返回日期参数所对应的毫秒数   MINUTE(t) 返回 t 中的分钟值   MONTHNAME(d) 返回日期当中的月份名称，如 Janyary   MONTH(d) 返回日期d中的月份值，1 到 12   NOW() 返回当前日期和时间。   PERIOD_ADD(period, number) 为 年-月 组合日期添加一个时段   PERIOD_DIFF(period1, period2) 返回两个时段之间的月份差值   QUARTER(d) 返回日期d是第几季节，返回 1 到 4   SECOND(t) 返回 t 中的秒钟值   SEC_TO_TIME(s) 将以秒为单位的时间 s 转换为时分秒的格式   STR_TO_DATE(string, format_mask) 将字符串转变为日期   SUBDATE(d,n) 日期 d 减去 n 天后的日期   SUBTIME(t,n) 时间 t 减去 n 秒的时间   SYSDATE() 返回当前日期和时间   TIME(expression) 提取传入表达式的时间部分   TIME_FORMAT(t,f) 按表达式 f 的要求显示时间 t   TIME_TO_SEC(t) 将时间 t 转换为秒   TIMEDIFF(time1, time2) 计算时间差值   TIMESTAMP(expression, interval) 单个参数时，函数返回日期或日期时间表达式；有2个参数时，将参数加和   TO_DAYS(d) 计算日期 d 距离 0000 年 1 月 1 日的天数   UNIX_TIMESTAMP([date]) 获取对应的时间戳，无参时，返回当前的时间戳。   WEEK(d) 计算日期 d 是本年的第几个星期，范围是 0 到 53   WEEKDAY(d) 日期 d 是星期几，0 表示星期一，1 表示星期二   WEEKOFYEAR(d) 计算日期 d 是本年的第几个星期，范围是 0 到 53   YEAR(d) 返回年份   YEARWEEK(date, mode) 返回年份及第几周（0到53），mode 中 0 表示周天，1表示周一，以此类推    MySQL聚合函数    函数 说明     AVG() 返回指定参数的平均值。   BIT_AND() 返回表达式的所有位按位 AND 结果。   BIT_OR() 返回表达式的所有位按位 OR 结果。   BIT_XOR() 返回表达式的所有位按位 XOR 结果。如果参数为 NULL，返回值也将为 NULL。   COUNT() 返回条数。   COUNT(DISTINCT) 返回指定不同值的条数。   GROUP_CONCAT() 将 GROUP BY 产生的同一个分组中的值连接起来，返回一个字符串结果。   MAX() 返回最大值。   MIN() 返回最小值。   STD() 返回总体标准差。   STDDEV() 返回总体标准差。   STDDEV_POP() 返回总体标准差。   STDDEV_SAMP() 返回样本标准差。   SUM() 返回运算和的结果。   VAR_POP() 返回总体方差。   VAR_SAMP() 返回样本方差。   VARIANCE() 返回总体方差。    MySQL其他函数    函数 说明     BIN(x) 返回 x 的二进制编码   BINARY(s) 将字符串 s 转换为二进制字符串   case when else end 分支选择   CAST(expr AS type [ARRAY]) 转换数据类型   COALESCE(expr1, expr2, &amp;hellip;.</description>
    </item>
    
    <item>
      <title>MySQL8.0尝试用json索引替换全文索引</title>
      <link>/mysql/mysql8.0%E5%B0%9D%E8%AF%95%E7%94%A8json%E7%B4%A2%E5%BC%95%E6%9B%BF%E6%8D%A2%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql8.0%E5%B0%9D%E8%AF%95%E7%94%A8json%E7%B4%A2%E5%BC%95%E6%9B%BF%E6%8D%A2%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/</guid>
      <description>原因  MySQL8.0.22版本 线上有一张task表的users字段因历史原因 存放了以逗号分隔的用户id列表 程序中会匹配用户id进行查询 用到的SQL如下：  # 查询 select users from task limit 2; | users | |300511164303031, 310406164883350, 151134164673502, 330203164377115, 310633164035316, 310408164888300, 170515164003106, 150636164603618, 310510164335822, 151336164653174, 310508164331806, 301115164423156 | |310406164883350,181138164432020,1000130,330312164322768,170515164003106,300608164825431,331015164472774,150304164442136,331108164613233,1000164,301113164430265,171016164003026,300333164732303,151134164673502,1000143,331034164487883,181033164253337,310633164035316,150304164442101,1000136,330312164636073,310508164331806,330302164334267,181017164275220,301115164423156,330203164377336,310303164733465,330312164322726,330203164377115,310408164888300,311116164231848,1000123,310214164825778,301317164618388,300333164732155,151013164628330,300511164303031,1000138,1000185,150636164603618,300415164783624,310237164871433,310510164335822,151336164653174,330210164387154 | -- 数据和表名，列名已做掩码转换。非真实数据 # 示例 select * from task where MATCH(users) AGAINST(&#39;19323422341234&#39; );	 表的数据量不多40多万条记录 但是频繁的出现慢查询(超过500毫秒)  优化思路  定位到全文索引慢的时候 第一反应是拆了这个全文索引查询 业务方将逗号字段拆表的改动量大暂时不考虑 折中办法是将这个字段换成json类型 然后用json的索引来替换全文索引 我在想这个方案的时候 给忠哥的预估是性能会提升3-10倍 当时没做测试 靠的是经验和信口开河 一通怂恿说服了研发同事 开始拉群开整  验证和测试 # 加json字段 alter table task add users_list json ; # 填值 update task set users_list = concat(&#39;[&#39; ,TRIM(BOTH &#39;,&#39; FROM users),&#39;]&#39;) where users is not null and users !</description>
    </item>
    
    <item>
      <title>MySQL组复制GroupReplication参数</title>
      <link>/mysql/mysql%E7%BB%84%E5%A4%8D%E5%88%B6groupreplication%E5%8F%82%E6%95%B0/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%BB%84%E5%A4%8D%E5%88%B6groupreplication%E5%8F%82%E6%95%B0/</guid>
      <description>参数 group_replication_allow_local_disjoint_gtids_join （已弃用）  布尔型，默认值为OFF，MySQL 5.7.17版本引入，5.7.21版本弃用，8.0.4版本中删除。 这是MySQL5.7时代搭MGR集群时经常会用到的参数，设置为开启后，新加节点变得更容易，但是慎用该参数，不正确的使用可能会导致复制组中的数据出现不一致。MySQL8版本中已弃用。 即使该组中缺失一些事务（joiner节点比组中的事务还要多），也允许joiner节点加入该组。   group_replication_allow_local_lower_version_join  布尔型，默认为OFF，MySQL 5.7.17版本引入。 也是新节点加入时使用到的变量，一般不会开启，开启后允许低版本的MySQL节点加入集群 MySQL 8.0.17及其之后的版本在比对版本号时，会考虑次要版本号（例如：MySQL 8.0.17，会将次要版本号17一起进行比较），在MySQL 8.0.16及其之前的版本在比对版本号时，只考虑主要版本号（例如：MySQL 5.7.22，只比对主要版本号5.7）。 将该系统变量设置为ON并不会使低版本的Server与组兼容，但能允许低版本Server加入组，不过没有任何措施来防止低版本Server与组中现有成员的不兼容行为，因此，为了确保低版本Server的正确操作，必须人为确保如下两项措施，如果不能确保这两项措施，则运行低版本的Server可能会碰到错误而导致加入组失败。 * 运行较低版本的Server加入组之前，必须先停止该Server上的任何写操作 * 从运行较低版本的Server加入组的位置开始，停止对组中所有成员的任何写操作   group_replication_auto_increment_increment  整型类型，默认值为7（如果你的组中有更多或更少的组成员，则，可以在组复制启动之前调整好该系统变量的值，以对应你的组中的成员数量），取值范围：1~65535。MySQL 5.7.17版本引入。 注意：该系统变量的值在所有组成员上必须相同 注意：一旦设置将代替系统变量auto_increment_inncrement。且将auto_increment_offset设置为Server id值 自动设置复制组中的每个成员的自增列的步长值，以确保在多主模式的组中，每个组成员的自增列值有序且不重叠。 当成员停止组复制时，普通系统变量auto_increment_inncrement和auto_increment_offset的值将会恢复原状（启动组复制之前的值） 只有当系统变量auto_increment_increment和auto_increment_offset保持默认值时，组复制启动时才会自动做与组复制的适配调整和恢复，如果这两个系统变量的值被设置了非默认值，则组复制不会做自动调整（从MySQL 8.0开始，当组处于单主模式下时，这两个系统变量也不会做自动调整）。所以，对于这两个系统变量的值，要么在组复制下不对其进行手工指定（让其使用默认值），要么就一定要设置正确，否则，在多主模式的主中，很容易造成主键冲突。 系统变量group_replication_auto_increment_increment在组复制运行时无法修改，需要先停止组复制，修改该系统变量的值，然后再启动组复制   group_replication_bootstrap_group  布尔型，默认为OFF，MySQL 5.7 17版本引入。 只在新建集群时第一个启动节点时用，用完就得关，别轻易设置，容易脑裂 指定使用哪个Server来引导组（这里指的是将此系统变量设置为ON的Server）。该系统变量只能在一个Server上设置，并且只能在首次引导组或重新引导整个组时在其中一个Server上设置。当复制组引导成功之后需要及时将该系统变量为OFF来动态关闭（在组所有可能涉及的Server的配置文件中建议统一将此系统变量设置为OFF）。如果在某Server上使用该变量引导复制组之后再在另外一个Server中使用该变量引导复制组，则如果两个Server使用了相同的组名称时，可能会产生人为的脑裂。   group_replication_consistency  注意：这是重要参数,控制事务一致性等级 从MySQL 8.0.14引进 对于绝大多数场景，使用默认的 EVENTUAL 等级就足够 一般我们建议用默认值或Before  1 EVENTUAL  在这个等级下，RO和RW事务执行前，都不会要求等待积压事务先行应用完成。 这是默认等级，也是在引入该选项前的行为。这意味着以下几点： RW事务无需等待，而可能先于其他节点进行外部化（将事务广播到其他节点）。 RO事务可能读取到旧数据。 在Primary节点切换时，新产生的RW事务有可能会因为冲突而回滚。  2 BEFORE_ON_PRIMARY_FAILOVER  当发生Primary节点切换时，在新的Primary上需要先等待把所有来自旧Primary节点的积压事务应用完毕，之后才能正式完成切换，转成ONLINE状态，成为新的Primary节点，继续响应新的事务请求。 这么做可以保证在发生故障转移时，客户端不会查到旧数据，保证了数据一致性，不过客户端上也可能会产生延迟等待。  3 BEFORE  RW事务在应用之前，RO事务在执行之前，都要先等待前面堆积的事务完成。 这可以保证RO事务总能读取到最新事务，但对于RW事务而言，只是等待堆积事务应用完成，但并不要求其他节点上也完成该事务。  4 AFTER  它比BEFORE更近一步，要求RW事务在其他节点上也要等待应用完毕。这样一来，后续的事务在任何节点上就都能获取最新事务数据。 事实上，要慎用该级别及更高以上级别，可能会引发其他问题，可参考这个文章：技术分享 | 为什么MGR一致性模式不推荐AFTER  5 BEFORE_AND_AFTER  一致性级别要求最高，对RO和RW事务都要求同步事务数据。也就是说，RW事务在应用之前，要先等待前面堆积的事务完成，并且还需要等待它的事务变更在其他所有节点上也都应用；RO事务在执行之前，也要先等待前面堆积的事务完成。  group_replication_member_weight  整型类型，默认值为50，取值范围为：0~100。MySQL 5.</description>
    </item>
    
    <item>
      <title>MySQL压力测试之MySQLslap</title>
      <link>/mysql/mysql%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E4%B9%8Bmysqlslap/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E4%B9%8Bmysqlslap/</guid>
      <description>使用语法如下： mysqlslap [options] 参数 [options]  &amp;ndash;auto-generate-sql, -a 自动生成测试表和数据，表示用mysqlslap工具自己生成的SQL脚本来测试并发压力。 &amp;ndash;auto-generate-sql-load-type=type 测试语句的类型。代表要测试的环境是读操作还是写操作还是两者混合的。取值包括：read，key，write，update和mixed(默- 认)。 &amp;ndash;auto-generate-sql-add-auto-increment 代表对生成的表自动添加auto_increment列，从5.1.18版本开始支持。 &amp;ndash;number-char-cols=N, -x N 自动生成的测试表中包含多少个字符类型的列，默认1 &amp;ndash;number-int-cols=N, -y N 自动生成的测试表中包含多少个数字类型的列，默认1 &amp;ndash;number-of-queries=N 总的测试查询次数(并发客户数×每客户查询次数) &amp;ndash;query=name,-q 使用自定义脚本执行测试，例如可以调用自定义的一个存储过程或者sql语句来执行测试。 &amp;ndash;create-schema 代表自定义的测试库名称，测试的schema，MySQL中schema也就是database。 &amp;ndash;commint=N 多少条DML后提交一次。 &amp;ndash;compress, -C 如果服务器和客户端支持都压缩，则压缩信息传递。 &amp;ndash;concurrency=N, -c N 表示并发量，也就是模拟多少个客户端同时执行select。可指定多个值，以逗号或者&amp;ndash;delimiter参数指定的值做为分隔符。例如：- &amp;ndash;concurrency=100,200,500。 &amp;ndash;engine=engine_name, -e engine_name 代表要测试的引擎，可以有多个，用分隔符隔开。例如：&amp;ndash;engines=myisam,innodb。 &amp;ndash;iterations=N, -i N 测试执行的迭代次数，代表要在不同并发环境下，各自运行测试多少次。 &amp;ndash;only-print 只打印测试语句而不实际执行。 &amp;ndash;detach=N 执行N条语句后断开重连。 &amp;ndash;debug-info, -T 打印内存和CPU的相关信息。  优点：  可以针对某些特定类型的语句进行测试， 例如：  mysqlslap --no-defaults -h 127.0.0.1 -P 3307 --query=敏感列无索引where查询.sql --create-schema=db_test --concurrency=10,20 mysqlslap --no-defaults -h 127.</description>
    </item>
    
    <item>
      <title>MySQL5.7实例无限重启故障定位及解决</title>
      <link>/mysql/mysql5.7%E5%AE%9E%E4%BE%8B%E6%97%A0%E9%99%90%E9%87%8D%E5%90%AF%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D%E5%8F%8A%E8%A7%A3%E5%86%B3/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql5.7%E5%AE%9E%E4%BE%8B%E6%97%A0%E9%99%90%E9%87%8D%E5%90%AF%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D%E5%8F%8A%E8%A7%A3%E5%86%B3/</guid>
      <description>故障开始时间：2023-06-30 09:18 故障实例：BI大数据业务
 环境  系统：CentOS Linux release 7.8.2003 (Core) MySQL: 5.7.28-log MySQL Community Server (GPL) 部署：多实例部署，当前实例bufferPool:8G 集群：3台主机  主：51 备：52 (和51做双主) 从：53 （同步自52）    故障现象  收到报警，该实例频繁报连接异常和恢复 检查发现该MySQL实例频繁重启 1.该实例访问量很小，不是资源不足引起 2.和研发确认该实例相关的业务最近未发生变更 3.DBA内部确认最近该实例没有做配置变更 4.报错时系统日志无异常报错 5.MySQL正常运行时可以提供服务，但1分钟左右就会自动shutdown 6.慢日志里没有异常SQL 7.MySQL错误日志里只有实例启动后自检的warinning 以及  2023-06-30T10:15:35.534553+08:00 0 [Warning] CA certificate ca.pem is self signed. 2023-06-30T10:15:35.546909+08:00 0 [Warning] Recovery from master pos 59075485 and file mysql-bin.***** for channel &#39;&#39;. Previous relay log pos and relay log file had been set to 416, /data/mysql******/relaylognew/relay-bin.</description>
    </item>
    
    <item>
      <title>MySQL5.7升级到8.0(二):配置和参数</title>
      <link>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B42%E9%85%8D%E7%BD%AE%E5%92%8C%E5%8F%98%E5%8C%96/</link>
      <pubDate>Thu, 20 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B42%E9%85%8D%E7%BD%AE%E5%92%8C%E5%8F%98%E5%8C%96/</guid>
      <description>研发：MySQL5.7升级到8.0(一):SQL语法变化
  DBA：MySQL5.7升级到8.0(二):配置和参数
   Note：这里面是升级到8.0,需要DBA参与修改部分或注意部分
 1.配置文件.cnf变化 以下参数变化 expire-logs-days =&amp;gt; binlog_expire_logs_seconds # 替换 expire-logs-days tx_isolation =&amp;gt; transaction_isolation tx_read_only =&amp;gt; transaction_read_only innodb_undo_logs =&amp;gt; innodb_rollback_segments have_query_cache = no # 永远为 NO expire-logs-days 后续可能废弃, 使用 binlog_expire_logs_seconds (目前还支持) 以下参数不再支持 innodb_stats_sample_pages innodb_locks_unsafe_for_binlog innodb_file_format innodb_file_format_check innodb_file_format_max innodb_large_prefix ignore_builtin_innodb skip-symbolic-links # 默认即 skip-symbolic-links. sync_frm # 8.0 版本去掉了 .frm 文件, 内置在 ibd 文件中 sql_log_bin # 仅支持会话级别设置 query_cache_xxx # 缓存相关的系统变量 metadata_locks_cache_size metadata_locks_hash_instances date_format datetime_format time_format max_tmp_tables 2.</description>
    </item>
    
    <item>
      <title>MySQL Group Replication(MGR集群)增加节点和迁移节点</title>
      <link>/mysql/mysqlgroupreplicationmgr%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9%E5%92%8C%E8%BF%81%E7%A7%BB%E8%8A%82%E7%82%B9/</link>
      <pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysqlgroupreplicationmgr%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9%E5%92%8C%E8%BF%81%E7%A7%BB%E8%8A%82%E7%82%B9/</guid>
      <description>MySQL Group Replication 多主结构的3节点需要切到另外三个节点上，任务需要先加3个节点到集群中，再删掉原来的3个节点。
 环境   原实例：
 172.0.2.30:3309 172.0.2.31:3309 172.0.2.32:3309    新实例：
 172.0.2.83:3309 172.0.2.84:3309 172.0.2.85:3309    修改host - 修改6台主机的/etc/hosts 172.0.2.30 dba-mysql3309-230 dba-mysql3309-230.dboop.com 172.0.2.31 dba-mysql3309-231 dba-mysql3309-231.dboop.com 172.0.2.32 dba-mysql3309-232 dba-mysql3309-232.dboop.com 172.0.2.83 dba-mysql3309-83 dba-mysql3309-83.dboop.com 172.0.2.84 dba-mysql3309-84 dba-mysql3309-84.dboop.com 172.0.2.85 dba-mysql3309-85 dba-mysql3309-85.dboop.com 旧实例上增加seed  检查状态  mysqlw -h 172.0.2.30 -P 3309 -e &amp;quot;show global variables like &#39;group_replication_group_seeds&#39;&amp;quot;; mysqlw -h 172.0.2.31 -P 3309 -e &amp;quot;show global variables like &#39;group_replication_group_seeds&#39;&amp;quot;; mysqlw -h 172.</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(四):BinLog</title>
      <link>/mysql/mysql%E7%9A%84binlog%E6%97%A5%E5%BF%97/</link>
      <pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84binlog%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续：
 MySQL的7种日志(一):概况 MySQL的7种日志(二):RedoLog MySQL的7种日志(三):UndoLog MySQL的7种日志(四):BinLog  1.什么是binlog  又名:MySQL归档日志,MySQL二进制日志 记录所有数据库表结构变更（DDL例如CREATE、ALTER TABLE…）以及表数据修改（DMLINSERT、UPDATE、DELETE …）的所有操作。 默认情况下，二进制日志并不是在每次写的时候同步到磁盘。因此，当数据库所在地操作系统发生宕机时，可能会有最后一部分数据没有写入二进制日志文件中，这会给恢复和复制带来问题。  2.binlog的作用  时间点的恢复：某些数据的恢复需要二进制日志，例如，在一个数据库全备文件恢复后，用户可通过二进制日志进行即时点（point-in-time）恢复。 主从复制：通过复制和执行二进制日志使一台远程的 Mysql 数据库（一般称为 slave）与一台 MySQL 数据库（一般称为 master）进行实时同步。 变更审计：用户可以通过二进制日志中的信息来进行审计，回溯是否对数据库的修改。 误操作回滚：当误修改(ins/upd/del)发生时,可以用binlog解析出修改前后的语句,用于快速回滚 异构数据同步：通过解析binlog,可以将MySQL的变更通知到异构数据源(kafka,es,mongo,redis,mq&amp;hellip;) 事务存储引擎的崩溃恢复。MySQL采用事务的两阶段提交协议。当 MySQL 系统发生崩溃时，事务在存储引擎内部的状态可能为 prepared 和 commit 两种。对于 prepared 状态的事务，是进行提交操作还是进行回滚操作，这时需要参考 binlog：如果事务在 binlog 中存在，那么将其提交；如果不在 binlog 中存在，那么将其回滚，这样就保证了数据在主库和从库之间的一致性。  3.binlog 和 redolog 区别  适用对象不同： binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用 redolog 是 InnoDB 引擎特有的 写入内容不同：  binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：语句 ROW：记录行数据最终被修改成什么样了 MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redolog 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；   写入方式不同： binlog 是可以追加写入的。“追加写” 是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志 redolog 是循环写的，空间固定会被用完 作用不同  4.</description>
    </item>
    
    <item>
      <title>MySQL中间件对比:ProxySQL/MaxScale/ShardingSphere</title>
      <link>/mysql/mysql%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Thu, 03 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%AF%B9%E6%AF%94/</guid>
      <description>MySQL中件间对比 目标：对比以下3款MySQL中件间
 ProxySQL MaxScale ShardingSphere  一.Proxy基础能力 兼容性  ProxySQL:良好 MaxScale:mairdb更友好，MGR支持不好 ShardingSphere:良好  发行方  ProxySQL:sysown MaxScale:mairdb ShardingSphere:京东  发布/更新/生态  ProxySQL:c++, 5.1k stars,最新版v2.4.4 1个半月前发布 MaxScale:c,1.6k stars,最新版22.08.2 3个半月前发布 ShardingSphere:java,17.6k stars,最新版5.2.1 16天前发布  运维便利性  ProxySQL:简单 MaxScale:普通 ShardingSphere:复杂（配置文件多且复杂）  二.Proxy高可用架构支持 主从架构下的从库故障  如何探测到   ALL: 每N秒主动探测一次
  如何响应故障   ALL:探测不到，主动处理: - ShardingSphere:从路由表中标记下线 - proxysql:从group中标记下线 - MaxScale:标记下线
  切换后状态   ALL:复制拓扑可用
  影响时长   ALL:1-5 秒</description>
    </item>
    
    <item>
      <title>MySQL高可用组件之ProxySQL</title>
      <link>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Bproxysql/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Bproxysql/</guid>
      <description>ProxySQL是什么 介绍  MySQL一款开源的中间件的产品 支持读写分离 支持 Query 路由功能 支持动态指定某个SQL进行缓存 支持动态加载（无需重启ProxySQL服务） 故障切换和SQL过滤功能。  ProxySQL初始化 安装 wget https://github.com/sysown/proxysql/releases/download/v2.4.4/proxysql-2.4.4-1-centos7.x86_64.rpm rpm -ivh proxysql-2.4.4-1-centos7.x86_64.rpm 产生报错： warning: proxysql-2.4.4-1-centos7.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 8217c97e: NOKEY error: Failed dependencies: gnutls is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28()(64bit) is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28(GNUTLS_1_4)(64bit) is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28(GNUTLS_3_0_0)(64bit) is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28(GNUTLS_3_1_0)(64bit) is needed by proxysql-2.4.4-1.x86_64 执行： yum install -y gnutls rpm -ivh proxysql-2.4.4-1-centos7.x86_64.rpm 检查安装情况  rpm -ql proxysql /etc/logrotate.</description>
    </item>
    
    <item>
      <title>MySQL高可用组件之orchestrator</title>
      <link>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Borchestrator/</link>
      <pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Borchestrator/</guid>
      <description>orchestrator是什么 介绍  GitHub公司的用go语言编写并开源的一个MySQL高可用管理工具 MySQL高可用性和复制拓扑管理工具，支持复制拓扑结构的调整，自动故障转移和手动主从切换等。 提供Web界面展示MySQL复制的拓扑关系及状态，通过Web可更改MySQL实例的复制关系和部分配置信息 同时也提供命令行和api接口，方便运维管理。 相对比MHA来看最重要的是解决了管理节点的单点问题，其通过raft协议保证本身的高可用。  特点  自动发现MySQL的复制拓扑，并且在web上展示。 重构复制关系，可以在web进行拖图来进行复制关系变更。 检测主异常，并可以自动或手动恢复，通过Hooks进行自定义脚本。 支持命令行和web界面管理复制。  功能限制  slave不能手动提升为master 不支持多源复制 不支持并行复制 不支持与PXC联合使用  注意事项  对主机名依赖严重，习惯用ip来管理实例的，需要注意确保主机名可解析 主从拓扑结果目前不支持两个实例互为主备或环形结构 自动故障转移，只负责将主从切换了，把从设置为主，其他的变更（dns或proxy变更以及运维信息的变更等需要自己写hook脚本）  orchestrator的工作原理 orchestrator的探测机制  orchestrator会每隔InstancePollSeconds（默认5s）时间用以下SQL去被监控的实例上读取实例状态  show global status like &#39;Uptime&#39; select @@global.hostname, ifnull(@@global.report_host, &#39;&#39;), @@global.server_id, @@global.version, @@global.version_comment, @@global.read_only, @@global.binlog_format, @@global.log_bin, @@global.log_slave_updates show master status show global status like &#39;rpl_semi_sync_%_status&#39; select @@global.gtid_mode, @@global.server_uuid, @@global.gtid_executed, @@global.gtid_purged, @@global.master_info_repository = &#39;TABLE&#39;, @@global.binlog_row_image show slave status select count(*) &amp;gt; 0 and MAX(User_name) !</description>
    </item>
    
    <item>
      <title>MySQL的参数对比方法</title>
      <link>/mysql/mysql%E7%9A%84%E5%8F%82%E6%95%B0%E5%AF%B9%E6%AF%94%E6%96%B9%E6%B3%95/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E5%8F%82%E6%95%B0%E5%AF%B9%E6%AF%94%E6%96%B9%E6%B3%95/</guid>
      <description>什么时候需要对比MySQL参数  迁移时，从一个集群到另一个集群 升级时，从一个版本到另一个版本 巡检时，需要关注重点参数是否有人为修改过 其他时候，自己去想  怎么对比 实例少时，比如两个集群的几组实例  可以去每台机器上把重点参数打印出来 手动对比  大量数据库实例的对比，需要用脚本工具实现 第一步，建一个收集表 CREATE TABLE `info_variables` ( `instanceid` int NOT NULL DEFAULT &#39;0&#39;, `var_key` varchar(100) NOT NULL DEFAULT &#39;&#39;, `var_value` varchar(1000) NOT NULL DEFAULT &#39;&#39;, `linkname` varchar(50) NOT NULL DEFAULT &#39;&#39;, clustertype varchar(10) not null default &#39;&#39;, `_timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`instanceid`,`var_key`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 第二步，建一个收集任务  我是在dboop平台上配置的，执行show global variables再 insert 到info_variables表  第三步，清理指标 delete from info_variables where var_key like &#39;wsrep%&#39;; delete from info_variables where var_key like &#39;performance_schema_%&#39;; delete from info_variables where var_key like &#39;ssl_%&#39;; delete from info_variables where var_key like &#39;log%&#39;; delete from info_variables where var_key like &#39;group%&#39;; delete from info_variables where var_key like &#39;validate%&#39;; delete from info_variables where var_key like &#39;gtid%&#39;; delete from info_variables where var_value like &#39;%/%&#39;; delete from info_variables where var_key in ( &#39;datadir &#39;, &#39;hostname &#39;, &#39;innodb_data_home_dir &#39;, &#39;innodb_log_group_home_dir &#39;, &#39;innodb_undo_directory &#39;, &#39;port &#39;, &#39;relay_log &#39;, &#39;relay_log_basename &#39;, &#39;relay_log_index &#39;, &#39;report_port &#39;, &#39;server_id &#39;, &#39;slave_load_tmpdir &#39;, &#39;socket &#39;, &#39;tmpdir &#39; ); 第四步，形成报告  可以按需求出报告或报表 甚至做可视化的参数对比页面  重点关注参数 时间类：  explicit_defaults_for_timestamp time_zone  自增主键：  auto_increment_increment auto_increment_offset innodb_autoextend_increment innodb_autoinc_lock_mode  连接属性：  join_buffer_size max_tmp_tables wait_timeout max_allowed_packet max_connections  字符编码：  character_set_server transaction_isolation collation_connection collation_database collation_server  mode:  sql_mode   以上参数的变化和不一致，可能会在迁移或升级过程中带来严重的后果，需慎重。</description>
    </item>
    
    <item>
      <title>万物不如MySQL_万物皆可Join</title>
      <link>/dba/%E4%B8%87%E7%89%A9%E7%9A%86%E5%8F%AFsql/</link>
      <pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E4%B8%87%E7%89%A9%E7%9A%86%E5%8F%AFsql/</guid>
      <description>当前数据库架构越来越复杂
  数据库MongoDB,Redis,Es,Kafka,Postgresql&amp;hellip;
  加上传统的关系型数据库（MySQL,Oracle,SQLServer)
  你是否因为各种数据库的查询语言不同而头晕眼花，到处撞墙！
   你是否各种分库分表后，不同的数据库之前没办法join联合查询而一蹶不振   你是否业务同学发给你一个excel，让你查这些订单的明细而不知所措，来回倒腾。   你是否在焦急的等待着BI大数据同事帮你把不同数据源的表都抽到一起才能join出想要的数据？   怎么办？怎么办？ 没办法!!! 拆开的数据库没办法放在一台服务器上 各种数据库也没办法统一成一种 大数据部门的同步任务正在走流程 走完的流程，他们也不能保证数据同步任务不中断 Excel不是数据库不能用SQL 怎么办？怎么办？ 这种混乱就没人能治吗？ 不要让这些问题挡住你前进的脚本 dboop平台的统一查询平台横空出世 不再区分数据库类型 所有的数据库种类都支持MySQL语法 是的，你没有听错 不管什么类型的数据库 统统只需要记住MySQL语法了 Oracle,SQLServer,MongoDB,kafka DBA在运维的每一种数据库 都可以当成MySQL一样使用了  kafka当成MySQL Mongo当成MySQL  而且这些表都是可以互相join ,union 的   现在我们来休验一下这神奇的黑科技 第一步 我们有个这样的excel 第二步 把excel上传到平台上 第三步 得到一个可以查询的excel文件 第四步 用excel join MySQL 这就是我们说的： 几个问题 问题1:查询会不会影响线上业务  绑定了dba的高可用架构系统，可以自动路由到专门给bi取数服务的专用只读实例上。不会对线上应用产生影响 理论上bi抽数进程会和它产生资源抢占，但是因为bi抽数多数是凌晨进行，两个并不冲突
 问题2:查询的性能怎样  快，非常快，普通的单表查询0.</description>
    </item>
    
    <item>
      <title>定时收集存储过程函数视图信息入库(Oracle,MySQL)</title>
      <link>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%87%BD%E6%95%B0%E8%A7%86%E5%9B%BE%E4%BF%A1%E6%81%AForaclemysql/</link>
      <pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%87%BD%E6%95%B0%E8%A7%86%E5%9B%BE%E4%BF%A1%E6%81%AForaclemysql/</guid>
      <description>需求 下午接到运维转来的一个权限申请流程：大数据部门研发要求开通保垒机权限。以方便在保垒机上安装SQL客户端去查看存储过程和视图的内容
保垒机直连数据库查询，这种不可控的方式，早在去年我就把这个历史问题给禁止掉了，没想到现在还有人要求开通
经过沟通得知，对方想查看Oracle数据库里的一些老的存储过程的代码。而DBA平台上只有表结构相关的数据字典，没有存储过程和视图的数据字典
所以，别慌，不就这点需求吗，马上就可以加上。
 为什么DBA平台上的数据字典不包括存储过程和视图？ 因为存储过程/函数/视图 也是被我禁掉的，研发人员上线不可以写存储过程和视图。 所以就没想过要在DBA运维平台上做这块功能
 但是因为
 历史原因，以前的Oracle数据库上已经存在很多的视图和存储过程 第三方原因，公司采购的一些第三方服务和软件，带了存储过程和视图 这些被禁止使用的数据库对象，也需要做统一维护  拆解  这些数据库对象的信息用定时任务收集线上的表结构到本地，存为两份 一份入库，做为快照信息，展示给用户。 一份落本地文件，上传到git，用git做版本管理   为什么不在用户请求查看某个数据库对象的信息时，实时查询给用户？
 1.因为历史原因，我们有的库有几万个数据库对象，当用户选择一个库时，list列表加载很慢，所以一开始设计的时候，我们做了快照 2.一份快照，还可以用作数据库对象的git版本管理   建表 在dboop库中建表
 CREATE TABLE `info_objects` ( `objectid` int NOT NULL AUTO_INCREMENT, `dbid` int NOT NULL DEFAULT &#39;0&#39;, `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT &#39;&#39;, `object_name` varchar(255) NOT NULL DEFAULT &#39;&#39;, `object_type` varchar(64) NOT NULL DEFAULT &#39;&#39;, `object_text` longtext, `cstatus` smallint NOT NULL DEFAULT &#39;1&#39;, `dba_freshtime` datetime NOT NULL DEFAULT &#39;1990-01-01 00:00:00&#39;, PRIMARY KEY (`objectid`), UNIQUE KEY `idx_infoobjects_id` (`dbid`,`object_name`,`object_type`), KEY `idx_info_objects_time` (`dba_freshtime`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 实现数据字典收集入库 建Oracle采集任务  &amp;lt;!</description>
    </item>
    
    <item>
      <title>MySQL的事务隔离和MVCC</title>
      <link>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8Cmvcc/</link>
      <pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8Cmvcc/</guid>
      <description>0.前言:为什么要写这篇文章？ 事务隔离和mvcc的重要性 不同于很多MySQL的原理，只需要DBA掌握，事务对于研发人员也是必须掌握的知识点和原理。并发程度越高，数据库里的锁和事务越明显，越重要。所以：数据库事务和mvcc是研发和DBA都要熟练掌握的另一方面的原因是现有的资料对mvcc写得不够直观 现有的对mvcc原理的讲解停留在画图阶段，我觉得光画图还不够，要实打实的一个字节一个字节的看MySQL真实的数据文件是怎么实现的。利用自研的MySQL数据文件分析工具（ 参考：innodb存储格式 )。可以很直观的把mvcc实现的底层逻辑给展示出来。
 以下两篇文章，可以协助你更好的理解本章节的内容
  MySQL行格式(compact,redundant,dynamic,compressed) ) MySQ事务id:trx_id )  环境准备  MySQL版本:8.0.22 事务隔离级别:REPEATABLE-READ (默认隔离级别)  建一张表dboopuser并insert几条数据 drop table dboopuser; create table dboopuser( userid int unsigned not null primary key , age smallint unsigned not null default 0, username varchar(20) not null default &#39;&#39;, userimg varchar(255) not null default &#39;&#39; ) ENGINE=InnoDB COMMENT=&#39;测试user表--用于mvcc测试20220727&#39; ; insert into dboopuser(userid,age,username,userimg) values(9527,25,&#39;cccccccccc&#39;,&#39;http://www.dboop.com/img/user/2002_innodbtrx_527.jpg&#39;); insert into dboopuser(userid,age,username,userimg) values(9528,15,&#39;dddddddddddddd&#39;,&#39;http://www.dboop.com/img/user/2002_innodbtrx_528.jpg&#39;); insert into dboopuser(userid,age,username,userimg) values(9529,25,&#39;eeeeeeeeeeeeeeeee&#39;,&#39;http://www.</description>
    </item>
    
    <item>
      <title>show engine innodb status 工具化实现</title>
      <link>/mysql/mysql%E7%9A%84showinnodbstatus/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84showinnodbstatus/</guid>
      <description>为什么要写这个工具 当MySQL出现性能问题时，DBA经常得去innodb status ，
但是innodb status的输出信息非常丰富也很复杂。滚了几个屏幕的指标，像这样的得翻好几页，几百行的结果。
 哪些是重要的指标 指标具体代表什么意思 指标的值是否正常  非常考验DBA的眼力。
考虑到以上的不方便，写了个小脚本把这些指标提取出来，并将指标对应的中文意思和合理取值范围做了详细的备注。
实现思路  当用户选中MySQL实例，并点击show engine innnodb statutus按钮时 后台进程去该实例执行 show engine innnodb statutus 语句 返回结果做正则筛选，将各种指标和值保存在一个字典中 提前准备好一个指标的字典，字典里记了该值的中文说明和取值范围（取值范围这个还没做好） 两个字典一合并，输出一个分好类的可视化结果  指标提取和定义 脚本内容是定义了一个数据字典去翻译这些指标
{ &amp;quot;background_thread&amp;quot;:(&amp;quot;后台进程:除掉用户请求的活动会话，MySQL后台进程也会定时的进行一系列工作。&amp;quot;,[(&amp;quot;master_thread_loops_active&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;后台master线程avtive执行次数合计值&amp;lt;/b&amp;gt;,后台master线程的每次循环时会选择一种状态来执行(active、shutdown、idle),active次数/idle次数 比值越高，代表系统的写操作越繁忙。&amp;quot;), (&amp;quot;master_thread_loops_idle&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;后台master线程idle执行次数合计值&amp;lt;/b&amp;gt;,和上一个指标连起来看,idle次数越高，代表系统的写操作越少。所以该指标值越大，系统写资源越充足&amp;quot;), (&amp;quot;master_thread_log_flush_and_writes&amp;quot;,&amp;quot;Bytes&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;后台master线程刷新redo日志&amp;lt;/b&amp;gt;,定期刷新redo日志，和参数innodb_flush_log_at_timeout控制刷新时间&amp;quot;) ] ) ,&amp;quot;bufferpool_memory&amp;quot;:(&amp;quot;缓冲池:有关已读和已写页面的统计信息。可以从这些数字中获得缓冲池的使用情况。&amp;quot;,[ (&amp;quot;total_large_memory_allocated&amp;quot;,&amp;quot;Bytes&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;分配给InnoDB Buffer Pool的总内存&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;dictionary_memory_allocated&amp;quot;,&amp;quot;Bytes&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;分配给InnoDB数据字典的内存&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;buffer_pool_size&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;分配给IBP的内存，单位pages&amp;lt;/b&amp;gt;,每页16k&amp;quot;) ,(&amp;quot;buffer_pool_hit&amp;quot;,&amp;quot;/1000&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool命中率&amp;lt;/b&amp;gt;每1000次请求有*次命中buffer pool,非常重要&amp;quot;) ,(&amp;quot;free_buffers&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool Free List 总大小，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;database_pages&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool LRU List 总大小，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;old_database_pages&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool old LRU 总大小，单位pages(冷端)&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;modified_db_pages&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool中脏页的数量，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pending_reads&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;等待读入Buffer Pool的页数量，单位pages&amp;lt;/b&amp;gt;,理论上不应该有等待队列&amp;quot;) ,(&amp;quot;pending_writes_lru&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;LRU中buffer中等待被刷的脏页数，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pending_writes_flush_list&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在checkpoint期间要刷新的缓冲池页数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pending_writes_single_page&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在缓冲池中写入挂起的独立页的数目&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_made_young&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;热点页数&amp;lt;/b&amp;gt;,在缓冲池LRU list中年轻的总页数(移动新页面到sublist的头部)&amp;quot;) ,(&amp;quot;pages_made_not_young&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;old sublist中的page数，冷端的page数&amp;lt;/b&amp;gt;,在缓冲池LRU列表中不年轻的页面总数(保留旧页面在sublist中，不改变)&amp;quot;) ,(&amp;quot;pages_made_young_per_sec&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒LRU链中被young的page数&amp;lt;/b&amp;gt;,oungs/s度量标准仅用于old pages，基于对page的访问次数，而不是页的数量。对页进行多次访问都会被计算。如果见到非常低的值，可能需要减小延迟或增加old page LRU list 的比例。增大后，页面需要更长的时间才会移动到尾部，这就增加了再次访问page，从而使他们made young的可能性增大&amp;quot;) ,(&amp;quot;pages_made_non_young_per_sec&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒LRU链中未被young的page数&amp;lt;/b&amp;gt;，可以一定程度上看出库的繁忙程度和命中率,Not young，如果在执行大表扫描时未看到较高的non-young和non-youngs/s，请增加innodb_old_blocks_time。&amp;quot;) ,(&amp;quot;pages_read&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool中读取的page总数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_created&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在bufferpool中创建的page数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_written&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool写入的page数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_read_per_sec&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool中读取的page数/秒&amp;lt;/b&amp;gt;, 比较重要，代表库的繁忙程度&amp;quot;) ,(&amp;quot;pages_created_per_sec&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在bufferpool中创建的page数/秒&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_written_per_sec&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool写入的page数/秒&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_read_ahead&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒平均预读操作次数&amp;lt;/b&amp;gt;k&amp;quot;) ,(&amp;quot;evicted_without_access&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒驱逐的page数&amp;lt;/b&amp;gt;k&amp;quot;) ,(&amp;quot;random_read_ahead&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒钟随机预读操作的次数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;lrn_len&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;LRU的长度&amp;lt;/b&amp;gt;&amp;quot;) ] ) .</description>
    </item>
    
    <item>
      <title>MySQL复制参数_slave_rows_search_algorithms及无主键表的处理</title>
      <link>/mysql/mysql%E5%A4%8D%E5%88%B6%E5%8F%82%E6%95%B0slave_rows_search_algorithms/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%A4%8D%E5%88%B6%E5%8F%82%E6%95%B0slave_rows_search_algorithms/</guid>
      <description>0.起因 线上MySQL实例，今天报大量延时，且卡住不动。（表现为seconds_behind_master不断上涨，从库gtid不动）
同时二级从库有复制SQL进程报错
[ERROR] [MY-010584] [Repl] Slave SQL for channel &#39;&#39;: Could not execute Update_rows event on table 「表名」; Can&#39;t find record in &#39;「表名」&#39;, Error_code: 1032; handler error HA_ERR_END_OF_FILE; the event&#39;s master log mysql-bin.000****, end_log_pos *******, Error_code: MY-001032 1.排查问题 排查问题时
 确认该实例上的从库不提供线上实时业务访问（业务可以接受延时）。不需要做从库切流量动作 先是看了一下从库的多线程复制是database级的，开启多线程复制到logical_clock ,问题并没有恢复       set global slave_parallel_type=&amp;lsquo;logical_clock&amp;rsquo;; set global slave_parallel_workers=4; start slave sql_thread;```
 排除掉是线程数不够的原因 发现processlist中是在等Applying batch of row changes (update)  确定是卡在sql进程，再看relaylog确实持续增长800M(表示该实例写入不频繁)   解析relaylog 发现是普通的update语句大约有8000次左右  这个量级的update且是row模式，理论1分钟内就追上了。   查看锁datalocks，发现有大量的行数50几万，都是同一个表的  slave的sql进程不应该有这个量级的行锁。   查看表结构发现这个表是无主键的表，里面大约有50几万条记录，无主键无索引  2.</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(三):UndoLog</title>
      <link>/mysql/mysql%E7%9A%84undo%E6%97%A5%E5%BF%97/</link>
      <pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84undo%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续：
 MySQL的7种日志(一):概况
  MySQL的7种日志(二):RedoLog
 1.什么是undolog  undo：撤销或取消，以撤销/回滚操作为目的，返回指定某个状态的操作。 undolog：数据库事务开始之前，会将要修改的记录存放到 Undo 日志里，当事务回滚时或者数据库崩溃时，可以利用Undo日志，撤销未提交事务对数据库产生的影响。 undolog在事务开始前产生；事务在提交时，并不会立刻删除undolog，innodb会将该事务对应的 undo log 放入到删除列表中，后面会通过后台线程purge thread进行回收处理。 undolog属于逻辑日志，记录一个变化过程。例如执行一个delete，undo log会记录一个insert；执行一个update，undo log会记录一个相反的update。  2.undolog的作用  实现事务的原子性 当事务回滚时或者数据库崩溃时，利用Undo日志，撤销未提交事务对数据库产生的影响。事务处理过程中，如果出现了错误或者用户执行了 ROLLBACK 语句，MySQL 可以利用 Undo Log 中的备份将数据恢复到事务开始之前的状态。 实现多版本并发控制（MVCC） Undo Log 在 MySQL InnoDB 存储引擎中用来实现多版本并发控制。事务未提交之前，Undo Log 保存了未提交之前的版本数据，Undo Log 中的数据可作为数据旧版本快照 供其他并发事务进行快照读。（构建read view视图）  3.undolog的存储 3.1 物理存储位置 找到具体存放的位置 MySQL5.6.3 之前的版本undolog存储在系统共享表空间里，后续的版本推荐存话在单独的文件中
mysql&amp;gt; show global variables like &#39;%undo%&#39;; +--------------------------+-------------------------+ | Variable_name | Value | +--------------------------+-------------------------+ | innodb_max_undo_log_size | 1073741824 | | innodb_undo_directory | /data/mysql3306/innolog | | innodb_undo_log_encrypt | OFF | | innodb_undo_log_truncate | ON | | innodb_undo_tablespaces | 2 | +--------------------------+-------------------------+ 5 rows in set (0.</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(二):RedoLog</title>
      <link>/mysql/mysql%E7%9A%84redo%E6%97%A5%E5%BF%97/</link>
      <pubDate>Fri, 13 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84redo%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续上一篇： MySQL的7种日志(一):概况
上一篇我准备写MySQL日志还是2个月前的事，这两个月里生活发生了天翻地覆的变化，都没时间更新。
昨天跟朋友聊天立了flag说今天要续写一篇
于是中午吃饭时在纸上画了一个流程图。来介绍下MySQL里的RedoLog
1.几个问题 redolog和binlog一样记录的是数据修改后的记录。区别是什么，存在的意义是什么？  如果不要redolog，直接修改数据行不行？ 答：可以，但是随机读写性能差 先写redolog还是先改数据？答：先写内存里的数据，再写redolog，再写binlog，再写磁盘里的数据 先写redolog还是先写binlog? 答：先写redolog,再写binlog 如果写完redolog，还没来得写binlog时就停电了,怎么办？答：修改不要了，从undolog中回滚数据 如果写完redolog,binlog时还没来得数据落盘就停电了,怎么办？答：重做redolog，提交数据。修改有效  redolog和undolog的关系  答：redolog用来恢复丢失数据（恢复到最后一次提交位置）也称之为前滚操作，undolog是用来回滚到之前的版本，称之为回滚操作  relaylog的作用  答：redolog是用来做崩溃恢复使用的，这种崩溃恢复不需要我们人为的参与，MySQL自己内部自己实现了这种崩溃恢复的功能，我们只管享受这种功能给我们带来的服务即可，这种服务给我们的感受就是：MySQL数据库异常宕机的时候，重启服务之后，数据库中之前提交的记录都不会丢失数据仍然可以正常恢复，不管这种提交的记录是否已经更到具体的表所对应的磁盘page也中。  2.修改数据的流程  当我们要更新一条数据时，比如有一条SQL update userinfo set name=&#39;dboop&#39; where name=&#39;张三&#39;; 最直接的方法：从磁盘上找到对应的数据库文件，把它修改完存放到磁盘中。  方法是可以的，很多简单的程序修改文件也是用的方法，但是性能差。   而数据库中一般会有以下几种方式来写入数据修改  按页组织数据，一些关联近的数据存放在一个页中，MySQL中默认一页是16k 读取和修改数据都是需要先把页加载到内存中,MySQL是放到innodb_buffer_pool中 先改内存，再合适的时候再写入磁盘 先改日志再改数据 日志也是先写内存中的日志buffer，再合适的时候刷入磁盘    下图是简化版的一个数据修改，真实的流程比这复杂很多，这里的数据修改不只是update，按页组织的insert/update/delete操作都是对页修改
3.Redolog在数据库意外崩溃时的作用 当故障发生时，数据库意外当机，有部分内存中已修改的页（脏页）没来得及刷新到磁盘里。
在写入redo log时，会顺便记录XID，即当前事务id。在写入binlog时，也会写入XID。
如果在写入redo log之前崩溃，那么此时redo log与binlog中都没有，是一致的情况，崩溃也无所谓。
如果在写入redo log prepare阶段后立马崩溃，之后会在崩恢复时，由于redo log没有被标记为commit。于是拿着redo log中的XID去binlog中查找，此时肯定是找不到的，那么执行回滚操作。
如果在写入binlog后立马崩溃，在恢复时，由redo log中的XID可以找到对应的binlog，这个时候直接提交即可。
总的来说，在崩溃恢复后，只要redo log不是处于commit阶段，那么就拿着redo log中的XID去binlog中寻找，找得到就提交，否则就回滚。
在这样的机制下，两阶段提交能在崩溃恢复时，能够对提交中断的事务进行补偿，来确保redo log与binlog的数据一致性
4.Redolog的刷盘 4.</description>
    </item>
    
    <item>
      <title>MySQL中show processlist详细解释</title>
      <link>/mysql/mysql%E4%B8%ADshow-processlist%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E4%B8%ADshow-processlist%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A/</guid>
      <description>0.概况 MySQL执行SHOW PROCESSLIST命令后，显示的各个字段的含义如下所示：
   列 说明     id 一个标识，要kill一个语句的时候有用。   user 显示当前用户   host 这条语句是从那个服务器的哪个端口上发出的，可以用来追踪出问题语句的用户   db 当前连接连接使用哪个数据库   command 显示当前连接的执行的命令，一般就是休眠（sleep），查询（query），连接（connect）   time 状态持续的时间，单位是秒。   state 当前连接的sql语句的状态，很重要的列   info sql语句。    1.详细说明 id 一个标识，要kill一个语句的时候有用。
user 显示当前用户
host 这条语句是从那个服务器的哪个端口上发出的，可以用来追踪出问题语句的用户
db 当前连接连接使用哪个数据库
command (重要) 共有以下几种状态
 Command 说明 Sleep 休眠，但是连接保持，可以通过设置超时时间的变量，使得连接在超时时间之后，断开连接。 Query 查询。Query的范围包括Create、Insert、Select、Delete、Drop语句类型。 Connect 连接。建立连接过程。  time 状态持续的时间，单位是秒
state (重要)  Checking table 正在检查数据表。 Closing tables 正在将表中修改的数据刷新到磁盘中，同时正在关闭已经用完的表。 Connect Out 复制从服务器正在连接主服务器 Copying to tmp table on disk 由于临时结果集大于 tmp_table_size，正在将临时表从内存存储转为磁盘存储以此节省内存。 Creating tmp table 正在创建临时表以存放部分查询结果。 deleting from main table 服务器正在执行多表删除中的第一部分，刚删除第一个表。 deleting from reference tables 服务器正在执行多表删除中的第二部分，正在删除其他表的记录。 Flushing tables 正在执行 FLUSH TABLES，等待其他线程关闭数据表。 Killed 发送了一个kill请求给某线程，那么这个线程将会检查kill标志位，同时会放弃下一个kill请求。MySQL会在每次的主循环中检查kill标志位，不过有些情况下该线程可能会过一段时间才能死掉。如果该线程被其他线程锁住了，那么kill请求会在锁释放时马上生效。 Locked 被其他查询锁住了。 Sending data 正在处理 SELECT 查询的记录，同时正在把结果发送给客户端。 Sorting for group 正在为 GROUP BY 做排序。 Sorting for order 正在为 ORDER BY 做排序。 Opening tables 正尝试打开一个表。 Removing duplicates 正在执行一个 SELECT DISTINCT 方式的查询，但是MySQL无法在前一个阶段优化掉那些重复的记录。因此，MySQL需要再次去掉重复的记录，然后再把结果发送给客户端。 Reopen table 获得了对一个表的锁，但是必须在表结构修改之后才能获得这个锁。已经释放锁，关闭数据表，正尝试重新打开数据表。 Repair by sorting 修复指令正在排序以创建索引。 Repair with keycache 修复指令正在利用索引缓存一个一个地创建新索引。 Searching rows for update 正在将符合条件的记录找出来以备更新。必须在 UPDATE 要修改相关的记录之前就完成。 Sleeping 正在等待客户端发送新请求。 System lock 正在等待取得一个外部的系统锁。 Upgrading lock INSERT DELAYED 正在尝试取得一个锁表以插入新记录。 Updating 正在搜索匹配的记录，并且修改它们 User Lock 正在等待GET_LOCK()。 Waiting for tables 该线程得到通知，数据表结构已经被修改了，需要重新打开数据表以取得新的结构。然后，为了能的重新打开数据表，必须等到所有其他线程关闭这个表。 waiting for handler insert INSERT DELAYED 已经处理完了所有待处理的插入操作，正在等待新的请求。  info SQL语句</description>
    </item>
    
    <item>
      <title>MySQL复制故障修复_无主键表大事务卡住</title>
      <link>/mysql/mysql%E5%A4%8D%E5%88%B6%E6%95%85%E9%9A%9C%E8%A7%A3%E5%86%B3_%E5%A4%A7%E4%BA%8B%E5%8A%A1%E5%8D%A1%E4%BD%8F/</link>
      <pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%A4%8D%E5%88%B6%E6%95%85%E9%9A%9C%E8%A7%A3%E5%86%B3_%E5%A4%A7%E4%BA%8B%E5%8A%A1%E5%8D%A1%E4%BD%8F/</guid>
      <description>0.故障现象 生产环境MySQL复制报警，现象
 从库复制延时越来越大,gtid一直停留在固定的地方不变 从库的relaylog越来越大，1G以上，但是增长不明显。 从库当前没有业务访问，不存在资源紧张 主库上最近一段时间没有明显的大批量写入  1.原因定位  从上面的现象卡，基本上可以推断是大事务卡住了， 我看的时候法爷已经把relaylog解出来了，也很明显的看到有很多的delete。 根据以上推断我们去主库上查时间点的日志，发现了： 一个SQL是 delete from t1 where c1=&#39;&#39; 删除了65万行数据 于是问题定位：生产环境的windows机器上有同事用navicat删除了线上MySQL的数据。  简单的一个SQL ，但是因为一些原因综合在一起引起了雪崩
 不幸的是：这张表是个没主键的表，导致从库追日志进程卡住，无法正常执行 幸运的是：这些从库没有业务访问，没有造成实际影响  2.安全规范 首先：生产环境的windows机器安装navicat访问数据库这种行为，肯定是不被允许的，
但是因为“历史原因”我们依然有少量同学（不超过10人）有这种特殊需求。
原计划是3月底推动消除的，
经过此事以后，DBA会加快推进禁止在生产环境安装数据库客户端连接数据库这个规范。
有时候就是这样，觉得这个地方可能有风险，我们排个期来解决，通常就会没等到期限就先暴出来了
 问：为什么我们不用限制账号访问来源的方法？ 答：因为一些原因,加ip限制代价太大，且不利于未来的docker虚拟化。
 3.问题修复 共有3个从节点，我和另外两个DBA用了三种不同的方式来修复
方法一：我用的方法，就很暴力的在从库上reset master 再set 跳过这个事务 use db_test; truncate table t1; stop slave ; reset master; set @@GLOBAL.GTID_PURGED=&#39;59939d78-de2d-11eb-ac46-e43d1a074d20:16020676&#39; start slave; 方法二：法爷用了相对温和的方法，模拟一个事务的方法。 use db_test; stop slave; SET gtid_next=&#39;59939d78-de2d-11eb-ac46-e43d1a074d20:16020676&#39; truncate table t1; SET gtid_next=&#39;automatic&#39;; start slave; 我和法爷讨论了一下，相对来说这个是更安全的方法。保证了事务的连续，偷换了一个事务的内容</description>
    </item>
    
    <item>
      <title>MySQL5.7升级到8.0(一):SQL语法变化</title>
      <link>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B41sql%E8%AF%AD%E6%B3%95/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B41sql%E8%AF%AD%E6%B3%95/</guid>
      <description>研发：MySQL5.7升级到8.0(一):SQL语法变化
  DBA：MySQL5.7升级到8.0(二):配置和参数
   Note：这里面是升级到8.0,需要开发人员参与修改部分或注意部分
   不常用 废弃了 GROUP BY 分组的排序 ASC 和 DESC, 存储过程中包含此语法的无法正常执行;
  不常用 最新版可能不支持 &amp;amp;&amp;amp;, ||, ! 的语法, 需要使用标准 SQL 的 AND, OR, NOT 进行替换;
  不常用 外键的名字在整个 schema 中必须唯一;
  常用 支持公共表表达式cte, 窗口函数 不再支持5.6，5.7 的土方法实现递规这种写法废了！不能再用了
 SELECT * FROM(SELECT @rn:= CASE WHEN @id = id THEN @rn + 1 ELSE 1 END AS rownum,@id:= id as id, volume, dateFROM(SELECT * from table_001 WHERE fdate &amp;lt;= &#39;2022-02-16&#39; ORDER BY id, date DESC) a ,(SELECT @rn=0, @id=0) b )aWHERE rownum &amp;lt;= 5   不常用 支持备份锁(backup lock)</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(一):概况</title>
      <link>/mysql/mysql7%E7%A7%8D%E6%97%A5%E5%BF%97/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql7%E7%A7%8D%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 和其他关系型数据库一样
MySQL也重度依靠数据库日志来完成一系统的功能。
总结了以下7种重要的日志。今天先简单给这7兄弟做个整体介绍。
接下来会分三个章节分别说清楚（binlog/redolog/undolog 这三位不省心的兄弟在做什么).
1.日志的分类 Binlog:  MySQL最重要的日志（没有之一），记录了所有的DDL和DML语句(除了数据查询语句select、show等)，以事件形式记录 DBA依赖它做：高可用方案，异构数据迁移，备份和恢复，误更新回滚。。。。等等 ，是整个MySQL的灵魂 研发/大数据人员依赖Binlog做数据订阅，数据同步  relaylog:  它是依赖于binlog的日志，格式也和binlog一样。 是MySQL复制进程把“别的实例的binlog&amp;quot;复制到本地后，就叫做relaylog. 作用是为了MySQL高可用复制服务的一种日志。  Slowlog:  慢查询日志用来记录在MySQL中响应时间超过阀值的语句，则会被记录到慢查询日志中。由long_query_time参数控制，默认值10秒， 一般线上环境，我们设置为:0.2秒 或0.5秒 两种标准 一般DBA通常会用脚本将日志收集归类，分析后对部分规则产生报警。 这个日志是文本类型的，打开就能看到，比较简单，很容易理解，也很有用  genlog:  一般轻易不开启，开启以后，会将所有经过的SQL都记录到日志里，非常费资源。 定位奇怪的问题时会用到。审计的时候也能用到。 不建议打开，定位问题后，要及时关闭  errorlog:  数据库产生warning,error时会打印的日志，实例启动失败了，或者实例崩溃了必看的日志。平时做好监控。建议开启死锁print，在errorlog中也能看到死锁信息。  redolog：  可能是最难理解的一个日志了，不同于上面的那些日志，redolog是innodb存储引擎的日志，不是MySQL自身的日志 redolog经常会和binlog/undolog搞混 记住最重要的一点：redolog是为了数据库突然关机或崩溃的时候用的。它的作用是：为了不丢失修改。 redolog通常是物理日志，记录的是数据页的物理修改（区别于binlog的逻辑修改)，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页(恢复数据页，且只能恢复到最后一次提交的位置)。  undolog:  undo和redonlog一样也是是innodb存储引擎的日志，用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。 它的作用除了和redolog一起保证数据库突然关机或崩溃的时候，数据不丢失，不混乱。它还是MVCC事务特性的重要组成部分。  小结： 简单介绍完7种日志，其中的三个日志（binlog/redolog/undolog) 涉及知识点非常多，会分别写一篇，慢慢聊。
&amp;ndash; done</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（四）:性能测试和改进</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B44/</link>
      <pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B44/</guid>
      <description>续： 从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》 从零写一个兼容MySQL/Oracle的Proxy中件间（二）:SQL捕获和改写 从零写一个兼容MySQL/Oracle的Proxy中件间（三）:MySQL协议捕获和转发
1.过去的三个文章我们实现了以下功能]  Oracle登录捕获：捕获了Oracle通信协议中的用户登录包 Oracle用户解析：抓到了用户传用户名和密码的内容（密码是加密串） SQL请求包：同时通过对比，确定了用户发送SQL请求的通信包 OracleSQL日志：分析这些包，把SQL语句拿出来，记到日志里。 OracleSQL改写：用户发起的SQL 经过中间层改写到了服务端收到的是另一个SQL执行返回结果。 MySQL兼容：增加配置文件，使中间件可以支持两种数据库 MySQL协议解析：将经过proxy的MySQL包里的SQL语句解析出来，记录到日志  在没更新的这几天里我又偷偷完成了配置变更等小功能。现在中件间其实已经在理论上可以发布使用了
在投入使用前，在测试环境对这个半成品的中件间做了些基准测试。
在测试环境上生成了5张表，每张表200万行数据，对其进行直连和proxy模式压测。
以下是测试报告： 结论是：加了Proxy，性能下降了14% ，在情理之中，一般的SQL中间层因为多了层中转，响应时间会降低20ms左右。tps/qps在不做连接池的情况下会下降10%。 分析性能下降的原因：
因为在proxy存把经过的网络包都拆开来分析其中的内容，且把SQL语句存在日志里，这些步骤是比较费资源和时间的。
为了提升Proxy性能，降低中间层的性能影响，我们加了个配置参数
cat /data/proxy/conf/proxy3308.cnf [basic] logfile = /data/proxy/log/3308.log daemon = true [proxy] proxytype = mysql bind = 0.0.0.0:3308 server = 127.0.0.2:3308 isssl = false iscatchquery = false #增加是否“拆包” false时，Proxy进入高性能模式 iscatchlogin = false maxsquerylsize = 16384 当 iscatchquery=false时，Proxy进入高性能模式
if Iscatchquery { #只有iscatchquery为true时才解析包。 switch ProxyType { case &amp;quot;mysql&amp;quot;: log.Printf(&amp;quot;mysql:sqlPipeMySQL\n&amp;quot;) sqlPipeMySQL(srcCon, dstCon) case &amp;quot;oracle&amp;quot;: log.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（三）:MySQL协议捕获和转发</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B43/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B43/</guid>
      <description>续： 从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》 从零写一个兼容MySQL/Oracle的Proxy中件间（二）:SQL捕获和改写
1.过去的两天我们实现了以下功能]  Oracle登录捕获：捕获了Oracle通信协议中的用户登录包 Oracle用户解析：抓到了用户传用户名和密码的内容（密码是加密串） SQL请求包：同时通过对比，确定了用户发送SQL请求的通信包 OracleSQL日志：分析这些包，把SQL语句拿出来，记到日志里。 OracleSQL改写：用户发起的SQL 经过中间层改写到了服务端收到的是另一个SQL执行返回结果。   MySQL兼容：增加配置文件，使中件间可以支持两种数据库 MySQL协议解析：将经过proxy的MySQL包里的SQL语句解析出来，记录到日志  开始动手：
步骤一：中件间可以同时支持MySQL和Oracle 中件间的配置应该放在哪，理论上是想放在MySQL或zk里，当配置有变更的时候，中件间获得变更，但这个实现有点麻烦，可能得写好久，就先一个本地的配置文件
准备一个配置文件
proxy] proxytype = mysql bind = 0.0.0.0:1106 server = 10.26.*.*:3307 isssl = false iscatchquery = true iscatchlogin = false maxsquerysize = 4096 [proxybak] #proxytype = oracle #bind = 0.0.0.0:1106 #server = 10.26.*.*:1521 #isssl = false #iscatchquery = true #iscatchlogin = false #maxsquerylsize = 4096 然后在通信进程中收到包时处理
func (t *Proxy) pipeSend(dstCon, srcCon *Conn, chSend chan int64) { defer pipeClose(dstCon) switch ProxyType { case &amp;quot;mysql&amp;quot;: log.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（二）:SQL捕获和改写</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B42/</link>
      <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B42/</guid>
      <description>续上一篇： 从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》
0.前言 昨天的文字里写开发这个中间件的原由和要解决的问题，有朋友留言
网上有现成的开源中间件为啥不用。
 答：网上有很多MySQL的中件间，Oralce目前还没有可以免费使用的中件间. 这可能就是开源和闭源的差别。
 Oracle自带的功能已经可以实现想要的功能（高可用/审计日志）
 答：
 Oracle官方的高可用方案RAC，无疑是非常非常非常优秀的,但我们现有的硬件不支持做跨机房RAC,以及我们迁移时需要proxy中间层来降低业务中断时间。 Oracle的审计日志太笨重/不支持慢日志/不支持SQL黑名单。   1.昨天我们实现了以下功能]  捕获了Oracle通信协议中的用户登录包 抓到了用户传用户名和密码的内容（密码是加密串） 同时通过对比，确定了用户发送SQL请求的通信包   SQL日志：分析这些包，把SQL语句拿出来，记到日志里。 SQL改写：用户发起的SQL 经过中间层改写到了服务端收到的是另一个SQL执行返回结果。  开始动手：
步骤一：从Oracle通信包中分解出SQL语句 已知有以下两种head的包是在传递SQL
0x1 0xf 0x0 0x0 0x6 0x0 0x0 0x0 0x0 0x0 0x11 0x6b 0x4 0xa5 0x10 0x0 0x0 0x35 0x1c 0x0 0x0 0x1 0x0 0x0 0x0 0x3 0x5e 0x5 0x61 0x80 0x0 0x0 0x0 0x0 0x0 0x0 0xfe 0xff 0xff 0xff 0x1 0x0 0x0 0x0 0x6 0x0 0x0 0x0 0x0 0x0 0x3 0x5e 0x6 0x61 0x80 0x0 0x0 0x0 0x0 0x0 0x0 0xfe 0xff 0xff 0xff 0xff 0xff 0xff 0xff 0x24 0x0 0x0 0x0 0xfe 0xff 0xff 0xff 0xff 0xff 0xff 1.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B41/</link>
      <pubDate>Wed, 05 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B41/</guid>
      <description>0.前言 MySQL由于开源的原因，有各式各样的中件间Proxy ，极大的丰富了做高可用或迁移的方案，习惯了MySQL生态圈的灵活和便利，Oracle官方不开源代码和协议，没有中间件proxy，显得很笨重。
比如以下的方案就会很不好办：
 实时抓取Oralce的访问SQL日志 慢日志捕获和收集 高可用中件间Proxy在故障时自动切换 SQL访问黑名单。  基于以上的一些困难，打算自己从头写一个兼容MySQL/Oracle的中件间，希望从中件间层同时支持两种数据库。方便我们做数据库的高可用管理和从Oracle到MySQL的迁移。
这个计划是在年前的2021年最后一次组内会议上提出来的构想。元旦放假期间我就一直在想这事怎么搞
问题的难点在于：Oracle的client/server端通信没有文档的说明，没人能说清楚Oracle是怎么交互的。
这两天用最原始的方法抓包，一个包一个包的去看，找到包的规律，分析它的通信协议。竟然发现这个方法可行
1.步骤 1.写一个Python脚本去连接（192.168.1.1:1521）上的Oracle  #!/usr/bin/env python ## coding: utf-8 import cx_Oracle conn = cx_Oracle.connect(&#39;dboopreader/dbooppassword@192.168.1.1:1521/testdb&#39;) print(&amp;quot;连接成功&amp;quot;) conn.close() print(&amp;quot;连接关闭&amp;quot;) 通过wireshark抓包，发现一次简单的连接，有38个通信包。
2.捕获这些包，发现它的规律 挨个点开这些包，发现了一些有用的信息，然后发现wireshark的包看起来不方便， 本地模拟一个端口1522端口，劫持这些请求，打印出来，得到如下这种的tcp包
抓到:127.0.0.1到192.168.1.1的包 二进制展示如下: 0.0x7 0xaf 0x0 0x0 0x6 0x0 0x0 0x0 0x0 0x0 0x2 0x54 0x3 0x54 0x3 0x3 0x2a 0x6 0x1 0x1 20.0x1 0x6f 0x1 0x1 0xc 0x1 0x1 0x1 0x1 0x1 0x1 0x1 0x7f 0xff 0x3 0xe 0x3 0x3 0x1 0x1 40.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_AlterTable</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_altertable/</link>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_altertable/</guid>
      <description>速记  当修改表结构的时候有三种选项: ALTER TABLE t1 ALTER COLUMN ... ALTER TABLE t1 CHANGE COLUMN ... ALTER TABLE t1 MODIFY COLUMN ... 容易记混 一般我们这样记 Change column 无所不能 Modify column 不能给列改名,其他都行 Alter Column 最弱,改点默认值什么的(但是他也有好处,非常快和安全)  增  增加列(单列)  ALTER TABLE t1 ADD col-name col-type comment &#39;xxx&#39;;  增加列(多列)  ALTER TABLE t1 ADD col-name col-type comment &#39;xxx&#39;, ADD col-name col-type(col-length) comment &#39;xxx&#39;;  增加表字段并指明字段放置为第一列  ALTER TABLE t1 add col-name col-type COMMENT &#39;sss&#39; FIRST;  增加表字段并指明字段放置为特定列后面  ALTER TABLE t1 add col-name col-type after col-name-1; 删  删除列  ALTER TABLE t1 DROP col-name;  删除表中主键  Alter TABLE t1】 drop primary key 改  修改字段类型  - 使用MODIFY修改字段类型 ALTER TABLE t1 modify column col-name col-type; - 使用CHANGE修改字段类型 ALTER TABLE t1 change col-name col-name col-type;  修改列名  使用CHANGE修改字段名称 ALTER TABLE t1 change old-col-name new-col-name col-type;  修改表名  - 重命名表1 ALTER TABLE t1 RENAME 【表新名字】 - 重命名表2 RENAME TABLE t1 to new-table-name;  修改默认值  - 为字段设置NULL和DEFAULT ALTER TABLE t1 modify col-name col-type not null default 100; - 修改字段的默认值 ALTER TABLE t1 alter col-name set default 10000; -字段删除默认值 ALTER TABLE t1 alter col-name drop default; </description>
    </item>
    
    <item>
      <title>MySQL常用脚本_binlog解析和回滚</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_binlog/</link>
      <pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_binlog/</guid>
      <description>解析binlog 方法1:mysqlbinlog mysqlbinlog --no-defaults --base64-output=decode-rows -vv --database=dboop --start-datetime=&#39;2020-10-11 00:00:00&#39; --stop-datetime=&#39;2020-10-11 15:00:00&#39; mysql-bin.000075 &amp;gt;75.sql 常用参数 database：只列出该数据库下的binlog数据，但无法过滤由触发器执行的SQL。 base64-output=decode-rows -vv：显示具体 SQL 语句。 skip-gtids=true：忽略 GTID 显示。  输出结果  # at 20001 #201011 12:04:09 server id 1 end_log_pos 20094 CRC32 0x2b305ac Query thread_id=53 exec_time=0 error_code=0 SET TIMESTAMP=1651011012/*!*/; BEGIN /*!*/; 上面输出包括信息：
 position: 位于文件中的位置，即第一行的（# at 20001）,说明该事件记录从文件第20001个字节开始 timestamp: 事件发生的时间戳，即第二行的（#201011 12:04:09） server id: 服务器标识（1） end_log_pos 表示下一个事件开始的位置（即当前事件的结束位置+1） thread_id: 执行该事件的线程id （thread_id=53） exec_time: 事件执行的花费时间 error_code: 错误码，0意味着没有发生错误 type:事件类型Query  方法2:my2sql wget https://www.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_故障定位</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D/</guid>
      <description>当前正在运行的SQL select id,user,db,info,Command,Time,State from information_schema.processlist where info is not null and user not in (&#39;dba&#39;,&#39;repl&#39;) order by time desc limit 50; -- 杀连接 select concat(&#39;kill &#39;,id,&#39;;&#39;) as ids from information_schema.processlist where time&amp;gt;50 and info is not null and user like &#39;%&#39; and db like &#39;%&#39; order by time desc ; -- 杀连接shell mysqlw -h 127.0.0.1 -P 3306 -e &amp;quot;select concat(&#39;kill &#39;,id,&#39;;&#39;) as ids from information_schema.processlist where db like &#39;dboop%&#39; and user like &#39;%&#39; &amp;quot; &amp;gt;&amp;gt;3306kill.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_mysqldump</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_mysqldump/</link>
      <pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_mysqldump/</guid>
      <description>如果用loginpath的可以用
  mysqldump --login-path=dba 代替 mysqldump -uroot -p123456 常用命令 备份整个实例(dump全实例) mysqldump -uroot -p123456 -h127.0.0.1 -P3306 --single-transaction --column-statistics=0 --skip_add_locks --skip-lock-tables --master-data=2 -A | gzip &amp;gt; /data/mysqlbackup/dboop_dump`date &#39;+%m-%d-%Y&#39;`.sql.gz 备份实例中的用户库(用于实例迁移或升级) mysql -uroot -p123456 -h127.0.0.1 -P3306 -e &amp;quot;show databases&amp;quot; |grep -Ev &amp;quot;Database|information_schema|mysql|performance_schema&amp;quot; | xargs mysqldump -uroot -p123456 -h127.0.0.1 -P3306 --single-transaction --column-statistics=0 --skip_add_locks --skip-lock-tables --master-data=2 --databases &amp;gt; /data/mysqlbackup/dboop_dump0401.sql -- 此时mysql.user用户也没有迁移过来,如果需要迁移用户,参考: https://www.dboop.com/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E7%94%A8%E6%88%B7%E7%9B%B8%E5%85%B3/ 导出db1、db2两个数据库的所有数据 mysqldump -uroot -p123456 --set-gtid-purged=OFF --skip_add_locks --skip-lock-tables --databases db1 db2 &amp;gt; /data/mysqlbackup/dboop_dump0401.sql 导出db1中的a1、a2表 mysqldump -uroot -p123456 --set-gtid-purged=OFF --skip_add_locks --skip-lock-tables --databases db1 --tables a1 a2 &amp;gt; /data/mysqlbackup/dboop_dump0401.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_用户相关</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E7%94%A8%E6%88%B7%E7%9B%B8%E5%85%B3/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E7%94%A8%E6%88%B7%E7%9B%B8%E5%85%B3/</guid>
      <description>生成随机密码 select substring(md5(rand()), 1, 15); select left(replace(uuid(), &#39;-&#39;, &#39;.&#39;),15); 创建用户及赋权 MySQL5.6及以前 grant select on 库名.* to `用户名`@`主机名` identified by &#39;密码&#39;; MySQL5.7+ create user `用户名`@`主机名` identified by &#39;密码&#39;; grant select on 库名.* to `用户名`@`主机名`; MySQL8.0 create user `用户名`@`主机名`identified with mysql_native_password by &#39;密码&#39;; GRANT select on 库名.* TO `用户名`@`主机名`; 常用语句 -- 创建一个管理员帐号 create user &#39;dba&#39;@&#39;%&#39; IDENTIFIED BY &#39;********&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;dba&#39;@&#39;%&#39; WITH GRANT OPTION; -- 创建一个复制帐号 create user repl@&#39;%&#39; identified with mysql_native_password by &#39;********&#39;; GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_免密登录login-path设置</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95loginpath/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95loginpath/</guid>
      <description>保存账号信息 mysql_config_editor set --login-path=dba --user=root --host=127.0.0.1 --password 其中可配置项
-h,–host=name 添加host到登陆文件中 -G,–login-path=name 在登录文件中为login path添加名字（默认为client） -p,–password 在登陆文件中添加密码（该密码会被mysql_config_editor自动加密） -u,–user 添加用户名到登陆文件中 -S,–socket=name 添加sock文件路径到登陆文件中 -P,–port=name 添加登陆端口到登陆文件中 查看配置 mysql_config_editor print -all 删除配置
mysql_config_editor remove --login-path=dba 登陆数据库
mysql --login-path=dba </description>
    </item>
    
    <item>
      <title>MySQL的锁:innodb锁粒度详解</title>
      <link>/mysql/mysql%E9%94%81_innodb%E9%94%81%E7%B2%92%E5%BA%A6/</link>
      <pubDate>Sat, 13 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E9%94%81_innodb%E9%94%81%E7%B2%92%E5%BA%A6/</guid>
      <description>锁定义 lock_rec_not_gap锁  Record Locks
 A record lock is a lock on an index record. For example, SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; prevents any other transaction from inserting, updating, or deleting rows where the value of t.c1 is 10. Record locks always lock index records, even if a table is defined with no indexes. For such cases, InnoDB creates a hidden clustered index and uses this index for record locking.</description>
    </item>
    
    <item>
      <title>MySQL binlog 问答</title>
      <link>/dba/mysqlbinlogquestion/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/dba/mysqlbinlogquestion/</guid>
      <description>binlog 是什么?  MySQL 的二进制日志 ,不是纯文本类的 记录的是数据库变动的日志(insert,update,delete,create,replace,grant &amp;hellip;.) 不包括 select,set 等  binlog 重要吗？是不是一定要开？  几乎是最重要的MySQL日志，严谨点说是最重要的之一 如果没有特殊情况，一定要开！为什么？你再往后看&amp;hellip;)  binlog 有什么作用？  高可用同步，经常用它来同步主库和从库的数据。 它本身就是记录了数据库变化的日志，放在那让你看也是它作为“日志”的作用 恢复指定时间点的数据, 想把数据库恢复到指定时间眯，得靠它 回滚数据 ，误删除数据时用到 审计 变更捕获到其他平台(kafka,es或其他数据库)  binlog 怎么开启，放在哪，怎么存储的？ binlog组提交  MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数 引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程： flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）； sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）； commit 阶段：各个事务按顺序做 InnoDB commit 操作； 上面的每个阶段都有一个队列，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。 对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率。  主从复制是怎么实现？ MySQL 集群的主从复制过程梳理成 3 个阶段：  写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引擎中的数据。  具体详细过程如下：  MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。  24、什么时候 binlog cache 会写到 binlog 文件？ 在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。</description>
    </item>
    
    <item>
      <title>MySQL的innodb中Next-Key锁的解析</title>
      <link>/dba/innodb_lock_2020/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/dba/innodb_lock_2020/</guid>
      <description>去年的某个时候，一个朋友在微信上问我MySQL间隙锁的案例，当时正在赶一个项目，没来得及看那个CASE，后来找不到了。昨天看到这篇jahfer写的博客: https://jahfer.com/posts/innodb-locks/ 觉得在介绍Next-Key锁的这方面很有创意的使用了自制的动画（非常简陋的动画 没啥用，我换成了截图做标记了)，不管是创意还是内容都值得一看
   作者:jahfer 翻译:51ak   &amp;ndash;翻译全文如下：
最近，我在调试MySQL高并发问题时有机会深入理解了InnoDB的锁定机制，这篇文章是我学习innodb锁行为的一个总结。
0.概念介绍 InnoDB只有几个锁定概念,但是它们的使用和行为取决于当前连接正在使用的事务隔离级别
 …the isolation level is the setting that fine-tunes the balance between performance and reliability, consistency, and reproducibility of results when multiple transactions are making changes and performing queries at the same time. 引自MySQL官方文档 https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html
 InnoDB一共有4种隔离级别（按最严格到最宽松的顺序）
 SERIALIZABLE 序列化 REPEATABLE READ (default) 可重复读 READ COMMITTED 读已提交 READ UNCOMMITTED 读未提交  每种隔离级别下的锁行为差异非常大，而我们现在只分析前两种隔离级别（SERIALIZABLE，REPEATABLE READ),首先让我们创建一个book 表。</description>
    </item>
    
    <item>
      <title>MySQL的drop/truncate Table影响分析和最佳实践</title>
      <link>/dba/droptable/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/dba/droptable/</guid>
      <description>0.前言 MySQL上直接Drop张大表,会有什么影响，能否直接写个 drop table ; 或者 truncate table ; 甚至是delete * from 如果这张表足够大，比如1亿行记录，drop 的时间需要多久，期间我的MySQL是否能正常访问？
首先明确一点，现在讨论的是要删掉的大表一定是没人访问的表，否则如果这张表仍然还有被高频的访问，你敢直接删那基本上就是茅坑里点灯，找死！ 如果MySQL版本是5.5.23以下，直接DROP一张大表，也是守着茅坑睡觉，离死不远。 好，现在明确了这张表肯定没人访问了，你的MySQL版本也足够新，并不表示你就远离了茅坑，但如果这张表足够大，仍然有被崩到的风险。
大表：我们定义为5000万行以上，或者单表文件大于100G
我们要讨论的是innodb存储引擎,myisam等存储引擎，DROP 表又快又安全
1.drop table 的风险和避免方法 Drop table 要做的主要有3件事：  把硬盘上的这个文件删了 把内存中的这个库已经加载加来的Page删了，腾出空间 把MySQL元数据字典中这张表关联信息删了  可能会引起的风险有3种：  MySQL长时间阻塞其他事务执行，大量请求堆积，实例假死。(锁) 磁盘IO被短时间大量占用，数据库性能明显下降(IO) 内存里的page大量置换，引起线程阻塞，实例假死（内存)  解决和避免的方法3种：  io占用的问题，对这个表建一个硬链，使Drop table 表的时候并没有真的去磁盘上删那个巨大的ibd文件，事后再用truncate的方式慢慢的删除这个文件，如果是SSD盘和卡,drop table后再直接rm文件也没问题 内存和IO占用的问题，升级MySQL版本   MySQL 5.5.23 引入了 lazy drop table 来优化改进了drop 操作影响(改进，改进，并没有说完全消除!!!拐杖敲黑板3次)
  MySQL5.7.8 拆分了AHI共用一个全局的锁结构 btr_search_latch
  MySQL8.0 解决了truncate table 的风险
   道路千万行，低峰第一条。选择低峰时间段，找个夜深人静，月黑风高的时候是更好的选择。  2.</description>
    </item>
    
    <item>
      <title>MySQL的行格式(Compact、Redundant、Dynamic和Compressed)</title>
      <link>/mysql/mysql%E7%9A%84%E8%A1%8C%E6%A0%BC%E5%BC%8Fcompact_redundant_dynamic_compressed/</link>
      <pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E8%A1%8C%E6%A0%BC%E5%BC%8Fcompact_redundant_dynamic_compressed/</guid>
      <description>前言  MySQL的默认存储引擎innodb是按16k大小的page来组织存储数据的 MySQL的*.ibd 数据文件，大小一定是能被16kB整除的 在逻辑上innodb是按btree来组织数据存储的 针对每一行具体的数据，共有4种存储方式可供选择：Compact、Redundant、Dynamic和Compressed 其中：Redundant 已经被淘汰了，不建议使用 Compact/Dynamic/Compressed 用的是同一个原理，只在细节上有点变化，不影响其实现逻辑 所以我们说行格式的时候，就可以从compact格式来分析，后两种是compact格式的变种   以下原理部分，都只说compact行格式。(?因为compact是基础，后两种都是基于它衍生出来的)
 行格式在哪里看，怎么修改行格式 查看 mysql&amp;gt; show table status like &#39;%dbooptest%&#39; \G *************************** 1. row *************************** Name: dbooptest Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 9 Avg_row_length: 1820 Data_length: 16384 Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: 2020-06-10 20:22:49 Update_time: 2020-06-10 20:22:49 Check_time: NULL Collation: utf8mb4_unicode_ci Checksum: NULL Create_options: Comment: 测试 1 row in set (0.</description>
    </item>
    
    <item>
      <title>MySQL的事务id:trx_id</title>
      <link>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1id_trx_id/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1id_trx_id/</guid>
      <description>什么是事务id(trx_id)  可以理解为MySQL官方存储引擎innodb维护的一个全局自增变量:max_trx_id, 一个6字节长度的整数。(max_trx_id如果一直增长，理论上也是有溢出的可能性的，超过2的48次方后，会重新从0开始，这时候会破坏事务的顺序规则) 每当一个事务开始时，需要申请一个新的trx_id值时，就获取max_trx_id的最新值，然后将max_trx_id值加1。  事务id的作用  主要是用来记录事务开始的顺序 会用在各种事务冲突和mvcc中  如何查看事务id 查看当前事务的trx_id  select TRX_ID from INFORMATION_SCHEMA.INNODB_TRX where TRX_MYSQL_THREAD_ID = CONNECTION_ID() 查看当前的事务id列表（活动)  select TRX_ID from INFORMATION_SCHEMA.INNODB_TRX 查看当前的事务id列表（活动+非活动) 看innodb status 的TRANSACTIONS 部分
show engine innodb status \G # 找到这一部分 TRANSACTIONS 部分 TRANSACTIONS Trx id counter 2419 -- 当前最大事务 ID Purge done for trx&#39;s n:o &amp;lt; 2419 undo n:o &amp;lt; 0 state: running but idle History list length 0 LIST OF TRANSACTIONS FOR EACH SESSION: ---TRANSACTION 421658589187480, not started 0 lock struct(s), heap size 1136, 0 row lock(s) ---TRANSACTION 421658589186624, not started 0 lock struct(s), heap size 1136, 0 row lock(s) ---TRANSACTION 421658589185768, not started 0 lock struct(s), heap size 1136, 0 row lock(s) 上面是我搭的测试环境，所以没有活跃事务， 需要注意的是几个事务id都非常大（例：421658589187480） 这个后面会解释说明</description>
    </item>
    
    <item>
      <title>MySQL原理_innodb存储格式详解(二)</title>
      <link>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A32/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A32/</guid>
      <description>MySQL用page页来管理和存储数据文件，这些page页是如何被组织起来的，真实的数据(data,index) 真实是怎么存放在ibd文件和内存中的呢，了解存储格式，将帮助我们更好的理解MySQL是如何工作的，从而更好的理解其他数据库知识点（索引，MVCC,等等），本视频中我将利用Python脚本把这些难以理解的数据页解析出来并图形化展示给大家，望大家知其然而知其所以然
视频较长，分两段录制，录制时音量较小，注意控制下声音大小。
  下面是分析.idb文件的脚本，执行结果的明细部分</description>
    </item>
    
    <item>
      <title>MySQL原理_innodb存储格式详解(一)</title>
      <link>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A3/</guid>
      <description>MySQL用page页来管理和存储数据文件，这些page页是如何被组织起来的，真实的数据(data,index) 真实是怎么存放在ibd文件和内存中的呢，了解存储格式，将帮助我们更好的理解MySQL是如何工作的，从而更好的理解其他数据库知识点（索引，MVCC,等等），本视频中我将利用Python脚本把这些难以理解的数据页解析出来并图形化展示给大家，望大家知其然而知其所以然
视频较长，分两段录制，录制时音量较小，注意控制下声音大小。
  核心问题：MySQL（*innodb)是如何组织，存储表数据的？
innodb单表最大能到多少，为什么？
为什么innodb数据文件的大小始终可以被16384整除？
int 和bigint 差别有多大?
varchar(10) 和varchar(100) 差别有多大？varchar(1000)呢? TEXT 呢?
1页(page)=基本单位，存储和读取的核心，每页大小默认：16k
1区(extend)=64页,64*16k=1M
1组(space)=256区, 256*1M=256M
每个page都有个编号，整型最大 2^32 x 16k = 64T （单表大小上限）
下面是分析.idb文件的脚本，执行结果截图 </description>
    </item>
    
  </channel>
</rss>
