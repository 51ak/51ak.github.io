<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>原创 on Classic</title>
    <link>/tags/%E5%8E%9F%E5%88%9B/</link>
    <description>Recent content in 原创 on Classic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Fri, 03 Jan 2025 00:00:00 +0000</lastBuildDate><atom:link href="/tags/%E5%8E%9F%E5%88%9B/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>有趣的数据_城市排名</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%9B%BD%E5%86%85%E5%9F%8E%E5%B8%82%E6%8E%92%E5%90%8D/</link>
      <pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%9B%BD%E5%86%85%E5%9F%8E%E5%B8%82%E6%8E%92%E5%90%8D/</guid>
      <description>有趣的数据：国内的城市
年快递业务收入城市排名前50名    序号 城 收入（亿元）     1 上海 1212.39   2 广州 595.94   3 深圳 476.25   4 杭州 294.28   5 北京 233.26   6 金华 225.67   7 东莞 204.43   8 苏州 176.27   9 揭阳 120.16   10 成都 120.14   11 佛山 115.21   12 武汉 104.2   13 天津 100.</description>
    </item>
    
    <item>
      <title>做数据同步程序过程的一些零散的笔记</title>
      <link>/book/%E5%81%9A%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%A8%8B%E5%BA%8F%E8%BF%87%E7%A8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%81%9A%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%A8%8B%E5%BA%8F%E8%BF%87%E7%A8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%B0%E5%BD%95/</guid>
      <description> 用python写了一个数据同步工作，过程零散的笔记，记录一下
 不要低估硬件的影响   因为中间硬件资源调整不开，运维同事给了一个sata硬盘凑数，结果慢到另人发指
  数据同步在不同硬盘下的实际运行速度和磁盘io性能：
  sas机械盘 iops：2k, 写入20M/s， io操作占比40%，写入速度1万行/秒
  sata机械盘	iops：0.7k, 写入10M/s， io操作占比100%，写入速度0.4万行/秒
  ssd盘	iops：22k, 写入262M/s， io操作占比70%，写入速度8万行/秒
  一开始就要想到并行  同步一个实例拆 同步一个库，再拆 同步一个表，一开始需要拆到这个级别，等跑起来，还要优化 同步表中的一部分  初始化很重要  表数据初始化的速度和性能非常重要 尤其是程序可能有bug的时候，会反复用到表初始化地步 比起增量同步，全量同步难在如何提速。 线上增量的消费速度，可需要追上就行。这部分单线程就能满足，反而容易 如何将一张上亿的大表，尽可能快的全量抽到另一个实例中，这个非常重要  注意时间和日期格式和中文编码  被oracle 的 -0002-1-1 这样的公元前日期折磨得怀疑人生 不同的数据库类型的日期要求和编码。处理起来会非常麻烦  python的优势  python的胶水特性在做数据同步时非常实用 可以快速的完成试错和让应用跑起来很重要  要有验证逻辑  同步是否成功，数据量对比 是否有延时 数据关键特征是否匹配  要做好规划  如果可能一个实例一个实例的去攻克 如果可能一开始画好程序的主要路径 要充分考虑部分同步出错时的补救方法  </description>
    </item>
    
    <item>
      <title>数据库迁移流程图</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%E6%B5%81%E7%A8%8B%E5%9B%BE/</link>
      <pubDate>Sat, 14 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%E6%B5%81%E7%A8%8B%E5%9B%BE/</guid>
      <description>合集 MySQL: MySQL迁移前 MySQL迁移中 MySQL迁移后 Oracle: Oracle迁移前 Oracle迁移中 Oracle迁移后 Redis: Redis迁移前 Redis迁移中 Redis迁移后 </description>
    </item>
    
    <item>
      <title>当“延迟退休”成为现实，我们该如何面对“老无所依”</title>
      <link>/book/%E4%BB%8E%E5%BB%B6%E8%BF%9F%E9%80%80%E4%BC%91%E5%88%B0%E8%80%81%E6%97%A0%E6%89%80%E4%BE%9D/</link>
      <pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E4%BB%8E%E5%BB%B6%E8%BF%9F%E9%80%80%E4%BC%91%E5%88%B0%E8%80%81%E6%97%A0%E6%89%80%E4%BE%9D/</guid>
      <description>人口老龄化问题  值得纪念的一件事 延迟退休的鞋子落地了 男63岁女58岁 很久很久以前 还是高中的时候 我的一位老师就用课下的时间说过 因为一些政策 中国人口老龄化的问题会随着我们这一代人越来越严重 当时完全听懂了他的逻辑 这一批年轻人老去的时候 会面临很严重的社会问题 再后来携程的梁建章反复在公开场合发表观点 包括三十年后的老人要不养老金减半 要不延迟到七八十岁退休 强调中国人口问题 在很多年轻人找不到工作的时候 就提前在呼吁政策松绑 事实上他的建议是对的 这几年的发展也验证了这些观点 未来10-20年 一个社会的持续发展 一定要有更多的年轻人在推动 否则老人越来越多 人越来越少 社会会失去活力 大多数老年人将会面对老无所依的囧境 这个趋势如此明显 数据不会反转 问题肯定会爆发 下面的这张当前的人口年龄分布图 未来的20-30年内 我们将面对一个巨大的人口持续老龄化过程 大约在2060年左右 60岁以上的老人应该在5亿人左右 20-60岁的劳动人口在4.5亿左右 0-20岁人口在2.5亿 这是非常非常不健康的数据 但是趋势正在往这个方向走 且完全没有改变的机会  应对策略 方案1:延迟退休  为什么要延迟退休？ 让还没老得不能动的老人参与工作 缓解未来的劳动人口不足问题  方案2:降低退休金  社保基金无法支撑未来几年的退休人员支出 这还是赶在经济快速增长的这些年积累基础上的 未来会更夸张 现有的科技不变情况下 未来老年人可支配收入的实际购买力会持续下降  方案3:增加年轻人  鼓励生育 大量增加年轻人 平衡人口比例  方案4:引进外来人口  引进其他发展中国家的劳动人口 来从事体力和服务业 降低我们的人口压力  方案5:新的科学技术飞跃  科技在发展 劳动生产率还在提升 ai和自动化将持续缓解&amp;lt;需要人的工作&amp;gt; 机器人正在更多的参与工厂和日常生活中  小结  我深信中国的人口老龄化问题已经显现 而我的同龄人将会面对这一困局 也一直觉得等我老了的时候 一定会处于“老无所依”的局面 而且还是会孤独终老  </description>
    </item>
    
    <item>
      <title>MySQL的7种日志(五):SlowLog</title>
      <link>/mysql/mysql%E7%9A%84slowlog%E6%97%A5%E5%BF%97/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84slowlog%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续：
 MySQL的7种日志(一):概况 MySQL的7种日志(二):RedoLog MySQL的7种日志(三):UndoLog MySQL的7种日志(四):BinLog MySQL的7种日志(五):SlowLog  1.什么是Slowlog  数据库执行一个SQL时，如果超过了设定值(比如说500毫秒),数据库将此SQL和相关信息记录到日志中，这个日志就是SLowlog，我们也称为慢日志 slowLog的开启，是为了定位和发现慢SQL用的。这一点跟前几篇文章里的redbolog,undolog,binlog等日志不一样  SlowLog的超时时间long_query_time  这个需要特别注意，并不是我们通常理解的一个SQL从开始执行到执行完用了多长时间 事实上MySQL判断一个sql是否要被记到slowlog中，是这样的逻辑： 假设我们设置了超过500毫秒的SQL是慢SQl要记下来，MySQL会这样处理 实际SQL执行消耗的时间- 锁等待消耗时间 如果这个时间&amp;gt;=500毫秒，则记下SlowLog否则不记 这就相当于说开车起点到终点的时间如果超过30分钟就很慢了 但我们说的30分钟不包括路上堵车和等红绿灯的时间  # @long_query_time ：我们设置了慢日志记录时间 # sqltime ：mysql判断一个sql的执行用时 # cur_utime ：这条sql从开始执行到结束，实际消耗的时间 # utime_after_lock：锁等待消耗时间 sqltime = cur_utim- utime_after_lock if sqltime&amp;gt;=@long_query_time: recordIt() #写入慢日志 else: pass 2.MySQL慢日志的常用操作 开启  修改my.cnf  [mysqld] slow_query_log = 1 slow_query_log_file = /data/mysql3306/mysql-slow.log #指定了慢查询日志的输出文件路径； long_query_time = 1 # 超过多长时间（秒）的SQL 被记录 修改  慢日志的几个项都可以在线联机修改的  set global long_query_time=0.</description>
    </item>
    
    <item>
      <title>微调有必要做吗？微调还是RAG?</title>
      <link>/book/%E5%BE%AE%E8%B0%83%E6%9C%89%E5%BF%85%E8%A6%81%E5%81%9A%E5%90%97%E5%BE%AE%E8%B0%83%E8%BF%98%E6%98%AFrag/</link>
      <pubDate>Mon, 09 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%BE%AE%E8%B0%83%E6%9C%89%E5%BF%85%E8%A6%81%E5%81%9A%E5%90%97%E5%BE%AE%E8%B0%83%E8%BF%98%E6%98%AFrag/</guid>
      <description>我需要对大模型做微调吗？ 想自定义大模型时，选择：微调还是RAG还是ICL？   需要对大模型做微调？  在人工智能的世界里，大型语言模型（LLM）已经成为了我们探索未知、解决问题的得力助手。 但是你想自己定义一个属于自己的大模型，它有自己特色的数据训练和回答方式。自己从头训练一个大模型的成本太高 这时候可能需要考虑在已有的大模型上做：微调 就像一个微整容手术一样，变得更帅     判断因素 是 否     是否需要特定领域的精确性？ 如果你的应用需要处理特定领域的数据，如医疗、法律或金融，并且需要高度的准确性和对专业术语的理解，那么微调可能是必要的。 如果你的应用是通用的，或者不需要深入特定领域的专业知识，那么可能不需要微调。   是否需要定制化模型行为？ 如果你需要模型以特定的风格、语调或格式响应，或者需要它表现出特定的行为特征，微调可以帮助你实现这些定制化需求。 如果模型的通用行为已经满足需求，或者你不需要特定的响应风格，那么微调可能不是必需的。   是否面临边缘案例的挑战？ 如果你发现模型在处理某些边缘或罕见案例时表现不佳，微调可以帮助改进这些特定情况的处理。 如果模型在所有常见和边缘案例中都表现良好，那么微调可能不是必要的。   是否需要提高模型的可靠性？ 如果模型在遵循复杂指令或生成期望输出方面存在失败，微调可以提高其可靠性。 如果模型已经足够可靠，能够满足你的输出要求，那么可能不需要微调。   是否需要降低成本？ 如果你希望通过微调将大型模型的技能转移到更小的模型中，以减少计算资源的使用和成本，那么微调是有益的。 如果成本不是主要考虑因素，或者你不需要优化模型的大小和性能，那么微调可能不是必需的。   是否需要快速部署新任务？ 如果你需要模型快速适应新任务或能力，微调可以帮助你实现这一点。 如果模型目前的任务已经足够，并且没有立即引入新任务的需求，那么微调可能不是必要的。   是否有足够的训练数据？ 如果你拥有足够的、高质量的、与任务相关的训练数据，微调可以显著提高模型的性能。 如果缺乏足够的训练数据，或者数据质量不高，微调可能不会带来预期的效果。   是否对模型的透明度有要求？ 如果你的应用需要模型的决策过程是可解释的，微调可以帮助你更好地理解和控制模型的行为。 如果模型的透明度不是关键考虑因素，那么可能不需要微调。   是否有足够的资源进行微调？ 如果你有足够的计算资源和专业知识来进行微调，那么这是一个可行的选项。 如果资源有限，可能需要考虑其他方法，如上下文学习或使用现成的模型。    1. 定制化风格与格式 你是否需要一个能够模仿特定人物或服务于特定受众的聊天机器人？通过使用定制数据集对LLM进行微调，我们可以使其响应更加贴近受众的具体要求或预期体验。例如，你可能需要将输出结构化为JSON、YAML或Markdown格式。</description>
    </item>
    
    <item>
      <title>MySQL创建远程链接服务器LinkServer步骤</title>
      <link>/mysql/mysql%E5%88%9B%E5%BB%BA%E8%BF%9C%E7%A8%8B%E9%93%BE%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8linkserver%E6%AD%A5%E9%AA%A4/</link>
      <pubDate>Thu, 05 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%88%9B%E5%BB%BA%E8%BF%9C%E7%A8%8B%E9%93%BE%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8linkserver%E6%AD%A5%E9%AA%A4/</guid>
      <description>环境准备  执行show engines;   mysql&amp;gt; show engines; +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | Engine | Support | Comment | Transactions | XA | Savepoints | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL | | MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO | | InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES | | PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO | | MyISAM | YES | MyISAM storage engine | NO | NO | NO | | MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO | | BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO | | CSV | YES | CSV storage engine | NO | NO | NO | | ARCHIVE | YES | Archive storage engine | NO | NO | NO | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ 9 rows in set (0.</description>
    </item>
    
    <item>
      <title>DTCC参会者视角：我在2024数据库技术大会的体验</title>
      <link>/dba/dtcc%E5%8F%82%E4%BC%9A%E8%80%85%E8%A7%86%E8%A7%92%E6%88%91%E5%9C%A82024%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8A%80%E6%9C%AF%E5%A4%A7%E4%BC%9A%E7%9A%84%E4%BD%93%E9%AA%8C/</link>
      <pubDate>Mon, 26 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/dtcc%E5%8F%82%E4%BC%9A%E8%80%85%E8%A7%86%E8%A7%92%E6%88%91%E5%9C%A82024%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8A%80%E6%9C%AF%E5%A4%A7%E4%BC%9A%E7%9A%84%E4%BD%93%E9%AA%8C/</guid>
      <description>前言  2024年数据库技术大会8月24日在北京成功举行 今年第十五届 是参与最深的一届 现场聆听了很多场的技术分享 还受邀做了一场数据安全的分享 感谢itpub的辛苦组织与坚持 给DBA提供了这么好的交流学习平台 这是发自内心的 每年能攒这么多的DBA在一起 非常难得 在会场会能见到很多老朋友 有的朋友可能是一年能见到一次的朋友 也遇到并结识了很多新的朋友 真的可谓收获满满 分享一下此次的个人参会体验 以及介绍一下我自己的分享主题  国产数据库在收缩  相比与前两年的火热 今年的国产数据库面孔在变少 信创的热度依旧很高 但随着信创的推进 部分单位已经完成了信创 而另一些信创可能正在进行中 前两年不断的涌现的新的数据库类型和创业公司 在减少 相比与前几年几乎每个大厂都在推新的数据库 tidb,ob,gaussDB&amp;hellip; 今年行业趋向收缩到几个大的国产db 这也意味着 未来再进入这个门槛的公司应该不多了 已有的这些厂商都拿到了自己的“地盘” 新势力不会再大量涌现了  向量和时序  因为这两种类型的库 我们都有在尝试 但是没有推 使用度几乎是0 所以格外关注这两个类型的库的发展状况 自己也在平衡 是否有必要在合适的时机引进和推广这类数据库 引进成本和推广代价是否值得 流窜了几个向量和时序的专场 也和同行交流 在特定场景下 是非常有价值的 我认可这件事 并打算后续尝试开始正式的试用  务实和务虚  新技术的快速引进和推广 你会在各个会场都听到各种高大上的解决方案 至少80%的专场都在聊新的技术方案 似乎传统的dba工作已经不在大家的视线范围内了 这样确实很高大上 但是相比与这些高大上 每个公司有特定的系统架构和环境 很多方案不一定能适配其他的场景 我更感兴趣的是基本的事 如何做好一个小目标 如何优化好一个小环节 这样的分享需要耐心的找 以及一点点运气 这些dba日常工作中遇到的那点事 怎么去优化和解决 是很有参考价值的 比如多融合库的在dba这边应该怎么选 比如快手上k8s的原因 等等&amp;hellip; 会在一定时间内 成为我们决定问题时的一个参考方向 其中有一场DBdoctor的分享 是我很想去听的分享 可惜因为时间关系 没有在现场 后期只能从ppt里看看了 而说到务实 不得不提一下 DBA之夜里林春老师了 分享dba从业20年经历的时候 林老师超认真的在说他们是如何辛苦的解决去O问题 怎么加班，怎么熬夜 怎么去想办法 得得的说了好久 非常的生动和具体 这真的是投身一线才有的体会  关于我的分享  说了很多现场的感受 其实我也是其中的分享 专场16里 我做为演讲嘉宾做了一场数据安全的分享 题目是《数据泄露了,有DBA什么事？》 这个名字是临时发挥的 其实有两种解读 我说的很显然是后一种 计划了3个章节来说这件事的 1.</description>
    </item>
    
    <item>
      <title>用ai技术来帮普通人完成扣篮</title>
      <link>/book/%E7%94%A8ai%E6%8A%8A%E7%85%A7%E7%89%87%E5%8F%98%E6%88%90%E8%A7%86%E9%A2%91%E6%95%88%E6%9E%9C%E6%83%8A%E4%BA%BA/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E7%94%A8ai%E6%8A%8A%E7%85%A7%E7%89%87%E5%8F%98%E6%88%90%E8%A7%86%E9%A2%91%E6%95%88%E6%9E%9C%E6%83%8A%E4%BA%BA/</guid>
      <description> 上周朋友给我拍了一张打篮球的照片 当时我想表演一下扣篮 奈何弹跳能力有限 只留一下朴素的照片 好在有ai技术 弥补了老年人跳不起来的尴尬  原片（照片） 成片（视频）   </description>
    </item>
    
    <item>
      <title>AI的电力需求以及国内的电力能源情况</title>
      <link>/book/%E8%83%BD%E6%BA%90%E4%B8%8Eai/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%83%BD%E6%BA%90%E4%B8%8Eai/</guid>
      <description>AI的惊人耗电量  当我们说大模型AI的使用成本很贵的时候 一方面其训练和推理的硬件GPU本身很贵，另一方面其消耗的电力也很夸张 这里我们只说AI的耗电量  AI训练阶段耗电量  OpenAI的GPT-3模型一次训练的耗电量高达1287兆瓦时 谷歌于2022年发布的大语言模型PaLM一次训练需要消耗3436兆瓦时的电量 模型越大，训练所需要消耗的电量越多。现在gpt-4o,文心一言4,通义千问这些新的大模型训练用电量会更夸张  AI推理阶段的耗电量  GPT每生成1000个英文单词大约消耗0.125千瓦时的电量 OpenAI需要3,617台英伟达公司的HGX A100服务器（共有28,936个图形处理单元 (GPU)）来支持 ChatGPT，这意味着每天的能源需求为 564 兆瓦时 谷歌搜索中应用生成式AI技术，谷歌每年的耗电量将高达290亿千瓦时，也就是每天约7900万度电  AI耗电的原因及改进方向 AI耗电的原因：GPU运算   以H800 GPU PCIE 服务器整机为例：
 CPU耗电约 300W*2， 内存16根耗电约 250W 硬盘6块盘约200W 风扇耗电约150W H800GPU卡耗电约700W*8    合计：最大耗电量约为6800W（90%以上的耗电都是GPU引起的）
  这些还不包括机房空调制冷的电力消耗
    这样的一台服务器，对OpenAI这样的公司来说，一个机房里需要放几万台
  AI耗电的改进方向  随着英伟达的GPU工艺不停的迭代发展 每次英伟达的新品发布会，都会发现新一代的显卡比上一代的能耗是几倍的减少 性能越来越高，单位算力的能耗成倍的减少，这是实大实的在减少能耗 可惜AI的算力要求越来越高，减少的能耗目前赶不上需求的增长 未来很长一段时间AI消耗的能源会越来越多。  国内电力能源现状  来自国家能源局发布的《2023年全国电力工业统计数据》 太阳能和风能装机量正在快速增长     类别 装机容量(万千瓦时) 占比 同比增长     火电 139032 47.</description>
    </item>
    
    <item>
      <title>利用开源大语言模型打包成自己的大语言模型</title>
      <link>/book/%E5%88%A9%E7%94%A8%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%89%93%E5%8C%85%E6%88%90%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%88%A9%E7%94%A8%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%89%93%E5%8C%85%E6%88%90%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</guid>
      <description>为什么要用开源模型 大语言模型有两种类型：  闭源的模型，如GPT-3.5、GPT-4、Cluade 、文心一言等 开源的模型，如LLaMA、ChatGLM,Qianwen等  开源模型的优势  已知目前最强的gpt4等大模型是商用闭源的，这些模型参数更大，更加智能，为什么我们会关注开源模型呢？ 可以本地部署运行（利用自己的电脑或服务器，运行）数据交互不需要和外网连接，数据安全性提升 不需要购买服务，不用开会员，跑在自己的电脑上，想用多少就用多少   怎么打包自己的模型 本地运行大模型  本地运行，需要至少一台性能很好的机器，不管是服务器，云服务器，或者自己的电脑，最好有张naviad 的显卡 选择大模型，现在目前最好的是llama3 是由Meta公司开源的，另外gemma是Google的，也非常不错，微软和苹果也开源了。中文的阿里开源的千问也不错 选好大模型后去下载到本地（体积看参数多少，在4G&amp;ndash;100G之间） 下载好后，就可以本地运行了，只需要在命令行中输入命令 如果想要个网页端上对话，可以再下载一个网页端，比如open webui （这是我喜欢用的）。看个人风格  打包自己的模型  有个新闻说是国内现在发布了几百个ai大语言模型，很多都是基于这些开源的模型上训练或改的 即使在开源模型上训练和微调也需要很多的显卡资源和算力。也不是个人可以做到的 如果你和我一样没有很大算力的服务器，又想尝试发布自己的大模型 可以考虑重新打包一个大模型，让它变成你的大模型  怎么打包  步骤1.下载开源模型  wget &amp;quot;https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit/blob/main/Llama3-8B-Chinese-Chat-q8.gguf&amp;quot;&amp;quot; ll -h -rw-r--r-- 1 root root 8.0G Apr 21 10:21 Llama3-8B-Chinese-Chat-q8.gguf -rw-r--r-- 1 root root 662 Apr 21 14:57 Modelfile  步骤2.编辑Modelfile  vim Modelfile #类型如下 FROM &amp;quot;/data/gguf/Llama3-8B-Chinese-Chat-q8.gguf&amp;quot; TEMPLATE &amp;quot;&amp;quot;&amp;quot;{{ if .</description>
    </item>
    
    <item>
      <title>如何把MySQL和Oracle里的表同步到一个加密库中</title>
      <link>/dba/%E5%A6%82%E4%BD%95%E6%8A%8Amysql%E5%92%8Coracle%E9%87%8C%E7%9A%84%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0%E4%B8%80%E4%B8%AA%E5%8A%A0%E5%AF%86%E5%BA%93%E4%B8%AD/</link>
      <pubDate>Wed, 24 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%A6%82%E4%BD%95%E6%8A%8Amysql%E5%92%8Coracle%E9%87%8C%E7%9A%84%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0%E4%B8%80%E4%B8%AA%E5%8A%A0%E5%AF%86%E5%BA%93%E4%B8%AD/</guid>
      <description>如图 加密配置元信息表 REATE TABLE `encrypt_baseinfo` (`from_linkname` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;来源数据库信息&#39;,`from_linktype` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;mysql&#39; COMMENT &#39;mysql,oracle,mongo&#39;,`from_connstr` varchar(200) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;127.0.0.1;3306;u_test;password;db_test;&#39;,`to_linkname` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;目标数据库信息&#39;,`to_linktype` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;mysql&#39; COMMENT &#39;mysql,oracle,mongo&#39;,`to_connstr` varchar(200) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;127.0.0.1;3306;u_test;password;db_test;&#39;,`kafka_name` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;kafka_name&#39;,`kafka_groupid` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;kakfak_groupid&#39;,`kakfak_offset` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;kakfak_auto_offset_reset&#39;,PRIMARY KEY (`from_linkname`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;加密字段基础信息表&#39;CREATE TABLE `encrypt_col_info` (`id` bigint unsigned NOT NULL AUTO_INCREMENT COMMENT &#39;ID、主键&#39;,`service_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;服务信息&#39;,`db_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;数据库信息&#39;,`tab_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;表名&#39;,`col_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;列名&#39;,`sub_col_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;json_key信息&#39;,`col_type` varchar(20) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;字段类型&#39;,`col_desc` varchar(5000) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;列名描述信息&#39;,`create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;创建时间&#39;,`update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#39;最后更新时间&#39;,`linkname` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39;,`sec_type` tinyint NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;加密类型：0，1：卡号，2：身份证，3：手机，4：地址&#39;,PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;加密字段信息表&#39;判断配置表里的数据类型是否正确的快速方法  select concat(&#39;mysqlw -h &#39;,replace(linkname,&#39;my&#39;,&#39;mysql&#39;),&#39;-w.</description>
    </item>
    
    <item>
      <title>国内大语言模型现状之法律篇</title>
      <link>/book/%E5%9B%BD%E5%86%85%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%E4%B9%8B%E6%B3%95%E5%BE%8B%E7%AF%87/</link>
      <pubDate>Tue, 23 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%9B%BD%E5%86%85%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%E4%B9%8B%E6%B3%95%E5%BE%8B%E7%AF%87/</guid>
      <description>发展时间线  2022年11月30日,国外OPenAI公司的ChatGPT发布，大语言模型开始普及 2023年2月，国内ChatGPT开始进入国内视野大量媒体报道 2023年5月开始，基于开源ai大语言模型上用国内的法律条文和案例训练的国产法律大模型开始发布（以高校为主） 2023年底。随着llama模型升级，国产法律大模型更加智能，同时商业化落地产品开始出现 2024年由案例检索，合同审查为典型应用场景的国内法律AI产品开始推广应用  国内法律大语言模型现状    国内法律行业的大语言模型对比    大模型名称 日期 发布人 基础模型 推荐指数     ChatLaw 2023-06-28 北京大学深圳信息工程学院 Anima-33B 4星   LaWGPT 2023-04-12 南京大学 LLaMA 4星   LexiLaw 2023-05-16 清华大学 ChatGlM 6B 2星   獬豸(LawGPT_zh) 2023-04-09 上海交通大学 ChatGlM 6B 3星   Lawyer LLaMA 2023-04-13 北京大学 LLaMA 2星   韩非(HanFei) 2023-05-30 香港中文大学 LLaMA 1星   lychee_law-律知 2023-07-13 德国萨尔大学,中国南京大学 GLM-10B 1星   智海-录问 2023-08-08 浙江大学,ali达摩院,华院计算 Baichuan-7B 1星   DISC-LawLLM 2023-09-26 复旦大学 Baichuan-13B-Base 1星   夫子•明察 2023-08-31 山东大学,浪潮云,中国政法大学 ChatGLM 1星    国内法律大模型介绍 ChatLaw-法律大模型  地址：https://github.</description>
    </item>
    
    <item>
      <title>斯坦福大学发布了《2024人工智能指数报告》</title>
      <link>/book/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E5%8F%91%E5%B8%83%E4%BA%862024%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8C%87%E6%95%B0%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E5%8F%91%E5%B8%83%E4%BA%862024%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8C%87%E6%95%B0%E6%8A%A5%E5%91%8A/</guid>
      <description>今天斯坦福大学发布了《2024人工智能指数报告》  原文在这里，一共有500多页pdf https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024.pdf 我将一些感兴趣的点总结出来  AI的发展状况 AI在2023年的恐怖的发展速度  2023年，有GPT4,Cluade2,DALL-E3，Gemini等非常有突破性的产品发布 以下是15个重要的大模型的发布时间点，这些大模型一定程度上代表了当前AI能达到的最佳水平   AI大模型的发布时间  横轴是发布时间（2012-2024） 纵轴是发布时的计算量（Training compute (petaFLOP - log scale)）   高昂的训练成本  谷歌的 Gemini Ultra 的训练计算成本估计为 1.91 亿美元 OpenAI 的 GPT-4 的训练成本估计为 7800 万美元。 高昂的训练成本使普通用户（甚至包括学术机构和政府）难以参与AI的训练。   不同地区分展的速度不同 大语言AI模型发布数量  美国在2023 年总共开发了 61 个模型。 中国在2023 年总共开发了 8 个模型。   AI领域的投资  2023年，美国投资额为672亿美元 是第二高国家中国投资额的8.7倍 是英国投资额的17.8倍。 缩小范围来看，这一阵容看起来是一样的：自 2013 年以来，美国的累计投资额为 3,352 亿美元，其次是中国，为 1,037 亿美元，英国为 223 亿美元。   </description>
    </item>
    
    <item>
      <title>Ollama的安装和配置</title>
      <link>/book/ollama%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/ollama%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</guid>
      <description>为什么要用Ollama  几乎是最方便的本地部署Ai大模型的方式 支持在 Mac、Windows、Linux 上运行 支持CPU,GPU 不用考虑复杂的本地环境 简直是大模型里的docker   Ollam官网的介绍是：Get up and running with large language models, locally.
 安装Ollla Mac  一行命令brew install ollama  Linux  一行命令  curl -fsSL https://ollama.com/install.sh | sh  Windows  下载ollama的安装包，下载地址 https://ollama.com/download/OllamaSetup.exe 安装上就即可  启动  ollama serve 启动服务 ollama list 查看本地的模型 ollama run 启动模型  配置Ollama 设置服务  vim /etc/systemd/system/ollama.service  [Unit] Description=Ollama Service After=network-online.target [Service] Environment=&amp;quot;OLLAMA_HOST=0.0.0.0:11434&amp;quot; ExecStart=/usr/local/bin/ollama serve #User=ollama #Group=ollama User=root Group=root Restart=always RestartSec=3 Environment=&amp;quot;PATH=/root/anaconda3/bin:.</description>
    </item>
    
    <item>
      <title>MySQL和Oracle数据库的一些审计要求</title>
      <link>/dba/mysql%E5%92%8Coracle%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%A1%E8%AE%A1%E8%A6%81%E6%B1%82/</link>
      <pubDate>Mon, 11 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/mysql%E5%92%8Coracle%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%A1%E8%AE%A1%E8%A6%81%E6%B1%82/</guid>
      <description>MySQL的审计要求  密码是否符合复杂度要求，需要包含大小写字母、数字、字符，密码长度是否达到8位以上  show variables like &#39;%validate%&#39;; INSTALL PLUGIN validate_password SONAME &#39;validate_password.so&#39;; set global validate_password_length=12 set global ... show global variables like &#39;%validate%&#39;;  密码是否定期更换，如，密码有效期设置为90天。  set global default_password_lifetime=360;  是否具有登陆失败锁定策略，例如，登陆失败5次锁定10分钟。   INSTALL PLUGIN CONNECTION_CONTROL SONAME &#39;connection_control.so&#39;; INSTALL PLUGIN CONNECTION_CONTROL_FAILED_LOGIN_ATTEMPTS SONAME &#39;connection_control.so&#39;; show global variables like &#39;connection_%&#39; ;  是否设置超时登出功能，例如，30分钟未操作自动登出  show variables like &#39;wait%timeout%&#39; ; Oracle的审计要求 </description>
    </item>
    
    <item>
      <title>Sora来袭：OpenAI公司是否又开启文本生成视频新篇章？</title>
      <link>/book/sora%E6%9D%A5%E8%A2%ADopenai%E5%85%AC%E5%8F%B8%E6%98%AF%E5%90%A6%E5%8F%88%E5%BC%80%E5%90%AF%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%91%E6%96%B0%E7%AF%87%E7%AB%A0/</link>
      <pubDate>Tue, 20 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/sora%E6%9D%A5%E8%A2%ADopenai%E5%85%AC%E5%8F%B8%E6%98%AF%E5%90%A6%E5%8F%88%E5%BC%80%E5%90%AF%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%91%E6%96%B0%E7%AF%87%E7%AB%A0/</guid>
      <description>Sora来袭  2024年2月15日OpenAI公司在推上连续发了几个Sora文生图视频，引起轰动 OpenAI并未单纯将其视为视频模型，而是作为“世界模拟器” Sora继承了DALL-E 3的画质和遵循指令能力 可以根据用户的文本提示创建逼真的视频 可以深度模拟真实物理世界，能生成具有多个角色、包含特定运动的复杂场景 能理解用户在提示中提出的要求，还了解这些物体在物理世界中的存在方式。  Sora来袭的反应  360集团创始人、董事长 周鸿祎   Sora将缩短AGI（通用人工智能）实现时间，从10年缩短到1年。OpenAI训练该模型应该会以视频和摄像头捕捉的画面为主，人工智能通过观看大量视频将对世界有更深入的理解，这离AGI实现不远
  电影导演兼视觉效果专家 迈克尔·格雷西   很快，像Sora这样的人工智能工具将允许电影制作者仔细控制他们的输出，从头开始创建各种视频，当技术剥夺了其他人的创造力、工作、想法和执行力，却没有给予他们应有的荣誉和经济报酬时，不是一件好事情。
  英伟达科学家 DrJimFan   Sora是一个数据驱动的物理引擎，它是对许多世界的模拟，无论是真实的还是幻想的，模拟器通过一些去噪和梯度数学来学习复杂的渲染、“直观”物理、长期推理和语义基础。
 Sora的团队  Sora核心团队有15人 Sora团队的Leader是Aditya Ramesh：他也是DALLE、DALLE2、DALLE3的主要作者 Sora的核心作者是Bill Peebles和Tim brooks Bill Peebles 在伯克利人工智能研究所完成了博士学位，导师是Alyosha Efros。在此之前，他在麻省理工学院攻读本科，指导老师是Antonio Torralba。他曾在FAIR、Adobe研究院和NVIDIA实习。 Tim brooks 在伯克利人工智能研究所获得了博士学位，导师是Alyosha Efros，他是InstructPix2Pix的作者。在此之前他曾在谷歌工作，参与Pixel手机相机的研发，在NVIDIA从事视频生成模型的研究。   Bill Peebles的说法是“每天基本不睡觉，高强度工作了一年。  如何试用Sora  答案是：现在还不行 到目前为止(北京时间 2023-02-20 18:20) Sora并没有对外开放 目前只有OpenAI内部员工,一批受邀请的视觉艺术家、设计师和电影制作人获得了Sora访问权限，他们也已开始在社交平台不断晒出使用Sora生成的新作品 未来开放时间不确定，但是首批可使用的用户数量不会太多，且肯定不会对中国大陆开放使用 国内用户要新手体验sora的功能，可能还是还很远  Sora的竞争对手    AI 公司 AI视频产品 发布日期      英伟达 PYoCo 2023.</description>
    </item>
    
    <item>
      <title>redis的内存报警OOM command not allowed when used memory&gt;maxmemory</title>
      <link>/dba/redis%E7%9A%84%E5%86%85%E5%AD%98%E6%8A%A5%E8%AD%A6oomcommandnotallowedwhenusedmemorymaxmemory/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/redis%E7%9A%84%E5%86%85%E5%AD%98%E6%8A%A5%E8%AD%A6oomcommandnotallowedwhenusedmemorymaxmemory/</guid>
      <description> 非核心业务的一次小故障，未造成用户感知到的业务影响，记录如下
 参与者  DEV1,DEV2 DBA1,DBA2 3主3从的RedisCluster集群：1.10,1.11,1.12,1.20,1.21,1.22  故障起因  DEV1想排查线上Redis是否有对指定的key有访问 11:45 DEV1找到DBA1协助排查 11:50 DBA1在1.11实例上开启monitor进程，监控Redis写入 11:55 monitor进程启动5分钟后，1.11实例的内存占用从2G涨到10G 触发该节点的内存占满，引发故障(该节点的新写入报错，其他节点正常读写) 12:05 DBA1在1.11实例上停止monitor进程，1.11实例的内存占用从10G回退到2G 12:05 Redis集群自动恢复正常   故障发现和处理  12:20 DEV2收到报警 12:23 DEV2找到DBA2反馈程序报错Caused by: io.lettuce.core.RedisCommandExecutionException: OOM command not allowed when used memory &amp;gt; &#39;maxmemory&#39; 12:25 DBA2上线检查问题，在节点1.10上查看内存使用率是2G/10G 正常 12:28 DBA2检查该集群的1.10，1.11,1.12三个节点内存都是2G/10G 没发现异常。 12:30 查不到问题，修改该集群的所有节点最大内存从10G 改到12G 12:30 DEV2重启应用，发现恢复。 12:40 DBA2检查Redis应用，发现set,get的命令从每秒的6000次/秒降到500次/秒，认为业务没有恢复，建议继续排查 12:45 DBA1，DEV1参与排查，DEV2发现有个status任务没有重启成功 12:46 DEV2重启status任务，1分钟后，Redis监控指标恢复正常，故障完成处理 13:12 回溯整个过程，确认是11:50的Monitor进程引起的内存占用异常，原因定位 13:25 沟通确认Monitor进和不可以长期开启的规范。故障完成处理和总结  总结  DBA协助研发排查问题时，开启Monitor进程时间过长，引起一个节点的内存占满，继而引起研发的进程挂掉 非核心业务，没有影响到用户和交易，处理过程中现象比较明显，处理难度低，监控还是不够周全 补充：考虑换LRU策略  </description>
    </item>
    
    <item>
      <title>利用OracleGoldenGate(ogg) 从Oracle同步数据到MySQL</title>
      <link>/oracle/%E5%88%A9%E7%94%A8oraclegoldengateogg-%E4%BB%8Eoracle%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E5%88%B0mysql/</link>
      <pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/oracle/%E5%88%A9%E7%94%A8oraclegoldengateogg-%E4%BB%8Eoracle%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E5%88%B0mysql/</guid>
      <description>案例A：增加增量同步 1.增加extract进程 dblogin userid GOLDENGATE@{{ Oracle server url }},password {{ Oracle Password }} register extract E02 database add extract E02,integrated tranlog, begin now add EXTTRAIL /data/ogg19oracle/dirdat/eb, extract E02,MEGABYTES 100 add schematrandata {{ oracle.Schema }} 编辑ogg文件  vim /data/ogg19oracle/dirprm/e02.prm  extract e02 setenv (NLS_LANG=AMERICAN_AMERICA.ZHS16GBK) userid GOLDENGATE@{{ Oracle server url }},password {{ Oracle Password }} exttrail /data/ogg19oracle/dirdat/eb GETUPDATEAFTERS GETUPDATEBEFORES NOCOMPRESSDELETES NOCOMPRESSUPDATES table {{ SCOTT.AAAA }}; table {{ SCOTT.BBBB }}; 2.增加pump进程  add extract p02 exttrailsource /data/ogg19oracle/dirdat/eb add rmttrail /data/ogg19mysql/dirdat/eb, extract p02 3.</description>
    </item>
    
    <item>
      <title>拜占庭将军的问题TheByzantineGeneralsProblem</title>
      <link>/book/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>什么是拜占庭将军问题？（The Byzantine Generals Problem） 拜占庭  拜占庭位于如今的土耳其的伊斯坦布尔,是当时东罗马帝国的首都 拜占庭将军问题它是由2013年图灵奖获得者计算机大神兰伯特在1982年发表的论The Byzantine Generals Problem提出。 拜占庭将军问题不是一个真实存在的问题，而是一个虚拟问题。 拜占庭将军问题本质是，在存在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。   由于当时拜占庭罗马帝国国土辽阔,为了防御目的，因此每个军队都分隔很远，将军与将军之间只能靠信差传消息。在战争的时候，拜占庭军队内所有将军和副官必需达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营。但是，在军队内有可能存有叛徒和敌军的间谍，左右将军们的决定又扰乱整体军队的秩序。在进行共识时，结果并不代表大多数人的意见。这时候，在已知有成员谋反的情况下，其余忠诚的将军在不受叛徒的影响下如何达成一致的协议，拜占庭问题就此形成
 拜占庭将军问题  拜占庭罗马帝国国土辽阔，为了达到防御目的，每个军队都分隔很远，将军与将军之间只能靠信差传消息。在战争的时候，拜占庭军队内所有将军和副官必须达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营。 一组拜占庭将军分别各率领一支军队共同围困一座城市。为了简化问题，将各支军队的行动策略限定为进攻或撤离两种。因为部分军队进攻，部分军队撤离可能会造成灾难性后果，因此各位将军必须通过投票来达成一致策略，即所有军队一起进攻或所有军队一起撤离。 因为各位将军分处城市不同方向，他们只能通过信使互相联系。 在投票过程中每位将军都将自己投票给进攻还是撤退的信息通过信使分别通知其他所有将军，这样一来每位将军根据自己的投票和其他所有将军送来的信息就可以知道共同的投票结果而决定行动策略。 系统的问题在于，可能将军中出现叛徒，他们不仅可能向较为糟糕的策略投票，还可能选择性地发送投票信息。由于将军之间需要通过信使通讯，叛变将军可能通过伪造信件来以其他将军的身份发送假投票。 而即使在保证所有将军忠诚的情况下，也不能排除信使被敌人截杀，甚至被敌人间谍替换等情况。 因此很难通过保证人员可靠性及通讯可靠性来解决问题。这时候，在已知有成员谋反的情况下，其余忠诚的将军在不受叛徒的影响下如何达成一致的协议，就由此形成了历史中鼎鼎有名的拜占庭将军问题。  解决方案 方案一：口头协议  设总人数为n, 叛徒数为m,只要满足 n＞3m, 那这个问题就是可以解决的。 我们可以将拜占庭将军问题简化为将军或司令和副官模型。将军是第一个提出建议的人，副官可以执行或不执行将军的命令。 这里需要注意的是：Lamport提出的容错的两个条件：  IC1：即所有的忠诚的副官要遵守同一个命令，即达成一致； IC2：假如将军是忠诚的，那么每一个忠诚的副官都应该按照将军的意思行事。（如果将军是叛徒，所以只要满足IC1条件即可。）    情景:m=1,n=3（出错了）  将军G(General), 副官1和副官2 当叛徒数m=1,总人数=3时 假设将军是忠诚的，副官1是叛徒。那么就会出现一下情况：将军发起进攻命令，1和2副官收到的都是进攻命令。1是叛徒，1收到进攻，同步给2后退，此时2收到1进攻1后退,2不知道怎么办。  情景:m=1,n=4（可行）  将军仍然记为G,副官分别为1,2,3。  假设将军G是忠诚的，副官2是叛徒  将军G对1,2,3号副官发起进攻命令，那1会从2那里得到撤退命令，从3那里得到进攻命令，那1得到的命令集合就是2个进攻1个撤退命令；同样3，得到的命令集合也是2个进攻1个撤退。  假设将军G是叛徒，副官1,2,3都是忠诚的。  将军对1,3号副官发起进攻命令，对2副官发起撤退命令，那1会从2那里得到撤退命令，会从3那里得到进攻命令，1得到的命令集合就是2攻击1撤退，同理2,3都是2攻击1撤退。 1,2,3会同时进攻，满足IC1  方案二：书面协议  之所以会出现在口头传达中的那些错误是因为一些叛徒可以说谎，这里通过签名就是为了防止说谎。在签名算法中加了两个条件： （a）忠诚将军的签名是不能伪造的，内容修改可检测。（即 即使是叛徒也要原封不动的签了名将消息转发出去） （b）任何人都可以识别将军的签名，叛徒可以伪造叛徒司令的签名。 而且这里规定：每条消息只可以复制，然后加上自己的姓名再发出去。 下面是具体的算法：   初始化中的 Vi 类似于一个集合，表示的是第i个将军收到的命令，比如 Vi= {Attack} 之所以说是个集合是因为Vi里面不会有重复的命令出现。这在算法步骤（2）的（B) 部分描述的很清楚。   在算法步骤（1）中将军将签了自己姓名的消息广播发给所有副官。注意这里发的格式是 V:0，V是命令，0代表自己的身份。  算法步骤（2）（A）中，每个副官将收到的消息 V:0 把命令V放入自己的命令集合Vi（因为初始的时候他们的命令集合都是空的，所以不存在重复问题） 然后他们将命令拷贝，然后加上自己的签名 ，得到消息： V:0:i 然后再发给其他的副官。   在算法步骤（2）（B）中因为副官i 也会收到别的副官发来的消息v:0:1:&amp;hellip;:jk.</description>
    </item>
    
    <item>
      <title>8种常用于数据库的数据结构</title>
      <link>/dba/8%E7%A7%8D%E5%B8%B8%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</link>
      <pubDate>Tue, 02 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/8%E7%A7%8D%E5%B8%B8%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</guid>
      <description>1.跳跃表(skipList) 什么是skiplist  跳跃表（skiplist）是一种随机化的数据， 由 William Pugh 在论文《Skip lists: a probabilistic alternative to balanced trees》中提出， 跳跃表以有序的方式在层次化的链表中保存元素， 效率和平衡树媲美 —— 查找、删除、添加等操作都可以在对数期望时间下完成， 并且比起平衡树来说， 跳跃表的实现要简单直观得多。  图示    用途：  Redis  2.哈希索引（Hash Index） 什么是hash Index  基于哈希表实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 哈希索引可细分为静态哈希和动态哈希这两大类，  静态哈希  基于散列技术的文件组织使我们能够避免访问索引结构，同时也提供了一种构造索引的方法。在对散列的描述中，使用桶(bucket)来表示能存储一条或多条记录的一个存储单位。通常一个桶就是一个磁盘块，但也可能大于或者小于一个磁盘块。 散列索引将散列函数作用于搜索码以确定对应的桶， 然后将此搜索码以及对应的指针存入此桶(或溢出桶)中。 静态散列最大的缺点在于必须在实现系统时选择确定的散列函数。此后若被索引的文件变大或缩小，要想再改变散列函数就不容易了。因为散列函数 h 将搜索码值映射到桶地址的固定集合 B 上： 根据当前文件大小选择散列函数，这样的选择会使得性能随着数据库的增大而下降。换言之，初始时集合 B 太小，一个桶就会包含许多不同的搜索码值的记录，从而可能发生桶溢出。当文件变大时，性能就会受到影响。 根据将来某个时刻文件的预计大小选择散列函数。 尽管这样可以避免性能下降，但是初始时会造成相当大的空间浪费。  动态哈希  针对静态散列技术出现的问题，动态散列（dynamic hashing）技术允许散列函数动态改变，以适应数据库增大或缩小的需要 当数据库增大或缩小时，可扩充散列可以通过桶的分裂或合并来适应数据库大小的变化，这样可以保持空间的使用效率。此外，由于重组每次仅作用于一个桶，因此所带来的性能开销较低。  图示    3.ssTable 什么是ssTable  SSTable文件是memtable 数据到一定阈值写入文件形成的，由于内存容量总是有限的，将一定量数据写入磁盘可以存放更多数据，所以leveldb相比redis能存放更多数据。既然数据持久化到磁盘，那么还有必然涉及到从磁盘中查询数据，从磁盘中查询数据与从内存中查询数据的效率是不一样的，所以SSTable 数据组织方式必然与众不同，因为必须要提高查询效率，不能给一个key就去遍历所有SSTable。因此本文的另一个目的就是学习SSTable 文件如何组织key-value，提高查询效率。为了提高内存中数据查询效率 我们学习了各种数据结构如红黑树，散列表，那么SSTable是学习如何提高文件查询数据效率的一个很好例子。  图示    4.</description>
    </item>
    
    <item>
      <title>ApacheDoris在Centos7环境下的安装部署</title>
      <link>/dba/apachedoris%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/apachedoris%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</guid>
      <description>环境准备 linux版本 Centos7:  cat /etc/redhat-release  系统最大打开文件句柄数​ cat etc/security/limits.conf * soft nofile 65536* hard nofile 65536关闭交换分区（swap）​ swapoff -ased -i &#39;/ swap / s/^\(.*\)$/#\1/g&#39; /etc/fstab检查文件系统  df -hT |grep &amp;quot;ext4&amp;quot;GCC版本(&amp;gt;=4.8.2) gcc -v# gcc version 4.8.5 2015062JAVA版本(&amp;gt;=1.8) java -version # java version &amp;quot;1.8.0_202&amp;quot;确认cpu是否支持avx2 cat /proc/cpuinfo | grep avx2安装 下载 cd /data/software/wget https://apache-doris-releases.oss-accelerate.aliyuncs.com/apache-doris-2.0.3-bin-x64.tar.gztar zxvf apache-doris-2.0.3-bin-x64.tar.gzcd cd apache-doris-2.0.3-bin-x64llmkdir /data/doris5306mv * /data/doris5306/安装fe 修改fe.</description>
    </item>
    
    <item>
      <title>AIGC在哔哩哔哩内部的使用场景探索</title>
      <link>/book/aigc%E5%9C%A8%E5%93%94%E5%93%A9%E5%93%94%E5%93%A9%E5%86%85%E9%83%A8%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%8E%A2%E7%B4%A2/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/aigc%E5%9C%A8%E5%93%94%E5%93%A9%E5%93%94%E5%93%A9%E5%86%85%E9%83%A8%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%8E%A2%E7%B4%A2/</guid>
      <description> 2023年11月24日 系统架构师大会在上海举行 其中有一场b站OTT&amp;amp;漫画技术部邬晶的分享 《大模型与AIGC 在B站OTT、漫画业务中的应用探索》 分享中提到AIGC在哔哩哔哩内部的使用场景探索 包括以下几个场景的尝试  客服助手  编码助手 漫画辅助创作 辅助内容处理 合同格式化 辅助本地化      我感兴趣的有：客服助手，合同格式化  AI客服助手 背景：  客服侧同学需要同时处理多个业务线，每条业务线也一直处 于高速迭代状态，业务细节多，用户问题杂，为了帮助客服 同学提高问题处理效率，我们基于ChatGPT3.5输出了智能客服系统  解决方案 挑战 收益 拦截率:88.5% -&amp;gt; 92% 节省人力:13%
AI合同格式化 背景： 解决方案 挑战 收益  目前还在开发进行中 目标: 将合同电子化、平台化。各种权利项平滑入库，提升可维护性。降低商务成本。  小结  已完成的AI客服助手方案 是比较现实的可以落地的解决方案 也就是利用向量数据库来做关联匹配 实际运行中遇到的挑战 也是做此类产口需要关注的 这个分享说得非常具体了 合同格式化的是件收益不错的事B站也在开发中  </description>
    </item>
    
    <item>
      <title>快速完成一个异构数据同步异常检查功能</title>
      <link>/dba/%E5%BF%AB%E9%80%9F%E5%AE%8C%E6%88%90%E4%B8%80%E4%B8%AA%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%BC%82%E5%B8%B8%E6%A3%80%E6%9F%A5%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%BF%AB%E9%80%9F%E5%AE%8C%E6%88%90%E4%B8%80%E4%B8%AA%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%BC%82%E5%B8%B8%E6%A3%80%E6%9F%A5%E5%8A%9F%E8%83%BD/</guid>
      <description>起因  上周我们做的一次Oracle到MySQL迁移，在迁移完成后需要将MySQL数据反向同步到Oracle中，以便于没有迁移干净的原Oracle业务还能提供只读访问 反向同步用的是OGG，我们在OGG同步层面加了异常报警（但是被DBA给人为禁用了 除掉同步软件层面的报警外，我们需要一个偏业务层面的，针对表数据的报警和监控  需求  支持各种数据源的同步对比 支持表的行数对比，最大id对比，表最后更新时间对比 扩展性好，方便配置 异常报警 监控同步状态的页面  解决 拆解思路  需要一个建两张表：表1:存放任务配置信息，表2:存放采集到的数据 需要一个任务：定时（5分钟）去源库和目标库，运行一个SQL,取当前状态并存入到刚才建的history表中 需要一个页面：展示采集结果，可以方便的查看同步状态和延时 需要一个报警任务：异常数据时，发送报警  任务1:建表  CREATE TABLE `msync_config` (`sync_name` varchar(50) NOT NULL,`source_linkconnstr` varchar(100) NOT NULL,`target_linkconnstr` varchar(100) NOT NULL,`source_sqlstr` varchar(2000) NOT NULL,`target_sqlstr` varchar(2000) NOT NULL,`alert_count` int NOT NULL DEFAULT &#39;5&#39;,`alert_maxid` int NOT NULL DEFAULT &#39;5&#39;,`alert_delaysecond` int NOT NULL DEFAULT &#39;60&#39;,`alert_userlist` varchar(100) NOT NULL DEFAULT &#39;&#39;,`add_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,`update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,`alert_type` varchar(50) NOT NULL DEFAULT &#39;count+time&#39; COMMENT &#39;count,id,time三种组合,+代表and,-代表or&#39;) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3 CREATE TABLE `msync_history` (`addtime` datetime NOT NULL,`sync_name` varchar(50) NOT NULL,`source_count` bigint NOT NULL DEFAULT &#39;0&#39;,`target_count` bigint NOT NULL DEFAULT &#39;0&#39;,`source_maxid` bigint NOT NULL DEFAULT &#39;0&#39;,`target_maxid` bigint NOT NULL DEFAULT &#39;0&#39;,`source_maxtime` datetime NOT NULL DEFAULT &#39;0000-00-00 00:00:00&#39;,`target_maxtime` datetime NOT NULL DEFAULT &#39;0000-00-00 00:00:00&#39;,PRIMARY KEY (`addtime`,`sync_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3任务2:配置采集任务 &amp;lt;action result=&amp;quot;allcount,okcount,errcount&amp;quot; type=&amp;quot;sql_loop&amp;quot; &amp;gt;&amp;lt;connstr&amp;gt;link:dboop_db&amp;lt;/connstr&amp;gt;&amp;lt;sqlstr&amp;gt;select sync_name,source_linkconnstr,target_linkconnstr,source_sqlstr,target_sqlstr from msync_config&amp;lt;/sqlstr&amp;gt;&amp;lt;action result=&amp;quot;sync_name,source_linkconnstr,target_linkconnstr,source_sqlstr,target_sqlstr&amp;quot; type=&amp;quot;sql_select&amp;quot; &amp;gt;&amp;lt;connstr&amp;gt;link:dboop_db&amp;lt;/connstr&amp;gt;&amp;lt;sqlstr&amp;gt;select sync_name,source_linkconnstr,target_linkconnstr,source_sqlstr,target_sqlstr from msync_config where sync_name=%s&amp;lt;/sqlstr&amp;gt;&amp;lt;sqlpara&amp;gt;{__0}&amp;lt;/sqlpara&amp;gt;&amp;lt;/action&amp;gt;&amp;lt;!</description>
    </item>
    
    <item>
      <title>MySQL8.0尝试用json索引替换全文索引</title>
      <link>/mysql/mysql8.0%E5%B0%9D%E8%AF%95%E7%94%A8json%E7%B4%A2%E5%BC%95%E6%9B%BF%E6%8D%A2%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql8.0%E5%B0%9D%E8%AF%95%E7%94%A8json%E7%B4%A2%E5%BC%95%E6%9B%BF%E6%8D%A2%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/</guid>
      <description>原因  MySQL8.0.22版本 线上有一张task表的users字段因历史原因 存放了以逗号分隔的用户id列表 程序中会匹配用户id进行查询 用到的SQL如下：  # 查询 select users from task limit 2; | users | |300511164303031, 310406164883350, 151134164673502, 330203164377115, 310633164035316, 310408164888300, 170515164003106, 150636164603618, 310510164335822, 151336164653174, 310508164331806, 301115164423156 | |310406164883350,181138164432020,1000130,330312164322768,170515164003106,300608164825431,331015164472774,150304164442136,331108164613233,1000164,301113164430265,171016164003026,300333164732303,151134164673502,1000143,331034164487883,181033164253337,310633164035316,150304164442101,1000136,330312164636073,310508164331806,330302164334267,181017164275220,301115164423156,330203164377336,310303164733465,330312164322726,330203164377115,310408164888300,311116164231848,1000123,310214164825778,301317164618388,300333164732155,151013164628330,300511164303031,1000138,1000185,150636164603618,300415164783624,310237164871433,310510164335822,151336164653174,330210164387154 | -- 数据和表名，列名已做掩码转换。非真实数据 # 示例 select * from task where MATCH(users) AGAINST(&#39;19323422341234&#39; );	 表的数据量不多40多万条记录 但是频繁的出现慢查询(超过500毫秒)  优化思路  定位到全文索引慢的时候 第一反应是拆了这个全文索引查询 业务方将逗号字段拆表的改动量大暂时不考虑 折中办法是将这个字段换成json类型 然后用json的索引来替换全文索引 我在想这个方案的时候 给忠哥的预估是性能会提升3-10倍 当时没做测试 靠的是经验和信口开河 一通怂恿说服了研发同事 开始拉群开整  验证和测试 # 加json字段 alter table task add users_list json ; # 填值 update task set users_list = concat(&#39;[&#39; ,TRIM(BOTH &#39;,&#39; FROM users),&#39;]&#39;) where users is not null and users !</description>
    </item>
    
    <item>
      <title>信创和国产数据库</title>
      <link>/dba/%E4%BF%A1%E5%88%9B%E5%92%8C%E5%9B%BD%E4%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E4%BF%A1%E5%88%9B%E5%92%8C%E5%9B%BD%E4%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93/</guid>
      <description>什么是“信创”  “信创”的全称是“信息技术应用创新产业”，旨在实现信息技术领域的自主可控，保障国家信息安全。信创产业的主体包括基础硬件、基础软件、应用软件、信息安全、系统集成等部分。 数字化基础设施的安全可控关系到经济发展，直接决定着供应链安全、产业链安全和信息化安全，进而决定国家安全。近年来，国际局势波诡云谲，在频繁的制裁断供事件下，科技自立自主自强变得更加迫在眉睫。“十四五”规划纲要已明确要将科技自立自强作为国家发展的战略支撑，2023年两会上发布的《政府工作报告》强调要建设现代化产业体系，推进科技自立自强。《数字中国建设整体布局规划》则提出要构筑自立自强的数字技术创新体系，筑牢可信可控的数字安全屏障。2022年我国中央及各地政府更是相继发布了上百条信创相关政策，构建具有完全自主知识产权的创新技术体系。 通俗的说是用国产硬件(芯片,存储,) 划重点:自主可控 划范围:基础硬件、基础软件、应用软件、信息安全、系统集成 在实际执行中核心的是：芯片、操作系统、数据库、中间件、整机  信创的主要厂商  芯片CPU：飞腾、鲲鹏、海光、龙芯、兆芯、申威 操作系OS：普华软件、中标麒麟、银河麒麟、统信UOS、红旗、中科方德、中兴新支点 数据库DB：武汉达梦、人大金仓、神州通用、南大通用、万里开源、华为GaussDB、阿里Oceanbase 中间件：东方通、金蝶、宝兰德、华宇软件、普元信息 办公软件：金山软件、福昕软件、万兴科技 安全保密：三六零、奇安信、中孚信息、万里红、格尔软件  信创的市场 行业  信创体系覆盖2+8+N个领域 2:即党、政 8:金融、电力、电信、石油、交通、教育、医疗、航空航天8个关于国计民生的重要行业 N:N个消费市场。N个行业中的办公OA、编辑类的国产软件。  市场规模  预计2023年中国信创产业规模将达20961.9亿元 2027年有望达到37011.3亿元  方向  信创的国有自主可控软硬件替代国外的商业软件一般采用两种方式进行 方向一：上云，通过将服务迁移或合并至阿里云，电信云，华为云，腾讯云&amp;hellip;由云厂商提供信创服务 方向二：自主替换，可以由单位自身的研发团队或第三方国产厂商支持完成软硬件服务的替换  信创的进展  进展非常快，趋势明确 除部分领域（芯片,操作系统）进展不顺利外 存储,数据库，整机，中件间等领域进展得非常顺利 大量的国产硬件,数据库已经完成了对国外商业软件的替换 部分单位和关键行业也完成私有云或公有云的迁移。 进展非常顺利的原因： 1.国家政策要求 2.国外商业软硬件有巨大的利润空间，这部分利润空间可以节省出来 最核心的信创是：芯片，存储，操作系统，数据库，通用软件 下面从我了解的角度来展开聊聊国产数据库的那点事 有哪些是真国产，哪些是假国产 哪些是真的自主研发，哪些是披着皮的洋鬼子  国产数据库 国产数据库的发展  国产数据库在信创政策出来之前 就已经在茁壮成长了 原因一：国外的商业数据库太贵了 一套oracle集群收费每年可能要到几十万块钱 正版的太贵，只能商业谈判走折扣 有的甚至直接用盗版 原因二：国外的开源数据库技术发展 主要是mysql,Pg的发展 给国内的厂商和技术团队提供了方便的二次开发定制的机会 等到信创的政策出来后 巨大的利润空间和强大的需求 引起国产数据库全面开花 这些年国产数据库的创业团队如雨后春笋一样 爆炸增长 其中有传统的老厂商 也有踩着互联网浪潮过来的创业新厂 这些国产数据库厂商不管有多少家 但总是逃不过以下三个大类 1.</description>
    </item>
    
    <item>
      <title>当程序出现了个bug</title>
      <link>/book/%E5%BD%93%E4%BD%A0%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%87%BA%E7%8E%B0%E4%BA%86%E4%B8%AAbug/</link>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%BD%93%E4%BD%A0%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%87%BA%E7%8E%B0%E4%BA%86%E4%B8%AAbug/</guid>
      <description>今天下午 研发有个数据误更新 需要回滚数据 在利用DBA提供的数据找回功能时 刚好我在旁边看着他在处理 眼瞅着生成的加滚SQL文件没有换行 一个大文本没有分行 所以有文字都在一行上 很难阅读 研发得自已处理下这个文件才能看 我让他换个文本编辑器试一下 结果还是不分行 我问另一个经常用这个功能的同事 以下是对话 - “你以前用的时候也是这样吗？” - 答：oracle就是不会分行,MySQL没问题 - “那怎么没跟我提起过呢“ - 答：以为就只能这样，不分行了处理下也能用 - .... -  我说这是一个bug 得处理一下 所以bug第一步  0.发现问题  一个平台或功能得有个反馈的机制 经常有同事或用户 在使用时发现了问题 不一定会主动反馈 需要有一定的鼓励措施 我在做大多数功能的时候 会强调如果使用过程中有任何问题 都可以找我沟通 每次有人找我反馈问题时 都会积极响应 （有时候是用户操作有问题引起的）  1.确认和复现问题  收到反馈后 需要自己确认和复现这个问题 要有再次触发的条件 某些极端的情况下 可能会很难触发和复现问题 此时需要自己阅读代码 找到可疑的模块 再在脑海里重构流程 从而发现问题 我知道这个能力不是所有人都具备的 但个人觉得这是一个好的程序员 应该具备的能力 在此过程中 有一定的概率会发现 这个问题可能是个bug 可能不是个bug 如果不是bug也需要找到问题所在（通常都是流程的条件不具备）  2.定位bug代码  通过现象 找到问题代码 这个定位过程有长有短 有时候甚至需要很久很久 反复的阅读代码 增加日志 才能定位到问题代码 如果是很长时间以前的代码 那简直是个灾难 也因为这个原因 我不喜欢把项目的架构设计得太复杂 因为时间长了 我可能已经记不得当时是怎么设计的了 就很头疼 在我主导的项目里 都力求简单 类设计通常都是按小模块独立 有时候我也需要阅读和调试别人的代码 讲真的 有的时候真的非常费劲 这可能就是不同人的不同表达差异 我也跟正经的研发同学 聊过这类问题 其中有非常善于review别人代码的 经常发现和定位问题的团队管理人员 他们有一种能力 就是架空与代码层去思考bug 以及某种直觉 猜到可能是哪个逻辑没有处理好 这个真的很厉害 需要长时间的一线代码管理 以及不断的定位和处理bug 才能产生的熟练程度  3.</description>
    </item>
    
    <item>
      <title>数据库工具选型</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B7%A5%E5%85%B7%E9%80%89%E5%9E%8B/</link>
      <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B7%A5%E5%85%B7%E9%80%89%E5%9E%8B/</guid>
      <description> 一个采访
 我的回答 1、对于您而言，选择使用数据库工具的原因是什么？为了解决哪些需求和痛点？  提高人效,减少DBA重复执行某些固定操作的时间成本 避免人为误操作,避免误操作等潜在风险 保证操作一致性,确保任务和流程按一致的方式执行,避免不同的DBA有不同的检查和执行动作造成的潜在问题  2、在进行数据库工具选型时，可以采取哪些方法和策略（可以从需求分析、候选工具筛选、实验和评估、考虑业务场景、参考案例、最终选型、持续优化和调整等方面展开）？  第一步:确认需求,明确我们需要工具来解决什么问题,基本需求是什么,最好可以实现什么样的效果 第二步:是否可以从现有的平台工具上扩展或衍生出来,如果可以改造和研发的成本是多少 第三步:同行交流及内部沟通,看看其他公司是如何解决此类问题,是否有类似的解决方案 第四步:从收集到的数据库工具中,筛选:功能是否满足需求,是否开源，开发语言是什么,是否适合二次开发,稳定性如何, 第五步:实验，在筛选的数据库工具中，在测试环境进行试用,如果需要改造的,投入小部分资源验证改造可行性 第六步:集合到现有的DBA平台中,让所有的工具在一个平台下使用  3、在选型过程中，需要综合考虑数据库工具的哪些关键因素？  功能性:能帮我们解决什么样的问题 可靠性:是否会带来其他的问题，尤其是涉及安全问题 行业认可度,尽可能的避免小众的解决方案和工具 可扩展性:是否开源及开发语言是什么,会优先选择团队习惯的开发语言的开源工具  4、数据库工具既有免费的，也有付费的，亦有国外和国内的软件之分，您更趋于选择哪一类？为什么？  趋向于选择开源免费的,付费的数据库工具很少会使用,一方面数据库软件的收费通常都比较贵,使用成本不低,另一方面我们选择数据库工具时更多的还是需要集成到现有的dba平台中,开源的改造和调试难度更低，一些自定义的需求和功能，需要二次开发，这时候开源的优势就非常明显 国内和国外的软件没有倾向性,如果是复杂的工具，偏向国内的一点,中文文档还是阅读性上还是比翻译过来的文档更容易理解  5、您使用过哪些数据库工具，它们的适用范围，优缺点分别是什么？在使用过程中，可能会遇到哪些问题？您有什么运维建议吗？  inception和goinception:数据库审核工具,适用于SQL审核上线,几乎完美的解决了SQL上线审核的难点和痛点问题，非常方便集成到现有系统中 gh-ost:数据库上线工具，一般联合上面的inception工具一起使用完成SQL上线，需要注意的是在一些特定情况下有数据丢失的风险 datax和cannal:优秀的数据同步工具，可以应付大多数的数据同步任务。 redis-shake和mongo-shake:在redis和mongo迁移时用到的工具,非常方便  6、在您看来，优秀的数据库工具应该具备哪些功能？请您推荐几款上手快、安装简单、方便易用、支持多种数据库的数据库管理工具？  优秀的数据库工具功能：稳定可靠,解决问题,方便集成和二次开发 推荐goinception,gh-ost以及集成了众多数据库工具的Archery  7、作为用户，您认为目前主流的数据库工具，还存在哪些需要改进的地方？  已经做得很好了，可能在数据防火墙等数据库安全领域还缺少一些通用的解决方法和工具  AI的回答 1、对于您而言，选择使用数据库工具的原因是什么？为了解决哪些需求和痛点？  提高数据管理效率：通过使用数据库工具，可以实现对数据的快速存储、查询和分析，从而提高工作效率。 支持复杂查询和分析：数据库工具通常具有强大的查询和分析功能，可以满足各种复杂的业务需求。 数据安全性和可靠性：数据库工具可以提供数据备份、恢复和高可用性等功能，确保数据的安全性和可靠性。 成本效益：选择合适的数据库工具可以降低硬件、软件和维护成本，提高投资回报率。  2、在进行数据库工具选型时，可以采取哪些方法和策略（可以从需求分析、候选工具筛选、实验和评估、考虑业务场景、参考案例、最终选型、持续优化和调整等方面展开）？  需求分析：明确业务需求和应用场景，了解所需的性能、功能和安全性要求。 候选工具筛选：收集耿耿于意的数据库工具信息，如开源与商业产品、国内外厂商等，并进行初步筛选。 实验和评估：针对初选的工具进行实际测试，包括性能测试、功能测试和安全测试等，以评估其是否符合需求。 考虑业务场景：根据实际业务场景和需求，选择适合的工具，例如分布式数据库适用于高并发场景，关系型数据库适用于事务馊处理等。 参考案例：查阅相关案例和评价，了解其他用户的使用经验和效果，为选型提供参考。 最终选型：综合以上因素，确定最适合的库工具。 持续优化和调整：在实际使用过程中，根据实际情况对工具进行优化和调整，以提高性能和满足新需求。  3、在选型过程中，需要综合考虑数据库工具的哪些关键因素？  剰性能：根据业务需求，选择具有足够性能的数据库工具。 功能：选择具备所需功能的工具，如数据存储、查询、分析和备份等。 安全性：选择具有强大安全特性的工具，如数据加密、访问控制和审计等。 可扩展性：选择可以随着业务增长而轻松扩展的工具。 成本：考虑工具的购买、维护和支持成本，以及可能带来的经济效益。 兼容性：选择能与现有系统和应用兼容的工具。  4、数据库工具既有免费的，也有付费的，亦有国外和国内的软件之分，您更趋于选择哪一类？为什么？  免费或开源的产品：这些产品通常具有较高的性价比，可以降低初始投入成本。 成熟稳定的产品：选择经过市场验证且具有良好口碑的产品，以确保稳定的运行和使用体验。 国内产品：考虑到文化背景和技术支持等因素，选择国内厂商的产品可能会更加合适。但具体情况还需根据项目需求和个人经验来判断。  5、您使用过哪些数据库工具，它们的适用范围，优缺点分别是什么？在使用过程中，可能会遇到哪些问题？您有什么运维建议吗？    Microsoft SQL Server：适用于大型企业级应用，如金融、零售等行业。优点是功能强大，性能稳定，易于管理和维护。缺点是价格较高，对于小型企业来说可能过于昂贵。在使用过程中，可能会遇到性能瓶颈、数据安全问题等。运维建议是定期进行性能调优、备份和恢复演练，以确保数据的可靠性。    MySQL：适用于中小型企业级应用，如博客、论坛等。优点是开源免费，性能良好，易于学习和使用。缺点是功能相对较弱，对于大型复杂应用可能不够灵活。在使用过程中，可能会遇到性能瓶颈、数据安全问题等。运维建议是定期进行性能调优、备份和恢复演练，以确保数据的可靠性。    Oracle Database：适用于大型企业级应用，如金融、电信等行业。优点是功能强大，性能稳定，适用于高并发和高可用性场景。缺点是价格较高，对于小型企业来说可能过于昂贵。在使用过程中，可能会遇到性能瓶颈、数据安全问题等。运维建议是定期进行性能调优、备份和恢复演练，以确保数据的可靠性。    PostgreSQL：适用于中小型企业级应用，如数据分析、物联网等。优点是功能强大，性能稳定，支持复杂的查询语句。缺点是非商业化，对于大型企业来说可能不够可靠。在使用过程中，可能会遇到性能瓶颈、数据安全问题等。运维建议是定期进行性能调优、备份和恢复演练，以确保数据的可靠性。    6、在您看来，优秀的数据库工具应该具备哪些功能？请您推荐几款上手快、安装简单、方便易用、支持多种数据库的数据库管理工具？  优秀的数据库工具应具备以下功能：   强大的数据管理功能，包括表结构设计、数据导入导出、数据备份与恢复等。    高效的数据查询和分析功能，支持复杂的查询语句和统计分析。    良好的扩展性和性能，能够应对大规模数据和高并发访问。    丰富的安全特性，确保数据的安全性和完整性。    友好的用户界面和便捷的操作方式，降低学习成本和使用难度。    7、作为用户，您认为目前主流的数据库工具，还存在哪些需要改进的地方？  推荐的几款上手快、安装简单、方便易用、支持多种数据库的数据库管理工具 understatement, pgAdmin, Sequel Pro 和 DBeaver。这些工具都具有良好的用户体验和功能，支持多种数据库，可以根据您的需求选择合适的工具。  总结  AI语言组织上更流畅，在讲功能作用画大饼时候比真人组织得有条理 但真实的工具，核心要解决的问题，离我还早。  </description>
    </item>
    
    <item>
      <title>爱狗人和恨狗人</title>
      <link>/book/%E7%88%B1%E7%8B%97%E4%BA%BA%E5%92%8C%E6%81%A8%E7%8B%97%E4%BA%BA/</link>
      <pubDate>Sun, 22 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E7%88%B1%E7%8B%97%E4%BA%BA%E5%92%8C%E6%81%A8%E7%8B%97%E4%BA%BA/</guid>
      <description> 2023年10月16日8时 成都一只没拴绳的罗威纳犬撕咬小孩 看了完整的视频 很难理解作为人类不会因同类的伤害感到愤慨 对这种爱狗养狗人士无知给社会带来的危害而选择视而不见 后来的连锁反应是 一些地方的民警和城管开始抓流浪狗 再然后民间的爱狗人士又开始出来说流浪狗有多可怜 再有杨迪和一些明星出来说流浪狗可怜 不管是张馨予的长微博还是杨迪的短视频 看起来都挺有理的 他们在自己的角度说他们这样的养狗人是怎么怎么和狗和谐共处 以及他们和狗狗们的心心有印 明星有条件可以做到“文明”养狗 普通人呢 普通人有条件对狗做各种训练吗 普通人有独栋别墅来不打扰邻居吗 我非常反感养狗人士 这跟爱心无关 主要是“不打扰别人”  遛狗不牵绳  傍晚的时候在小区里公园里 至少有一半的遛狗的都不牵绳子 “我的狗狗不咬人，牵绳子干吗” 问就是说他的宝贝不咬人 这种神仙逻辑在于 1.没咬人之前你当然以为你家狗不咬人 2.不咬人它还是会打扰到别人 比如扑过去闻的，吓唬别人的&amp;hellip; 以及有些人天生对狗就不喜欢 比如说我 我从来不觉得这种带毛的动物有啥可爱之处的 狗在我旁边经过 不管是大只的还是一脚就能踩死的小狗 我都不喜欢 都会感到紧张 当一只狗热情的过来凑热闹 我是真实的觉得被恶心和冒犯到的 每个人有权利保持自己的爱心和爱好 但是都应该想到自己的爱好 不能建立在对其他人的正常生活或权利产生侵害上面 你也不能把你家狗不咬人这种设定 强行要求别人去接受和理解 更不能选择性的觉得别人也喜欢你家狗去接近 当侵犯别人私人空间的事情 发生了或者潜在发生了 请意识到让别人为你的爱好买单是种错误的行为  侵犯别人的空间  在公共场合的言行 跟在家或私人关系不同 保持互相尊重 不侵犯陌生人的空间 应该成为常识 我会经常思考和反思 自己的行为是否打扰到别人 这些年我也会经常看到有人开始讨论 在公共场所吸烟 在安静封闭的公众场合大声喧哗 广场舞 插队 甚至随地大小便 &amp;hellip; 我很欣慰越来越多的人开始意识到 公共场合的“文明” 就是保持人和人之间的距离 不同与熟人,爱人,亲人 在公共场合 人和人之间是要保持距离的 所以爱狗人士要明白 一旦你的狗离开了家门 它的行为就可能对邻居 对一起坐电梯的人 对路上的行人 对小区里公园里正常遛弯的行人 造成了“麻烦” 这个“麻烦”不是你自以为是的小事 极端情况下 可能像成都那个恶狗一样 对一个无辜的一家人造成不可挽回的损失 就这还有脸说狗不咬人不拴绳的  控制力  大多数养狗的人 会对自己狗如此有信心 是因为一部分场景下 验证了狗的听话 对自己的控制力过分自信了 国内现状下养狗人 可能只有20%的狗 是办过狗证的 大多数的狗可能都没有经过训练和筛选 都是凭着狗主人的主观感受 来断定狗是否听话和忠诚 然后流行的是 一些胳膊细得跟胡萝卜一样 抓一只鸡都抓不牢的女人 喜欢养一些大型犬禁养犬 这些狗根本就不是她们能拉得住的 牵根绳子也就是装个样子 经常会被狗牵着走 &amp;hellip; 这种身处危险而不自知的愚蠢 让人无奈同时 而这种愚蠢还给周围的其他人带来了危害 所以一有这种恶犬伤人事件出来 各平台的异烟肼的搜索量就上来 这世界上有人喜欢刺激和冒险 也有很多普通人 不希望把自己放在危险下 是件很正常的事  关于陪伴  这世上还是有很多喜欢狗 或者说喜欢宠物的人 尊重他们 好像只有养狗这个群体 会妨碍到其他人 所以我对养其他宠物的人 都很理解和支持 人嘛 有点自己的喜好 本身就是件好事 (前题是别影响其他人) 有情感有喜好 对自己喜欢的人或物投入感情 所以才是有血有肉的人 我也喜欢动物 比如动物园里的动物 cctv9，10里的动物 比如朋友家养的猫 我妈以前也养过好久的猫 我自己养过乌龟和鱼 养鱼是很多很多年前的事 那会我刚从学校走入社会 和当时的女朋友分手的时候 她送过给我一盆鱼 努力的喂鱼,定时换水 养了很长的时间 不知道为啥 那会我还是比较闲的 换作现在不一定有这么耐心了 再然后大约是16年前后 试着养乌龟 因为听说这玩意特别简单 买了个专用的水景鱼缸 部了景 去花鸟市场买了最皮实的巴西龟 还有乌龟食物 结局一样 一只接一只的挂了 后来就放弃了养动物的心了 我养了两次宠物 但都不是真正的宠物 因为它们其实不会陪伴 不会互动 人和宠物的关系 在不同的影视作品和文字中都有感受过 动物或者说宠物对人的陪伴 心理上的作用 我也能感受到 但我却没有这样的经历或投入 我更喜欢人 觉得动物再灵性 也没有人可爱和机灵 真的 宠物不如宠人  </description>
    </item>
    
    <item>
      <title>一段电视讲话看巴以冲突的美国人态度</title>
      <link>/book/%E4%B8%80%E6%AE%B5%E7%94%B5%E8%A7%86%E8%AE%B2%E8%AF%9D%E7%9C%8B%E5%B7%B4%E4%BB%A5%E5%86%B2%E7%AA%81%E7%9A%84%E7%BE%8E%E5%9B%BD%E4%BA%BA%E6%80%81%E5%BA%A6/</link>
      <pubDate>Fri, 20 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E4%B8%80%E6%AE%B5%E7%94%B5%E8%A7%86%E8%AE%B2%E8%AF%9D%E7%9C%8B%E5%B7%B4%E4%BB%A5%E5%86%B2%E7%AA%81%E7%9A%84%E7%BE%8E%E5%9B%BD%E4%BA%BA%E6%80%81%E5%BA%A6/</guid>
      <description>10月19晚上美国总统拜登跟以色列总理见面后发表电视讲话 美国和以色列明显的在中东在欺负人 加沙那个地方都被圈成那样了 加沙那个地方都被圈成那样了 但是在美国政客眼里 依旧能理直气壮的说出来 “以色列被欺负了 美国要坚定的帮助以色列” 如果是大毛和二毛打架 帮乌克兰还能说得通 但是以色列都把巴勒斯坦的加沙欺负成那样了 眼瞅着加沙就要被推平了 这也能站在真理的一方 说得头头是道 哪有什么公平正义 对政客来说 屁股早就歪了 我的英文水平还是有点费劲 得借助翻译软件才能看懂 还是羡慕那些英语好的人 想学却学不会的技能 只能羡慕了 术业有专攻 技能也有方向 当我在羡慕别人的语言能力时 说不定也有人会羡慕我的代码表达能力 各自加油.  以下是全文
Good evening, my fellow Americans.
We’re facing an inflection point in history. One of those moments where the decisions we make today are going to determine the future for decades to come. That’s what I’d like to talk with you about tonight.</description>
    </item>
    
    <item>
      <title>测试环境治理数据库相关部分</title>
      <link>/dba/%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%B2%BB%E7%90%86/</link>
      <pubDate>Thu, 19 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%B2%BB%E7%90%86/</guid>
      <description> 下午邀请了去哪儿网的测试同学来公司分享 介绍了去哪儿网的测试环境3.0版本 1.0是固定环境(实体) 2.0是noah平台(测试云平台) 3.0是软路由平台 这里面的代际变更和功能点这里不多做介绍 我想总结和对比的是测试环境治理时的数据库部分  测试环境的数据库的不同阶段需求  1.业务量小的时候，一套测试环境就可以了(&amp;lt;10人研发:test) 2.继续发展,需要把测试环境和开发环境分离出来(10-30人研发:test,dev) 3.业务继续壮大,研发人员开始出现金个分组,需要多套测试环境(30-100人研发团队:test1,test2,dev) 4.业务继续壮大,研发人员开始出现金个分组,需要多套测试环境(100-500人研发团队:test1,test2,test3&amp;hellip;,dev,preprod) 5.业务继续壮大,研发人员分事业部，需要云环境(500+研发团队:测试云环境)  多套测试环境的数据结构同步  这里要分两种情况来区别处理 如果是测试云环境，需要有个基准环境出来，过程中有大量的自动化运维参与 如果是独立部署的测试环境,需要手动同步  测试云环境  以生产库为最终标准 定义一个基准环境，基准环境的数据库结构会实时跟线上同步 生产环境的上线将会自动应用到基准环境 其他环境的数据结构会和基准环境对齐 在一个固定的时间点，自动或手动的强制和基准环境对齐 允许非基准环境下的数据库结构和基准环境不一致 非基准环境造的数据和变更会随着环境一起销毁 非基准环境可以随时销毁，重新从基准环境拉一套出来 基准环境的数据维护是件很重要的事  独立部署的测试环境  在非云状态的固定的独立部署的一套或多套测试环境 基准环境就是生产环境 数据库表结构会和生产环境对齐 每个固定的时间点所有测试环境都会和生产环境对比表结构和其他对象 发送出来异常报告 是否修复需要测试或研发人员自行决定是否对齐 SQL上线时可以选择多环境上线，一段脚本在多个环境同时执行 参见：数据库多环境SQL上线。  造数据问题  测试环境的数据谁来造？ 字典类的基础数据从线上同步（允许定时同步） 客户,订单类数据尽可能的由测试人员随机生成 部分数据允许按一定规则从线上同步回来（需要脱敏） 测试环境的数据也会有完整的防误删和备份策略  </description>
    </item>
    
    <item>
      <title>二进制和太极,两仪,四象,八卦,五行</title>
      <link>/book/%E5%A4%AA%E6%9E%81%E7%94%9F%E4%B8%A4%E4%BB%AA%E4%B8%A4%E4%BB%AA%E7%94%9F%E5%9B%9B%E8%B1%A1%E4%B9%8B%E5%85%AB%E5%8D%A6%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6/</link>
      <pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%A4%AA%E6%9E%81%E7%94%9F%E4%B8%A4%E4%BB%AA%E4%B8%A4%E4%BB%AA%E7%94%9F%E5%9B%9B%E8%B1%A1%E4%B9%8B%E5%85%AB%E5%8D%A6%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6/</guid>
      <description>从莱布尼茨说起  莱布尼茨（Leibniz，1646年7月1日－1716年11月14日） 德国哲学家,数学家,律师 他在1679年发明了二进制 他和牛顿先后独立发现了微积分，大学里的微积分的那些奇怪的数学符号大部分是这个人画的  1701年法国汉学大师若阿基姆·布韦向莱布尼茨介绍了《周易》和八卦的系统。 莱布尼茨表示：“阴”与“阳”基本上就是他的二进制的中国版。   二进制  二进制是计算机世界的主宰 所有计算机(电脑/网络/手机&amp;hellip;)相关的设备里的存储和通信都是用二进制的0和1来表示 无论是电脑里的几百G的游戏或手机里的高清视频 还是程序员写的一段代码 或是我此刻输入的这段文字 都会被转换成0,1这两个简单的数字来存储，来传输  二进制与八卦  无论是三百多年前的莱布尼茨，还是现在的程序员 都发现八卦和二进制的奇怪契合 八卦可以很容易直观的用二进制表示出来 都是0-阴,1-阳来表示 都可以通过增加位数来表达更充实的数据 还都可以用二进制进行运算  两仪生四象  两仪生四象 两仪是指(阴:0，阳:1) 四象是指(太阴,少阳,少阴,太阳) 用阴(0)生成出来的:00太阴,01少阳 用阳(1)生出来的:10少阴,11太阳  四象也可以用季节表示(春:01,夏:11,秋:10,冬:00) 这里面冬天(00)阴气到达极点，阳气开始升发，阴气减少。 春的二进制表示是:01 夏阳气到达极点:11 秋阳气开始退去:10 冬又变回了:00 于是我们有了：两仪生四象 及四象的运转规律   四象生八卦  当用两根线表示的时候，阴阳生成了四象 那用3根线表示的时候，就生成了八卦  坤：黑黑黑，卦符阴阴阴，二进制数为000 艮：黑黑白，卦符阴阴阳，二进制数为001 坎：黑白黑，卦符阴阳阴，二进制数为010 巽：黑黑白，卦符阴阳阳，二进制数为011 震：白黑黑，卦符阳阴阴，二进制数为100 离：白黑白，卦符阳阴阳，二进制数为101 兑：白白黑，卦符阳阳阴，二进制数为110 乾：白白白，卦符阳阳阳，二进制数为111    而四象生八卦的方式是：         六十四卦  继续沿生下去：八八重卦，得六十四卦。 也就是说用三枚铜钱的正反面来算8卦的时候 需要6枚铜钱来算64卦  只是这六十四卦很多生僻字 我也难得去细细看每一卦是啥意思了 看这个方圆图就觉得挺事事的   五行  古人认为世间的万物都是由金、木、水、火、土五种元素 因为我的名字跟五行有关 所以金木水火土这五个字比较在意 八卦和五行是互相转换的 八卦的字太难念了，很多字我都不认识，更别说古人了。 于是我们可以换成通俗一点的表示 乾代表天 坤代表地 巽代表风 震代表雷 坎代表水 离代表火 艮代表山 兑代表泽 八卦对应的五行分别是： 金:天,泽； 木:雷,风； 土:地,山； 水:水； 火:火 除掉天,泽对应金不好理解外，其他基本上都挺好理解的 关于金:天,泽，可以用京东来辅助记忆下  五行及其应用  说到五行 这时候神神叨叨的中医又出场了 做为一个中医黑 肯定是不信中医这套奇怪理论的 但是不妨看看他们的五行之说  以及五行的其他表达形式     五行 木 火 土 金 水     五气 柔 息 充 成 坚   季节 春 夏 长夏 秋 冬   方位 东 南 中央 西 北   气候 风 热 湿 燥 寒   五天 苍天 丹天 黄天 素天 玄天   五气色 青 红 黄 白 黑   五宫 东宫 南宫 中宫 西宫 北宫   五味 醯 酒 饴蜜 姜 盐   五形 直 尖 方 薄 圆   方位 左 上 中 右 下   五菜 韭 薤 葵 葱 藿   五果 李 杏 枣 桃 栗   五化 生 长 化 收 藏   五谷 麻麦 黍 稷 穀稻 豆   五畜 鷄 羊 牛 马 彘   五律 角 征 宫 商 羽   五虫 毛虫 羽虫 倮虫 介虫 麟虫   五方 东 南 中 西 北   五岳 泰山 衡山 嵩山 华山 恒山   五地 山林 丘陵 原湿 坟衍 川泽   五彩 青 赤 黄 白 黑   五帝 太昊 炎帝 黄帝 少昊 颛顼   五神 句芒 祝融 后土 蓐收 玄冥   五器 规 衡 绳 矩 权   五正 后稷 司马 司徒 司寇 司空   五官 春官 夏官 中官 秋官 冬官   五教 父义 母慈 子笑 兄友 弟恭   五常 仁 礼 信 义 智   五事 视 言 思 听 貌   五德 明 从 睿 聪 恭   五瑞玉 青圭 赤璋 黄琮 白琥 玄璜   五脏 肝 心 脾 肺 肾   五官 目 舌 口 鼻 耳   五体 筋膜 血脉 肌肉 皮毛 骨髓   五声 呼 笑 歌 哭 呻   五情 怒 喜 忧思 悲 惊恐   病位 头项 胸脇 脊 肩背 腰股   五华 爪 面 唇 皮毛 发   五音 嘘 呵 呼 哭 吹   五劳 步 视 坐 卧 立   五嗅 臊 焦 香 腥 腐   五神 魂 神 意 魄 志   五津 泣 汗 湿 燥 寒   五味 酸 苦 甜 辛 咸   五欲 色 味 安佚 臭 声   五恶 肝 心 脾 肺 肾   志伤 怒 喜 思 悲 恐   志胜 悲 恐 怒 喜 思   五象 少阴 老阳 中宫 少阳 老阴   五星 木星 火星 土星 金星 水星   五灵 青龙 朱雀 麒麟 白虎 玄武   五季 春 夏 长夏 秋 冬   六气 风 热、火 湿 燥 寒   五促 生 长 化 收 藏   五时 日旦 日中 日西 日入 午夜   八卦 震巽 离 坤艮 乾兑 坎   天干 甲乙 丙丁 戊己 庚辛 壬癸   地支 寅卯 巳午 辰戌 申酉 子亥   五色 青 赤 黄 白 黑   五臭 臊 焦 香 腥 腐   五味 酸 苦 甘 辛 咸   五畜 鷄 羊 牛 鷄 豚   五谷 麦 黍 稷 稻 豆   五体 筋 血(脉) 肉 皮 骨   五藏 血 脉 营 气 精   五情 怒、惊 喜 思 悲、忧 恐   五液 泣 汗 涎 涕 唾   五官 眼(目) 舌 唇(口) 鼻 耳   五音 角 征 宫 商 羽   五智 魂 神 意 魄 志   五腑 胆 小肠、三焦 胃 大肠 膀胱   五指 食指 中指 大拇指 无名指 小指   五性 少刚(雅) 阳刚(急) 申和(直) 少柔(刚) 阴柔(隐)    </description>
    </item>
    
    <item>
      <title>透明数据加密测试</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%AF%86%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Sun, 08 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%AF%86%E6%B5%8B%E8%AF%95/</guid>
      <description>加密环境  MySQL主从实例&amp;ndash;&amp;gt; 主:127.0.2.30:3401,从:127.0.2.30:3402 MySQL空白实例：127.0.2.27:3405 启用127.0.2.30:3401 主实例的闪电加密  加密工具  **透明数据库透明加密  加密规则  ogg_pump 表：pumpid,tablename 两列做列加密 wc_host_arch 表：cpu,network_in 两列做列加密  加密前后对比  访问数据库得到的数据如下   mysqlw -h 127.0.2.30 -P 3401 dbsec -e &amp;quot;select pumpid,tablename from ogg_pump order by pumpid limit 10&amp;quot; +--------+-------------------+ | pumpid | tablename | +--------+-------------------+ | 1 | SYS_DICTIONARY | | 2 | SYS_USR | | 3 | OWN_HI_TASK_FINAL | | 4 | OWN_HI_PROCESS | | 5 | RC_CST_CAR_INFO | | 6 | RC_CST_BSC_INFO | | 7 | RC_XJNP_APPLY | | 8 | HSJ_BASIC | | 9 | HSJ_ENTINV_ITEM | | 10 | HSJ_FR_POSITION | +--------+-------------------+ mysqlw -h 127.</description>
    </item>
    
    <item>
      <title>DBA操作规范-2023</title>
      <link>/dba/dba%E6%93%8D%E4%BD%9C%E8%A7%84%E8%8C%83-2023/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/dba%E6%93%8D%E4%BD%9C%E8%A7%84%E8%8C%83-2023/</guid>
      <description>基本规范意识  所有操作应该遵循下述原则: 故障优先:当发生线上故障时,需立即暂停原有的工作安排,部分流程规范要求可以适度简化,优先故障恢复 事前通知:影响业务的操作，必须事先和DBA团队负责人确认影响范围。在钉钉里发《运维变更通知》 业务低峰:有潜在业务影响的操作，需和业务方沟通业务低峰执行 备份先行:所有操作，必须事前做好备份,给快速回滚创造条件。包括但不限于：配置文件变更，数据变更,文件删除  服务器操作  连接数据库服务器统一走jumpserver平台 在生产数据库服务器上安装任何软件需经过DBA负责人确认 拷贝大于50G的文件，注意和网络部门确认 生产环境的数据库服务器删除大于500G的文件，需留意IO影响  新建数据库选型  优先MySQL数据库，原则上Oracle,SQLServer,PG,不再支持新建实例和数据库 新建数据库实例需要明确版本,用一键安装脚本完成安装，同时做好实例的备份计划 新数据库选择端口，应考虑：同业务优先，测试和生产同端口  数据库操作  参见《MySQL开发和操作规范》,《Oracle_SQL标准规范》  流程与工单  流程优先:已经做完自动化流程的日常工作，必须走流程，禁止绕开自动化流程的手动操作 工单补充:一些没有自动化流程的工作或特殊操作,须提交DBA工单，不认可：口头沟通,微信,钉钉 与其他部门的协作与沟通也应遵守相应的流程规范要求，例运维部门要求的服务器和网络变更流程规范  权限操作  root,dba账号作为特殊账号不可以配置在业务中，不可以提供给非DBA人员使用或测试 业务新申请账号原则上统一走DBA流程，禁止手动创建账号和密码 用于运维的用户账号，例如：备份账号backupuser,主从同步账号repluser可以手动创建 任何新建账号注意弱密码问题：要求12个字符串的随机字符串 禁止在生产测试申请开通个人帐号，业务和研发查询数据库只能通过DBA平台的页面 原则上不提供给研发人员明文数据库账号和密码，只提供加密后的密码串，统一配置nacos  敏感数据保护  敏感数据包括但不限于：真实姓名、手机号码、邮箱地址、家庭住址、身份证号、银行卡号、交易记录、账号密码等； 严禁私自查询与工作无关的用户或业务数据，包括但不限于：客户信息,员工信息,人力信息,oa信息,财务信息； 禁止在内部群发邮件、群聊、Github、技术分享时泄露敏感数据，比如密码等； 从生产环境同步数据到测试环境，需要和DBA负责人确认：数据量级，是否脱敏 原则上不支持帮业务和研发人员手动导数据的需求，此类需求走DBA平台的在线查询，离线查询 以及大数据库部门的导出  值班制度  值班参考《dba值班表》的排班，每天的值班时间是9:00~次日8:59 当天值班在非工作时间，要求外出带上可随时联网的笔记本。发生故障时15分钟以内可以联网处理问题 当天值班DBA优先处理流程与工单,非工作时间可以用钉钉的DBA模块完成快速审批  </description>
    </item>
    
    <item>
      <title>数据库故障演练纪实</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%95%85%E9%9A%9C%E6%BC%94%E7%BB%83%E7%BA%AA%E5%AE%9E/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%95%85%E9%9A%9C%E6%BC%94%E7%BB%83%E7%BA%AA%E5%AE%9E/</guid>
      <description> 敢不敢随机挑选几台数据库机器，拔掉网线？
 演练时间  2023-09-26 19:30  演练目的  模拟线上服务器异常时 1.业务产生的影响 2.数据库高可用的生效时间 3.业务影响程度  前期准备  DBA将负责的三条数据库线的机器列表提供给运维 由运维同学采用抽奖程序，随机每条业务线抽到一台服务器 DBA检查抽中的服务器上的数据库及影响的业务范围 拉上受影响业务线的研发负责人和相关人士 注：各种leader 开会沟通时间点和风险点 确认时间点2023-09-26 19:30 开始  实际演练过程 影响范围  部分业务线在切换时产生短暂报错（计划中，影响可忽略不计） 数据库这边高可用方案可正常发挥作用，流量切换和高可用都是按预期的进行 总体演练结果，非常顺利。  2023-09-27 更新  Redis的演练导致大数据部门的一个故障 表现为flink的任务卡住了。 排查时还跑错了key和任务，用时较长，故障影响较大 Redis_Cluster集群 ，从节点的断网，也会影响业务短暂异常，如果程序处理不好，会造成很大的问题  </description>
    </item>
    
    <item>
      <title>一个简单有趣的程序</title>
      <link>/ops/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E6%9C%89%E8%B6%A3%E7%9A%84%E7%A8%8B%E5%BA%8F/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E6%9C%89%E8%B6%A3%E7%9A%84%E7%A8%8B%E5%BA%8F/</guid>
      <description>会编程实在是一件很好玩的事 手机里选了300张照片 想着单纯的洗照片有点无聊 得在每张照片上加个单词 这时候 写一段python 简单而高效  代码如下： from PIL import Image, ImageDraw, ImageFont import os # def writeWord(fontname=&#39;沐瑶软笔手写体.ttf&#39;,words=&amp;quot;Python&amp;quot;): # img = Image.new(&#39;RGB&#39;, (800, 400), color = &#39;white&#39;) # draw = ImageDraw.Draw(img) # font = ImageFont.truetype(f&#39;fonts/{fontname}&#39;, 100) # draw.text((100, 150), words, font = font, fill = &#39;black&#39;) # img.save(&#39;PythonArt.png&#39;) def watermark_Image(img_path,output_path, text): img = Image.open(img_path) drawing = ImageDraw.Draw(img) black = (254, 223, 225, 179) font = ImageFont.truetype(&#39;fonts/opposansb.ttf&#39;, 350) drawing.</description>
    </item>
    
    <item>
      <title>【大聪明】微信机器人关机下线</title>
      <link>/book/%E5%A4%A7%E8%81%AA%E6%98%8E%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%85%B3%E6%9C%BA%E4%B8%8B%E7%BA%BF/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%A4%A7%E8%81%AA%E6%98%8E%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%85%B3%E6%9C%BA%E4%B8%8B%E7%BA%BF/</guid>
      <description>   大聪明下线了  大聪明微信机器人不再提供AI回复 生于2023-02-24，卒于2023-09-08  一些时间点  2023-02-20 计划将ChatGPT搬到微信上 2023-02-24 第一版微信机器人上线，基于ChatGPT3.5 2023-02-25 建群:《大聪明的应答测试》 2023-03-18 支持:百度文心一言 2023-03-20 第一个微信群满员 2023-03-24 支持:GPT4.0和bing Chat 2023-03-29 支持:私聊 2023-04-02 建第二群:《大聪明的2群》 2023-04-02 支持:生成图片 2023-04-04 支持:本地训练的机器人（不需要和外网交互） 2023-04-21 支持:生成语音 2023-04-24 支持:图片识别,可以识别图片内容。 2023-04-25 支持:生成视频 2023-05-12 开始做网页版ChatGPT工作平台(www.top580.com) 2023-09-08 下线,不再提供服务  下线原因  ChatGPT热度已经下降 随着热度下降 更新这个功能的热情也不高 大聪明的最后一次代码更新:2023-05-12 更多的精力我在做网页版的AI应用 主要是AI的私有化训练方面 而ai问答这些基础应用 当前各类ChatGPT应用已经随处可见 体验和实用性比微信大聪明要好很多  一些数据  半年的时间里 大聪明共回复了:9201 条消息 其中:1535条私聊消息,7666 群聊消息 共有557人和大聪明在微信上沟通 互动最多的是:李明浩/dba:747次对话,luluci:305次,顾杰~学生:271次 ChatGPT让彼此陌生的人挤在一个群里 共度这一段探索的旅程 AI还在继续进步和发展 感谢大家半年的陪伴 陪我做了一件有趣的事 青山不改绿水长流 有缘江湖再见  </description>
    </item>
    
    <item>
      <title>华为mate60pro开卖,国产麒麟9000S芯片是否打破封锁</title>
      <link>/book/%E5%8D%8E%E4%B8%BAmate60pro%E6%89%8B%E6%9C%BA%E7%AA%81%E7%84%B6%E5%BC%80%E5%8D%96%E5%9B%BD%E4%BA%A7%E9%BA%92%E9%BA%9F9000s%E8%8A%AF%E7%89%87%E6%98%AF%E5%90%A6%E6%89%93%E7%A0%B4%E5%B0%81%E9%94%81/</link>
      <pubDate>Wed, 30 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%8D%8E%E4%B8%BAmate60pro%E6%89%8B%E6%9C%BA%E7%AA%81%E7%84%B6%E5%BC%80%E5%8D%96%E5%9B%BD%E4%BA%A7%E9%BA%92%E9%BA%9F9000s%E8%8A%AF%E7%89%87%E6%98%AF%E5%90%A6%E6%89%93%E7%A0%B4%E5%B0%81%E9%94%81/</guid>
      <description>华为mate60pro手机突然开卖  昨天(2023-08-29)华为商城的一条消息突然冒了出来 华为开卖Mate60pro 这个本来预计在9月底才开发布会的新品突然就开始卖了 时间点选在美国商务部长雷蒙多访华期间(08-27到的北京) 这个消息为什么惊爆在于 这次搭载的麒麟9000S芯片是在美国制裁华为后的国产芯片 目前可收集的信息是：7nm,5G&amp;ndash;&amp;gt;国产 华为这波真的太牛逼了！！！！！！！！ 因为如果9000S芯片能量产的话 国内的芯片被卡脖子问题不说解了 但至少是可以呼吸了 你就说现在这9000s的手机芯片跟高通骁龙对使用者来说有多明显的差别 而且，技术这东西一旦掌握了 就有了底气可以一直在此基础上迭代发展 别人进步的时候，你也可以跟着进步 哪怕进步得没对方快 但是对方卡脖子就没那么容易了 虽然对华为的一些宣传策略不太感冒 但是如果真的做到了国产7nm芯片 只能是：服气 因为7nm的芯片已经可以满足99%的国内芯片使用市场了 华为要是真的做到了 这颗芯片和华为是可能会被写进历史书里的  官网截图     国产麒麟9000S芯片 这个芯片为啥这么惊喜  在这个芯片出来之前国内没有量产7nm芯片的能力 中芯国际工艺只能到14nm，没有更多消息证明中芯国际突破量产7nm工艺，在官网能查到的也只有14nm资料 台积电和三星的代工厂，华为被制裁显然用不了 但是7nm的9000S芯片被量产出来 也就侧面验证了国产（中芯国际）已经可以做出7nm的工艺了 这个突破就很厉害了  麒麟9000S芯片谁生产的  目前收集到的信息：华为与中芯国际合作生产的  麒麟9000S芯片性能如何  麒麟9000S的CPU主频2.62GHz 麒麟9000S的GPU为Maleoon910，主频750MHz 强于骁龙888 略低于骁龙8Gen1 略低于麒麟9000 一句话：基本上是性能不是最强的，但也差不到哪去  DUV还是EUV光刻机  EUV光刻机荷兰那还是封锁状态 中芯国际这次用的还是DUV 而7nm是DUV理论上能做到的极致 中芯应该是掌握了用DUV做7nm芯片的技术 且有效的提高了良品率达到了可以量产  麒麟9000S芯片 一点疑虑  一点疑虑“这芯片是国产的吗？” 这次华为太低调了 从头到尾没有宣传“国产芯片”的重大突破 包括官宣的公告也在强调其他的内容 那么这颗芯片是不是“纯国产” 是存在疑问的 退一步讲 哪怕这个国产芯不一定有想象中的那么纯 但是往前挤的态势还在 有了技术突破和积累 我们看到芯片贸易战的一点亮光 希望总是让人兴奋的  </description>
    </item>
    
    <item>
      <title>日本排放核废水的前因后果</title>
      <link>/book/%E6%97%A5%E6%9C%AC%E6%8E%92%E6%94%BE%E6%A0%B8%E5%BA%9F%E6%B0%B4%E7%9A%84%E5%89%8D%E5%9B%A0%E5%90%8E%E6%9E%9C/</link>
      <pubDate>Thu, 24 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%97%A5%E6%9C%AC%E6%8E%92%E6%94%BE%E6%A0%B8%E5%BA%9F%E6%B0%B4%E7%9A%84%E5%89%8D%E5%9B%A0%E5%90%8E%E6%9E%9C/</guid>
      <description> 北京时间2023-08-24中午12:00 也是日本福岛第一核电站启动核污染水排海的时间  起因  2011年3月11日，日本福岛县发生了里氏9级地震 福岛核电站的降温装置，导致降温机组出现停机。几十万度的反应堆堆芯瞬间升温，引发了著名的福岛核泄露事故 这是继1986年苏联切尔诺贝利核事故之后 世界上最严重的核事故之一 核泄露发生后东京电力公司开始引海水灌注反应堆，将堆芯温度给降下来 这些废水，不同于平时核电站的外循环用到的冷却水 这些水是接触到了核污染的水，叫做：核废水 这些核废水非常危险 经过处理后核废水中仍然残留了74种放射性物质 其中包括的氚、碳-14、锶-90、钚-239等 这些物质都有较长的半衰期 可以在环境中持续存在数千年甚至数万年 如果通过食物网进入人体内 会对身体健康导致不可避免的危害性 包含癌症 基因遗传基因突变等病症 所以全世界的人都知道这些核废水很危险 于是日本在海边建了很多的罐子来存放这些核废水 存了一罐又一罐，全都封存在储水罐里 福岛的核事故还没处理完 新的核废水还在不断产生，储水罐不可能永远建造下去 而且这些罐子一直放在那也不是个办法   核废水的处理方案      第一个方案是，排。经过一定的稀释处理后，再直接排回大海。代价：污染海洋 第二个方案是，烧。把核废水送进锅炉里烧，排到空气里去。代价：污染空气 第三个方案是，埋。从地表钻洞，然后搞一根深入地底达2500米的管线，把核废水排入地底2500米深处。代价：污染地下水 第四个方案是，电解。将核废水经过电解变成氢气和氧气，然后再排放进大气 。代价：污染空气 第五个方案是，固化。混入水泥，形成这样一个个水泥块，埋进土里。代价：污染地下水 5个方案里越往后，成本越大 只有第一个方案最偏宜 而且对日本本土的伤害最低 很明显对日本来说 其他方案都不如第一个 虽然对海洋生态确实会带来伤害 但是西方国家（美国）不反对 也没人能拦得住他 且日本百分之八十左右的日本人是支持日本政府决定将核废水直接排放入海。  核废水的影响  这是今年5月份的日本核废水的检测报告 https://www.iaea.org/sites/default/files/first_interlaboratory_comparison_on_the_determination_of_radionuclides_in_alps_treated_water.pdf  从里面看放射性物质还是很多很重的 不是日本媒体宣传的只有氚超标这么简单 数据上来这些核废水是非常非常危险的 那么影响有多大呢？ 首先看日本的排放方案是分批次排放 充分利用海洋的稀释能力 再加上排放前可能会做一些净化处理 所以总体上方案是可信的 不要低估海洋的稀释能力 那些水放在整个海洋里也是非常非常小的存在 大约是一条河里的倒进去一杯咖啡 因为日本海附近洋流的顺时针运动 最先影响的是俄罗斯，加拿大，美国 反而离日本近的中国，韩国受其影响小一些  海鲜还能不能吃？  能吃 但没必要 因为我本就不喜欢吃海鲜 现在还多了一个理由：放射性超标  </description>
    </item>
    
    <item>
      <title>宇宙的尽头是什么</title>
      <link>/book/%E5%AE%87%E5%AE%99%E7%9A%84%E5%B0%BD%E5%A4%B4%E6%98%AF%E4%BB%80%E4%B9%88/</link>
      <pubDate>Wed, 23 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%AE%87%E5%AE%99%E7%9A%84%E5%B0%BD%E5%A4%B4%E6%98%AF%E4%BB%80%E4%B9%88/</guid>
      <description> 应该是中学开始 有个问题就一直困扰着我 抛开鬼怪神说 现实世界的东西都是真实存在的物质 都是真实存在的 在空间里真实存在的东西 不管你感觉到有多大 它最终是可以被计量的 海洋一望无际 无边无界 但是只要视角足够大 最终海洋是有边界的 地球也是有边界的 后来人们也发现天上的星星月亮 也是真实存在的星球 人们可以接触到它 它是真实存在物质 记录片里会告诉我们在宇宙中 人类有多尘埃 地球有多小 这些都没问题 现实存在的这些星体，星系 未来科技进步了终究是可以达到的 哪怕是几亿光年的星球 这个星球也是真实存在的“物质” 但宇宙呢 这么多星球漂浮其中 它是个什么东西 它有边界吗 真实的世界里不会有一个“无限大“的物质 一只蚂蚁在森林里 它会认为森林是无限大的 但是只要把蚂蚁放大到一点 视线拉远一点 森林的尽头有河有山有人家 蚂蚁的森林不是无限大的 人类的地球也不是无限大的 星球的宇宙为什么是无限大的 而无限大就有个奇怪的地方 假设有种生物一直超宇宙的一个方向前进 那么它最终会一直看到宇宙中的各种种样的物质 这些物质都是什么 而假如这个生物最终会卡在一个边界上 那宇宙的尽头是什么？ 宇宙这个东西的外面又是什么东西？  </description>
    </item>
    
    <item>
      <title>DTCC参会者视角：2024年数据库大会和DBA之夜</title>
      <link>/dba/2023dtcc%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%A7%E4%BC%9A%E5%92%8Cdba%E4%B9%8B%E5%A4%9C/</link>
      <pubDate>Wed, 16 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/2023dtcc%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%A7%E4%BC%9A%E5%92%8Cdba%E4%B9%8B%E5%A4%9C/</guid>
      <description> 刚参加完DTCC2023的DBA之夜回来 原计划21:30结束的活动 快22点才收尾 还赶上北四环的下大雨 慢悠悠的开回来了 趁热呼劲写点内容  旧识  今天遇到了好多旧时的同事 艺龙的，去哪儿的 甚至还遇上了搜房的同事 不期而遇的快乐 感觉十分亲切 可以凑一起吧啦吧啦聊得起劲 疫情原因DTCC已经好几年没正经举办了 虽然人数不如2019年前 人气也不如以往 对我来说 在满地的年轻面孔里 看到几位旧相识 是件很开心的事了  国产数据库  依旧最火的话题 得益于国家的信创支持 把饭已经喂到我们这一行的嘴边了 聪明机灵的已经吃得撑起来了 先是政府事业单位 有钱的金融单位 国有企业 正在快速把数据库国产化 这一波的行情 预计还会持续3-5年 主要是去O，去DB2 然后用商业数据库的版权费 来支持国产DB的发展 这个不光是可以把核心数据系统 掌握在自己手里 也确实可以省掉很多的费用 就算是国产改造阶段产生了更多的费用 也是落在自己人手里不是吗  开源协议  MySQL和PG 国产数据库99%的源头 还有Mongo和Redis的协议约束 有个观点非常好 在国内的知识产权现状下 只要你掌握了开源数据库的代码 协议是约束不了你的 这个是现实情况 而且会持续很多年不会变 国外的开源协议 在国内当前的环境下 确实比较吃亏 我们也确实在占国外的偏宜 一方面用着国外的开源代码 一方面用这些代码改造了国产数据库 再代替国外的商业数据库软件 讲道理 这个我们在占便宜 但是现实是 一大批国内的数据库内核开发者已经养成了 现在拦也拦不住了 OB,TIDB这些年的代码自主量已经越来越少的开源的影子了  PG和MySQL  两个门派之争 我肯定是支持MySQL啊 主观客观上都得是MySQL 有时候简单也是优势 还有现在这覆盖规模和先发优势 没有大的变化 MySQL在国内的发展前景远胜于PG 但华为高斯类的PG改版活跃 还有平安之类的大企业在用 PG派也是有可能壮大的 然后天天骂PG的姜老师 人气还是旺 不少人就是过来看热闹的 只是今年的火药味不足 只有姜老师一个人在那冷嘲热讽 少了上次PG同学们的针锋相对 场面还是平淡了些 两派相争 目前还是周搞的世纪之争 战况激烈深入人心  然后  今天竟然有人说我新换的表带：有点娘炮 这个词让我耿耿于怀 得再换一个表带了  </description>
    </item>
    
    <item>
      <title>训练一个自己的AI机器人</title>
      <link>/book/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E7%9A%84ai%E6%9C%BA%E5%99%A8%E4%BA%BA/</link>
      <pubDate>Tue, 08 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E7%9A%84ai%E6%9C%BA%E5%99%A8%E4%BA%BA/</guid>
      <description> 试用入口
 AI私有化训练的好处  已知ChatGPT之类的大语言模型的数据是在训练时的数据集基础上产生的人工智能 这些AI训练完成后，就不再更新，所以ChatGPT的数据和认知停留在2021年9月份 私有化训练的好处是 可以让AI学习你想要的知识，更新AI的能力 可以训练公司或个人的数据，满足特定的业务需求   训练公司的产品操作手册，用于客服AI    训练一些行业信息，提供信息咨询    用最新的数据，更新AI的知识库    用个人的聊天记录，训练出AI模仿这个人的对话语气    &amp;hellip;    训练需要哪些条件  训练AI一方面需要提供有价值的数据（这个就看积累了） 这些数据最好是文字格式的电子文件:word,excel,pdf,txt&amp;hellip;其中txt格式最佳 以及训练用到的工具  一台或几台高性能带显卡的电脑或服务器（硬件越好，训练得越快） 一个现成的AI(可以是国外的ChatGPT，也可以是自己部署的私有化AI) 如果训练的数据很重要，为了数据隐私可以选用私有化部署AI   如果你没有以上的工具，或不知道如何做私有化训练 请参考以下内容，我们做了一个免费的工具帮你训练一个属于你的私人定制AI  如何训练一个自己的AI机器人 第1步：填写AI名称 填写AI名称 第2步：上传数据  支持pdf,txt等格式 上传的数据越多，训练后的AI对相关的回答越智能 默认上传的数据是真实可信的，ai训练时不会去判断数据是否正确  上传数据 第3步：等待AI训练完成  到这一步后就只需要等着就行 而且需要等很久，建议隔一天再过来看进度。 这里为了演示，我上传的文件很少，也用了 AI训练非常费资源,文件件越多越大，训练用时越长。  等待AI训练完成 训练完成  这里上传了107MB文件，训练用了1个小时。  第4步：等待管理员审批  训练完的AI并不是立即可以用的 管理员（也就是我）会检查一下是否有不合适的内容（只要不出现敏感内容，都会通过的） 审批通过后，会收到一条短信提醒 这个训练好的AI就可以使用了  第5步：开始与新机器人对话  这里我们训练的文件里有大量的2022年国内家电行业的pdf文件 所以AI可以很准确的回答出相关的问题   总结  我们做了一个AI私有化训练平台 平台上的每个用户都可以训练自己的数据，形成属于自己的AI机器人 受制与网络带宽，如果你要训练的数据集很大，请用移动硬盘寄给我 请不要上传训练敏感内容（政治，色情，宗教&amp;hellip;） AI训练用时较长，上传完数据后要耐心等待 训练完成的AI机器人，可以在平台上自由使用  </description>
    </item>
    
    <item>
      <title>怎么通俗的理解向量和数字化</title>
      <link>/book/%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E5%90%91%E9%87%8F/</link>
      <pubDate>Thu, 03 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E5%90%91%E9%87%8F/</guid>
      <description>从《向量数据库》说起  上周同学来望京聊天时 我说最近在做：向量数据库 然后就向他解释 什么是向量 当时我们在楼下公园里遛弯 路边停了一排摩托车 我指着一辆比较酷的摩托给他举例子 意外的很容易他就听懂了 向量的核心是：把世间万物都数字化 一个人，一个美女，一辆车，一条狗 一群僵尸，一段记忆，一句话&amp;hellip; 不管是什么 都用数字来表达  怎么做到向量的数字化呢  比如说：我 怎么用数字来描述我这个人呢 第1个维度： 在这个维度的设定是（物种:人：5001,狗:1001，猪:1002,花:2001,草:2002&amp;hellip;） 我是个人：不是狗，不是猪，不是花，不是草 所以得到一个数字:5001 第2个维度: 在这个维度的设定是（性别，男：1，女:0 ,不男不女:0.5,又男又妇:2,其他:3） 我是纯爷们,得到数字1,现在我的向量是[5001,1] 第3个维度是身高 于是我的向量:[5001,1,174] 第4个维度是籍贯 于是我的向量:[5001,1,174,551] 第5个维度是体重 于是我的向量:[5001,1,174,551,70] 第6个维度是小学学校名对应的数字 于是我的向量:[5001,1,174,551,70,22] 第7个维度是喜欢的运动 第8个维度是喜欢的食物 第9个维度是喜欢的书 第10个维度是脸长 第11个维度是脸宽 第12个维度是眼睛大小 第13个维度是鼻孔大小 第14个维度是腿上有几根毛 第15个维度是肌肉密度 第16个维度是帅气程度 第17个维度是脾气 &amp;hellip;. 于是我的向量:[5001,1,174,551,70,22,....] 维度越多 对我的描述越具体 最终我们定义一下，比如700个维度 那么最终我们用[5001,1,174,551,70,22,....]类似的700个数字定义了我 这一组数字就是我的向量表达  向量 有什么用？  刚才我们用向量表达了我这个人 这样会有什么好处呢 假设： 因为我的帅气英俊加上气宇不凡 有众多的追求者 但是我已经名花有主了 这时候追求者们 对我这个优秀的稀缺资源已经不能再被占有了 于是她们想找个：最接近我的人 这时候怎么办 去向量数据库里匹配向量和我重合度最高的人 这时候就会得到一批非常接近我的人 快去抢吧&amp;hellip; 机会不等人  向量 崩塌  这时候就会有人质疑了 会不会出现一个人的向量和我无限接近 只有某一项或少数几项和我的不一样的人 比如一个各个向量都接近的女版的我？ 比如会不会有条狗跟我的向量表达很近？ 如果向量的维度不够 是会出现的 但是维度足够多 就会避免出现此类向量崩塌的场景 一个女版的我 一定会在其他的很多维度上和我产生明显差异 比如头发，运动能力，比如胸围，比如&amp;hellip; 这就是向量的维度多的好处 大力出奇迹 维度多，细节到位 向量的匹配越真实  向量与AI  到这里是不是已经理解了向量是什么了 再从一句话来说 “帮我写一篇今天晚饭的健康食谱” 这一句话 在向量维度也是用一组数字来表达 首先做分词 再对每个词的词性做数字化 词之间的关联数字化 最终这句话会对应成一个向量表达 [2,82,234,5,22,2197,74.</description>
    </item>
    
    <item>
      <title>一个很会骂人的AI</title>
      <link>/book/%E4%B8%80%E4%B8%AA%E5%BE%88%E4%BC%9A%E9%AA%82%E4%BA%BA%E7%9A%84ai/</link>
      <pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E4%B8%80%E4%B8%AA%E5%BE%88%E4%BC%9A%E9%AA%82%E4%BA%BA%E7%9A%84ai/</guid>
      <description>骂人的AI  中午吃饭时看到一个项目 说有人做了一个很会骂人的AI 好奇心有点重 能有多会骂人？ 就去试了几句 真的是张口就骂 给我整得一愣一愣的 当AI批上市巾的外衣 骂起人来也是顺风顺水。 原项目地址是：https://github.com/vastxie/Happy-ChatGPT 我看了这个项目的源码 找到它让AI变坏的关键点 复制出来一个粗欲的AI 放在我的AI平台上了 给没试过被AI骂过的朋友体验下 小心上头哈。  AI平台的进度  目前在做定制机器人的开发。 目标是给平台上的每个用户都可以训练自己的AI 用户提供文件 我们提供免费的训练平台 目前核心功能已完成 预计8月份上线试用  AI生成视频的解答  AI生成视频功能上线后 有很多的用户在平台上制作短视频 甚至7月26日有人加我微信说 生成的视频播放不了 检查发现 生成的视频把服务器磁盘空间占满了 于是做出以下策略 AI生成的视频保留7天 AI生成的图片保留30分钟 过期会自动删除 如果觉得有用，请自行在有效期内下载保留 另外平台上的生成图片，视频功能是免费的 有些用户会反复生成同一个主题的视频和图片 请节约有限的资源 不要恶意浪费  </description>
    </item>
    
    <item>
      <title>Google医疗大模型Med-PaLM突然进化到接近临床医生水平</title>
      <link>/book/google%E7%9A%84%E5%8C%BB%E7%96%97%E5%A4%A7%E6%A8%A1%E5%9E%8Bmed-palm%E5%B7%B2%E6%8E%A5%E8%BF%91%E4%B8%B4%E5%BA%8A%E5%8C%BB%E7%94%9F%E6%B0%B4%E5%B9%B3/</link>
      <pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/google%E7%9A%84%E5%8C%BB%E7%96%97%E5%A4%A7%E6%A8%A1%E5%9E%8Bmed-palm%E5%B7%B2%E6%8E%A5%E8%BF%91%E4%B8%B4%E5%BA%8A%E5%8C%BB%E7%94%9F%E6%B0%B4%E5%B9%B3/</guid>
      <description>Med-PaLM的进化  7月12日《Nature》发表文章，详解了医疗大模型Med-PaLM的进化过程 研究人员表示，当回答医学问题时，微调后的医疗大模型Med-PaLM表现良好，一组临床医生对其回答的评分为92.6%，与现实中临床医生的水平（92.9%）
  论文原地址：https://www.nature.com/articles/s41586-023-06291-2 有兴趣的同学可以看一下原文   记住这组数字：  在MultiMedQA评估基准下 临床医生对ai的正确率评估是：92.6% 而现实中临床医生正确率水平是：92.9% Google的Med-PaLM大模型还没有正式开放 从他们发表的论文上看 医疗领域的正确率这次提升非常明显 在AI+医疗领域目前Med-PaLM是最先进的AI 比通用大模型:GPT4的正确率要高很多  92.6%是什么水平  临床医生正确率：92.9% Med-PaLM进化版正确率：92.6% GPT4正确率：85.1% 初代版的Med-PaLM正确率：67.2% ChatGPT(GPT3.5)正确率:60.2% 这是个了不起的数据   AI的正确率有哪些影响  ai正确率的影响参考我3个月前的一篇文章: 《职业选择的新思考：ChatGPT是否会取代你的职位？》  AI+医疗领域  在AI+医疗领域，谷歌走在前列。 其最新的医疗大模型Med-PaLM 2为首个在美国医疗执照考试中达到专家水平的大模型。 据华尔街日报报道，自4月份以来，该模型一直在美国梅奥诊所等顶尖私立医院进行测试。 我和几个同学先前讨论过几次 一致认为：教育，律师，会计，医疗 这几个领域是AI最好落地的场景 而这4个领域一定会有专门训练且优化过的专业AI出现 专业的大模型在特定场景下比现在的chatgpt等通用大模型要好用 而伴随着谷歌这次抽风式的进化 AI医疗方面的应用落地 AI检测心电图、X光片会更快的一些医院中投入应用 而AI问诊,AI用药咨询也将快速落地  怎么使用Med-PaLM进化版  目前只有Google的测试结果 没有开放试用 而且这些专业领域的大模型一般会和指定的公司和商家合作 不会对大众开放 我在个人网站上增加了两个医疗ai机器人 基于ChatGPT 正确率一般，跟google此次的进化版有点弱 有想体验的同学也可以试用一下   </description>
    </item>
    
    <item>
      <title>北大法律大模型ChatLaw发布:中国法律界的智能助手</title>
      <link>/book/%E5%8C%97%E5%A4%A7%E6%B3%95%E5%BE%8B%E5%A4%A7%E6%A8%A1%E5%9E%8Bchatlaw/</link>
      <pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%8C%97%E5%A4%A7%E6%B3%95%E5%BE%8B%E5%A4%A7%E6%A8%A1%E5%9E%8Bchatlaw/</guid>
      <description>7月3日 北京大学信息工程学院袁粒课题组+北大兔展AIGC联合实验室两个团队联合发布了ChatLaw 可能是目前国内比较先进的法律大模型 从实际体验来看 1.在国内法律领域比ChatGPT回答得更专业 2.在其他回答上很勉强，比ChatGPT弱很多,有点呆 3.本地部署非常费资源,对机器要求较高 4.性能不佳，每次回答容易卡住且不流畅 考虑到6月27日幂律智能联合智谱AI法律垂直大模型 PowerLawGLM。 还有再早一点开源社区也发布了一些AI法律微调模型 由于ChatGPT的普及，LLMA的开源 而法律行业的规律性，可文本化，可预测性比较强 大语言模型的崛起为普通人的咨询法律相关问题提供了很多便利  关于ChatLaw 信息汇总  官网地址：https://www.chatlaw.cloud/ 论文地址：https://arxiv.org/pdf/2306.16092.pdf GitHub 地址：https://github.com/PKU-YuanGroup/ChatLaw 根源上这个模也还是基于meta公司的LLaMA模型(小扎的这次开源给国内的大语言模型贡献巨大，现在市面上大多数开源模型都是基于Meta开源的LLAMA） 北京大学信息工程学院袁粒课题组 北大-兔展AIGC联合实验室联合发布 主要团队成员： Jiaxi Cui Zongjian Li Yang Yan Bohua Chen Li Yuan  这是官方宣传视频   这是实际体验&amp;hellip;  问了一个踩到狗屎的问题，等了5分钟，还卡在这   ChatLaw的版本  共有三个版本，分别如下： ChatLaw-13B，为学术 demo 版，基于姜子牙 Ziya-LLaMA-13B-v1 训练而来，中文各项表现很好。但是，逻辑复杂的法律问答效果不佳，需要用更大参数的模型来解决； ChatLaw-33B，也为学术 demo 版，基于 Anima-33B 训练而来，逻辑推理能力大幅提升。但是，由于 Anima 的中文语料过少，问答时常会出现英文数据； ChatLaw-Text2Vec，使用 93w 条判决案例做成的数据集，基于 BERT 训练了一个相似度匹配模型，可以将用户提问信息和对应的法条相匹配。  普通人怎么用上ChatLaw  方法1.官网上申请内测 https://www.</description>
    </item>
    
    <item>
      <title>Langchain的生态链开源产品</title>
      <link>/book/langchain%E7%9A%84%E7%94%9F%E6%80%81%E9%93%BE%E5%BC%80%E6%BA%90%E4%BA%A7%E5%93%81/</link>
      <pubDate>Wed, 05 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/langchain%E7%9A%84%E7%94%9F%E6%80%81%E9%93%BE%E5%BC%80%E6%BA%90%E4%BA%A7%E5%93%81/</guid>
      <description>Langchain生态 低代码  Langflow: LangFlow是LangChain的UI Flowise - LangchainJS UI：拖放 UI 以使用 LangchainJS 构建自定义的 LLM 流程 Databerry：用于语义搜索和文档检索的无代码平台 LangchainUI: 开源聊天人工智能工具包 Yeager.ai：Yeager.ai Agent 是第一个 Langchain Agent 创建者，旨在帮助您轻松构建、原型设计和部署 AI 支持的代理  服务  GPTCache：用于为 LLM 查询创建语义缓存的库 Gorilla：LLM 的 API 商店 LlamaHub：社区制作的 LLM 数据加载器库 EVAL：带有 Langchain 的弹性多功能代理。 将执行您的所有请求。 Auto-evaluator：使用Langchain进行问答的轻量级评估工具 Langchain Visualizer：LangChain工作流程的可视化和调试工具 LLM策略：使用LLM实现策略模式 datasetGPT：使用 LLM 生成文本和会话数据集的命令行界面。 spellbook-forge：使您的 LLM 提示可执行并受版本控制。 自动评估器: Langchain 自动评估器 Jina：使用 Jina 进行生产的 Langchain 应用程序 Gradio 工具: Gradio LLM 代理 steamship-langchain：Steamship 适配器，使 LangChain 开发者能够在 Steamship 上快速部署应用程序🐍 LangForge：用于创建和部署 LangChain 应用程序的工具包 BentoChain: LangChain 在 BentoML 上的部署 LangCorn: 使用 FastApi 自动为 LangChain 应用程序提供服务 Langchain 服务：带有 Qdrant 矢量存储和 Kong 网关的自定 Langchain 设置 Lanarky: 使用 FastAPI 交付可用于生产的 LLM 项目 Dify：一个用于插件和数据集的 API，一个用于快速工程和可视化操作的界面，所有这些都用于创建强大的 AI 应用程序。 LangchainJS Worker：cloudflare 上的 LangchainJS Worker Chainlit：在几分钟内构建 Python LLM 应用程序 Zep：Zep：LLM/Chatbot 应用程序的长期内存存储 Langchain Decorators：LangChain 顶部的一层，为编写自定义 langchain 提示和链提供语法糖 FastAPI + Chroma：ChatGPT 的示例插件，利用 FastAPI、LangChain 和 Chroma AilingBot：快速将Langchain上构建的应用集成到Slack、企业微信、飞书、钉钉等IM中。  代理  CollosalAI Chat：使用 RLHF 实施 LLM，由 Colossal-AI 项目提供支持 AgentGPT：使用 Langchain 和 OpenAI (Vercel / Nextjs) 的 AI 代理 本地 GPT：受到私有 GPT 的启发，将 GPT4ALL 模型替换为 Vicuna-7B 模型，并使用 InstructorEmbeddings 代替 LlamaEmbeddings ThinkGPT：增强 LLM 并超越其极限的代理技术 Camel-AutoGPT：法学硕士和自动代理（如 BabyAGI 和 AutoGPT）的角色扮演方法 私有GPT：利用GPT的力量与您的文档进行私密交互，100%私密，无数据泄露 RasaGPT：RasaGPT 是第一个构建在 Rasa 和 Langchain 之上的无头 LLM 聊天机器人平台。 SkyAGI：LLM 代理中新兴的人类行为模拟能力 PyCodeAGI：一个小型 AGI 实验，根据用户想要构建的应用程序生成 Python 应用程序 BabyAGI UI：让在网络应用程序中使用babyagi更容易运行和开发，比如ChatGPT SuperAgent：将 LLM Agent 部署到生产环境 Voyager：具有大型语言模型的开放式实体代理 ix：自治 GPT-4 代理平台 DuetGPT：对话式半自主开发助手，无需复制粘贴即可进行 AI 结对编程。 生产环境中的多模式 LangChain 代理: 部署 LangChain Agent 并将其连接到 Telegram DemoGPT：DemoGPT 使您只需使用提示即可创建快速演示。 它在 Langchain 文档树上应用了 ToT 方法。 SuperAGI：SuperAGI - 开发优先的开源自主人工智能代理框架 自主 HR 聊天机器人：一个自主代理，可以使用现有工具自主回答 HR 相关查询 BlockAGI：BlockAGI 进行迭代、特定领域的研究，并输出详细的叙述性报告来展示其发现  模板  AI：Vercel 模板，用于使用 React、Svelte 和 Vue 构建人工智能驱动的应用程序，对 LangChain 提供一流的支持 create-t3-turbo-ai：基于 t3、Langchain 友好的样板，用于构建类型安全、全栈、LLM 驱动的样板 使用 Nextjs 和 Prisma 的 Web 应用程序 LangChain.</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_生成视频</title>
      <link>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_8/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_8/</guid>
      <description>准备工作  1.打开www.top580.com 2.登入系统 这一步参见文档:注册和登录的详细说明  2.登入系统  这一步参见文档:注册和登录的详细说明  教学视频   </description>
    </item>
    
    <item>
      <title>定时收集Oracle索引信息</title>
      <link>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86oracle%E7%B4%A2%E5%BC%95%E4%BF%A1%E6%81%AF/</link>
      <pubDate>Mon, 26 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86oracle%E7%B4%A2%E5%BC%95%E4%BF%A1%E6%81%AF/</guid>
      <description>需求  把Oracle各实例的索引信息，统一收集存储到一张表里  建表 在dboop库中建表
 CREATE TABLE `info_indexs` ( `dbid` int NOT NULL DEFAULT &#39;0&#39;, `table_name` varchar(255) NOT NULL DEFAULT &#39;&#39;, `index_name` varchar(255) NOT NULL DEFAULT &#39;&#39;, `index_type` varchar(64) NOT NULL DEFAULT &#39;&#39;, `uniq_type` varchar(64) NOT NULL DEFAULT &#39;&#39;, `num_rows` int NOT NULL DEFAULT 0, `sample_size` int NOT NULL DEFAULT 0, `last_analyzed` datetime null, `column_name` varchar(2000) NOT NULL DEFAULT &#39;&#39;, `cstatus` smallint NOT NULL DEFAULT &#39;1&#39;, `dba_freshtime` datetime NOT NULL DEFAULT &#39;1990-01-01 00:00:00&#39;, PRIMARY KEY (`dbid`,`table_name`,`index_name`), KEY `idx_info_indexs_time` (`dba_freshtime`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ; 实现数据字典收集入库 建Oracle采集任务  &amp;lt;!</description>
    </item>
    
    <item>
      <title>关于数据安全_DBA篇</title>
      <link>/dba/%E5%85%B3%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8_dba%E7%AF%87/</link>
      <pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%85%B3%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8_dba%E7%AF%87/</guid>
      <description>前言 明天有个会，大家一起商量一下怎么做好数据安全。提前整理一下思路：
什么是数据安全  数据安全是指保护数据不被非法获取、篡改、破坏或泄露的一种技术和管理措施。 在我这里，数据安全要更具体一点 1.存储安全：数据只要写到数据库里了，就不会丢  1.1 存储上：多节点存储，防止物理损坏 MySQL高可用组件之ProxySQL 1.2 数据按固定的周期进行全备和日志备份 数据库备份管理制度 1.3 自动化脚本检查备份成功和验证 1.4 保证数据被意外删除后，还能找回来 自动化流程:数据找回(一:MySQL数据闪回) 自动化流程:数据找回(二:Oracle部分) 1.5 保证数据和数据库备份是双机房异地存储 数据库备份管理制度 1.6 额外做孤岛备份，以防止内网机房的病毒大面积感染 孤岛备份机和勒索病毒   2.账号策略：只有指定权限的用户可以访问可控范围内的数据（到库级别）  要求研发分业务存储库，不要混用数据库 账号自动化管理，权限限制在可控范围内 账号密码不分发给研发，由运维人员统一配置（这点很重要，为第三步的访问控制提供前题）   3.访问控制：将业务人员和运维人员隔离  业务人员指研发，产品，测试，大数据，风控&amp;hellip; 运维人员：DBA 运维 只有运维人员可以接触到线上数据库，研发和其他人员均不可连接到数据库机器和实例 将研发等业务人员和数据库的接触限定在两个方式内：1.通过程序代码操作数据库 2.通过DBA的Web平台操作数据库 线上查询和线上变更。走DBA提供的平台执行 限制DBA等运维人员，了解业务逻辑，杜绝DBA直接查询和修改线上业务数据 我为什么要反对DBA参与业务(出报表/改数据)   4.安全审计：线上的数据异常，要有日志可查  数据变更日志（binlog，归档日志等） MySQL的7种日志(四):BinLog SQL上线日志 （记录变更新镜像和更新后结果，方便快速回滚） 数据库多环境SQL上线 异常日志和慢日志收集到es 服务器操作日志，数据库账号变更日志 个人查询日志，部分线上查询审计日志   5.数据加密：数据库里的敏感信息应该加密存储  哪些属于敏感信息：手机号.卡号.身份证号.住址&amp;hellip; 首先需要把敏感信息标识出来。  为此我们开发了一个工具，在用户建表或者修改表结构时，会识别出来 外加一个兜底脚本，定时扫描SQL查询结果，如果发现有敏感信息未标识的就会提示出来   敏感信息标识后，不管底层是否做了加密存储，DBA和大数据平台都可以对这些字段做针对性的掩码，防止信息泄露 数据的加密存储，这个单做一节，详细说说    数据加密  如上一节最后说的，我们已经将敏感信息识别出来了，现在怎么做数据的加密存储。根据实际情况展开来说  新项目的数据加密  如果有开发资源：架构组开发一套通用的加密服务，新项目调用 如果没有开发资源： 研发用通用的加密算法对敏感信息进行可逆的加密（例AES)后入库  老项目的数据加密改造 方法一：数据库里存的是加密数据，研发存放和读取都是明文数据  应付审计之法。 优点是：库里的数据确实是加密的 缺点是：研发和业务人员查询时是明文的 这个需要借助第三方中间件来实现：（例如SphereEx） 我头一次听SphereEx讲他们的中件层加密时，觉得这个思路非常棒 这可能是比较节约开发资源的，又可以应付审计的一种加密方式。 这是它的优点也导致了一个缺点：研发查出来数据库里的信息还是明文，数据防泄露效果差 只防住了DBA和运维人员的泄密，而更关键的业务泄露并没防住 加了中间层，稳定性待考证 加密收益： 2颗星 ，加密工作量：1颗星  方法二：数据库里存的是明文数据，研发读取到前台展示的时候是密文的  防前台泄密之法 在SQL层将所有的查询接口都改造一下，需要花费不少的时间（2-3周） 优点是，前台用户看到的数据是加密或掩码的。解密记录是可审计的，防止信息泄漏 缺点是，数据库明文存储了，治标了但没治本 加密收益： 4颗星 ，加密工作量：3颗星  方法三：数据库里存的是密文数据，研发读取到前台展示的时候是密文的  这个就是把旧项目彻底改造了，存数据和读数据的地方都要改一下 这个改造的工作量非常大，但是效果是最彻底的 最完整的方案是分成三个角色 DBA提供存储，架构组提供加解密服务，研发存储和读取的都是密文 其中架构组是核心，提供整套加解密服务 研发参与成本最大，需要在写数据和读数据时修改代码 加密收益： 5颗星 ，加密工作量：4颗星  </description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_生成流程图</title>
      <link>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_7/</link>
      <pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_7/</guid>
      <description>准备工作  1.打开www.top580.com 2.登入系统 这一步参见文档:注册和登录的详细说明  2.登入系统  这一步参见文档:注册和登录的详细说明  生成流程图 功能介绍  提供一些关键词，让AI依据这些关键词生成指定的流程图 支持流程图,状态图,时序图,甘特图,类图,饼图,关系图  使用说明  平台是业务时间个人写的，刚刚试用，可能会有很多bug和使用不便的地方，欢迎和我反馈
 &amp;hellip;
enjoy it .</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_生成图片</title>
      <link>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_6/</link>
      <pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_6/</guid>
      <description>准备工作  1.打开www.top580.com 2.登入系统 这一步参见文档:注册和登录的详细说明  2.登入系统  这一步参见文档:注册和登录的详细说明  生成图片 功能介绍  提供一些关键词，让AI依据这些关键词生成指定的图片 支持卡通画,简笔画,素描画,中国画,水彩画&amp;hellip;  使用说明  平台是业务时间个人写的，刚刚试用，可能会有很多bug和使用不便的地方，欢迎和我反馈
 &amp;hellip;
enjoy it .</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_AI对话</title>
      <link>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_5/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_5/</guid>
      <description>准备工作  1.打开www.top580.com 2.登入系统 这一步参见文档:注册和登录的详细说明  AI对话 功能介绍  和ChatGPT进行对话，在对话中解决问题或生成各式各样的文章 支持：闲聊，散文，诗歌，小说，故事，笑话&amp;hellip;.  使用说明  平台是业务时间个人写的，刚刚试用，可能会有很多bug和使用不便的地方，欢迎和我反馈
 &amp;hellip;
enjoy it .</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_用户注册和登录</title>
      <link>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_4/</link>
      <pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_4/</guid>
      <description>第一步，打开www.top580.com  界面如下图所示   第二步：点击按钮进入 用户登录页  首次加载时，需要大约10秒钟的准备时间 加载成功后，会进入登录页  如果已经注册过的用户，直接用手机号和密码登录 新用户点下方的点击进入试用申请  第三步：新用户注册（可选）  老用户无需此步 认真填写资料后点提交 密码会用【短信的方式】发送到你的手机上 这个密码很重要，记住它 暂时还没做忘记密码功能，如果密码忘了，就没了。切记 -  第四步：登入系统  已经注册过的用户，直接用手机号和密码登录  当你看到这个页面的时候，就表示你登入成功了   第五步：开始使用  进入系统后，就可以正常使用AI功能了 每个AI的使用方式和技巧，会单独出文章 -   平台是业务时间个人写的，刚刚试用，可能会有很多bug和使用不便的地方，欢迎和我反馈
 &amp;hellip;
enjoy it .</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com_使用说明(汇总)</title>
      <link>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_3/</link>
      <pubDate>Fri, 19 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_3/</guid>
      <description> 我做了一个智能AI平台 免费使用.让更多的人接触ChatGPT类似的AI智能产品 为防资源滥用,每个用户每天可以和AI对话/生成/次数有限制 未认证用户:【20次/天】 实名验证用户:【50次/天】 用完次数后，需要等第二天再继续。 功能包括： ChatGPT的AI对话 ChatGPT生成图片 ChatGPT生成各种文档，创意文案，诗词&amp;hellip; ChatGPT生成流程图,时序图,类图,甘特图&amp;hellip; ChatGPT生成视频（调试中,暂时未开放） 网址是：www.top580.com 文档1:注册和登录的详细说明 文档2:ChatGPT对话的详细说明 文档3:AI生成图片的使用说明 文档4:AI生成流程图的示例 文档5:AI生成视频的视频讲解 更多功能和使用方式，正在准备中&amp;hellip;  如果你想了解目前该项目的进度:进度更新    </description>
    </item>
    
    <item>
      <title>人工智能平台top580.com_进度更新</title>
      <link>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_1/</link>
      <pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_1/</guid>
      <description>试用入口
 当前进度 系统功能  域名转到cloudflare 已完成 阿里云主机注册 已完成 openai的api申请 已完成 azure的api申请 未开始 域名备案 已完成 用户注册 已完成 手机验证码平台 已完成 密码生成下发 已完成   用户登录 已完成 用户权限 已完成  通用AI  文本问答 进行中  结果记录代码 已完成 结果展示优化 已完成 翻译助手 未开始 超长文本 已完成   流程图 已完成 生成图片 已完成 生成音频 40% 生成视频 已完成 图片识别 已完成，代价太大，放弃  定制AI  私有数据上传 已完成 私有数据训练 已完成 私有ai问答 已完成 专用ai 进行中  医疗助手 已完成 症状助手 未开始 药品辅助 未开始 心理辅导 未开始   法律助手 进行中  文律文书生成 已完成 国内法规向量训练 已完成 案件线索 未开始 法规图谱 未开始 模拟法庭 进行中 30%   代码助手 未开始  代码生成 代码解释      其他  多平台ai接入 未开始 隐私离线ai 已完成   按日期更新 20230920  增加图生图功能（用一张图+文字生成一张新的图）  20230918  生成照片列表页改成首页即可见 增加生成3d图标和超现实主义照片的模型 禁用了平台上提擦边球图片的用户，并增加负面提示词防止生成色情图片  20230916  生成照片，脸容易变形的问题修复 这个bug是在去天津的路上修复的，这个bug太恶心了，出来的美女脸一变形跟鬼一样，实在是忍不了  20230915  妹子功能上线，发现脸部总是有点扭曲，待修复  20230914  在二手显卡的帮助下，生成妹子功能取得质的突破，不管是二次元，还是超写实，效果都比以前的好太多。 放弃了以前的生成图片功能和接口，改用全新的训练模型。预计3-5天可以完工。  20230911  生成声音功能规划，最终还是要文字部分区分开来。共用一个LOG表，和视频共用一套流程  20230910  生成视频页面改造，页面动态调整，为生成其他媒体内容改造。  20230908  用户反馈需要忘记密码功能，完成页面增加 70% ,没做完。  20230907  录制《AI私有化训练》教学视频，视频太长，效果不好。  20230906  调整AI生成各种功能时放在不同库表中存储的问题，共用一套库表，共用一个前/后端页面 又买了块二手显卡，用于生成细粒度的人物图（漂亮妹子）  20230905  忙了一段时间没有更新这个AI 继续更新：AI私有化增加：国产,本地模型选项  20230815  正式上线：AI私有化训练功能，离线AI问答功能  20230814  显卡周末到货了，今天花了大半天的时间在安装这个显卡 现有的服务器，台式机的电源都供不上（需要2个8针的750w供电） 叠了台式机和服务器的电源，也没带动 考虑退货了，就先将就着云上资源先用着  20230810  AI训练：完成AI训练完的机器人的问答测试 下单了一块显卡。打算提升一下训练速度  20230806  AI训练：远程调用的问题修复完成  20230802  发布AI训练到线上，发现远程调用有问题  20230730  AI训练：去掉训练完由用户确认的步骤  20230728  增加粗俗的AI,出口成脏，很会骂人 调整现有的ai对话prom类型。将类型人格化  20230725  AI训练：完成流程的初步调试 发现打包上传后出错（原因未定位）  20230724  AI训练：完成用户上传后，后台训练的部分 完成celery的任务互动，完成异步任务 完成内网传透，已节约阿里云费用  20230723  AI训练：AI训练流程增加审核节点  20230720  AI训练：AI训练流程（新建,训练，确认,完成） 制作流程页，主要是用户上传部分  20230718  增加了实时联网的ai：用搜索引擎查询用户输入，返回相应结果（未上线，待私有ai一起打包上线）  20230717  增加医疗ai。去掉文本对话中不常用到的4个ai  20230715  调试完成langchain和向量支持 完成私有ai训练的核心模块（重要）  20230711  生成视频功能出错,bug修复 07-09到07-11 共39个用户生成视频的请求失败。  20230708  私有数据上传页面（前端） 80% 模拟法庭初版页面 30%  20230628  生成视频功能正式开放使用  20230625  生成视频功能补全（上传到阿里云，视频在线播放，视频生成日志）  20230624  生成视频前端页面完成  20230622  生成视频功能完成核心代码（基本能用）  20230617  尝试网站微信扫码登录（用于代替手机短信验证） 失败：我的微信公众号不是用企业名义注册的，无法开通此功能 扫码登录的代码已完成，功能暂不可用  20230614  平台来了很多新用户，系统资源有点紧张 开了阿里云PolarDB，后台数据存储转向RDS 修改了图片生成功能，用户的输入将会被翻译成英语以后，再生成图片。（提高图片的准确性）   20230611  生成视频，去掉审核环节 修复注册页bug ：appuseid传输列表时报错。  20230607  修复ai不能连续对话的bug。ai可以记住历史聊天内容了 修复加载卡住的问题（首页加载时间依旧会过长，还需要做个加载动画） 修复注册页面直接跳到首页的bug ai生成视频功能开始做了  20230530  1.</description>
    </item>
    
    <item>
      <title>要做一个人工智能平台</title>
      <link>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0/</link>
      <pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0/</guid>
      <description>功能  在做一个AI智能平台 可以对接多个ai平台 不仅仅是ChatGPT 还支持私有化数据训练 用户可以流畅的在国内访问 用户可以提交自己的私用数据做训练 用户训练的私有数据可以选择是否公开给其他人用 会有模块化的已定义好的ai可以直接用 支持图片,音频,视频,图表的生成 支持针对行业的定制化ai功能  成本  前期完全免费 接受用户反馈 不断迭代改进 等产品成熟了以后 估计在6月份以后 会收取一定的费用 以覆盖服务器的运行成本 理论上应该会非常偏宜 现在估计的运行成本 大约是500元/月 如果有100个用户的话 每个人收5元/月 应该问题不大 实在收不上来 自已负担也没问题  </description>
    </item>
    
    <item>
      <title>微信上的ChatGPT机器人新功能语音的使用说明</title>
      <link>/book/%E5%BE%AE%E4%BF%A1%E4%B8%8A%E7%9A%84chatgpt%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BC%80%E5%A7%8B%E6%94%AF%E6%8C%81%E8%AF%AD%E9%9F%B3%E4%BA%86/</link>
      <pubDate>Fri, 21 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%BE%AE%E4%BF%A1%E4%B8%8A%E7%9A%84chatgpt%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BC%80%E5%A7%8B%E6%94%AF%E6%8C%81%E8%AF%AD%E9%9F%B3%E4%BA%86/</guid>
      <description>上一节内容：
 我在微信上用chatGPT做了个大聪明
 20230421更新  微信ChatGPT机器人（大聪明）开始支持语音回复了 又是一个中看不中用的功能（文字问答才是最简单实用的） 触发语音回复的方式如下图所示  进阶技巧：  大聪明的声音是可以设置的，默认是个青年男子的声音，你可以切换成其他的声音  1.获取声音列表  如下图所示，共有25种声音可供选择，支持中，粤，日，韩，美 等主要语音  1:晓晓,女,中文-内地,活泼、温暖的声音，具有多种场景风格和情感 2:云扬,男,中文-内地,专业、流利的声音，具有多种场景风格。 3:云希,男,中文-内地,活泼、阳光的声音，具有丰富的情感，可用于 4:云健,男,中文-内地,适合影视和体育解说， 5:云夏,男,中文-内地,少年年男声， 6:晓伊,女,中文-内地,女声可定制:angry,sad等9种情绪 7:曉佳,女,粤语-香港,曉佳(HiuGaai),粤语女声 8:曉曼,女,粤语-香港,曉曼(HiuMaan),粤语女声 9:雲龍,男,粤语-香港,雲龍(WanLung),粤语男声 10:曉臻,女,中文-台湾,曉臻(HsiaoChen),湾湾女声 11:曉雨,女,中文-台湾,曉雨(HsiaoYu),湾湾女声 12:雲哲,男,中文-台湾,雲哲(YunJhe),湾湾男声 13:Yan,女,英语_香港,Yan,港式英语女声，不支持中文。 14:Sam,男,英语_香港,Sam,港式英语男声，不支持中文。 15:七海,女,日语_日本,七海(Nanami),日语女声 16:圭太,男,日语_日本,圭太(Keita),日语男声 17:선히,女,韩语_韩国,선히(SunHi),韩语女声 18:인준,男,韩语_韩国,인준(InJoon),韩语男声 19:Ana,女,英语_美式,美式英语，女童 20:Aria,女,英语_美式,美式英语，成年女性 21:Jenny,女,英语_美式,美式英语，成年女性 22:Michelle,女,英语_美式,美式英语，成年女性 23:Christopher,男,英语_美式,美式英语，成年男性 24:Eric,男,英语_美式,美式英语，成年男性 25:Guy,男,英语_美式,美式英语，成年男性 2.切换声音  如下图所示，收到回复后，以后大聪明回复你的语音就会使用新的声音  其他说明  语音回复功能的内容也是从chatGPT中获得的 语音回复比纯文字回复，会额外多用5-20秒用时 语音回复在群聊和私聊模式都可以用 另外，经过最近几个星期的测试，私聊功能对一些对隐私比较重视的朋友更友好。 继续开放一些私聊的名额，加大聪明为好友通过以后即可一对一聊天。  加群（推荐）： 加大聪明的微信，进行一对一私聊（不推荐）：  请不要在私聊和群里谈论或诱导机器人谈论敏感话题。
 </description>
    </item>
    
    <item>
      <title>MySQL5.7升级到8.0(二):配置和参数</title>
      <link>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B42%E9%85%8D%E7%BD%AE%E5%92%8C%E5%8F%98%E5%8C%96/</link>
      <pubDate>Thu, 20 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B42%E9%85%8D%E7%BD%AE%E5%92%8C%E5%8F%98%E5%8C%96/</guid>
      <description>研发：MySQL5.7升级到8.0(一):SQL语法变化
  DBA：MySQL5.7升级到8.0(二):配置和参数
   Note：这里面是升级到8.0,需要DBA参与修改部分或注意部分
 1.配置文件.cnf变化 以下参数变化 expire-logs-days =&amp;gt; binlog_expire_logs_seconds # 替换 expire-logs-days tx_isolation =&amp;gt; transaction_isolation tx_read_only =&amp;gt; transaction_read_only innodb_undo_logs =&amp;gt; innodb_rollback_segments have_query_cache = no # 永远为 NO expire-logs-days 后续可能废弃, 使用 binlog_expire_logs_seconds (目前还支持) 以下参数不再支持 innodb_stats_sample_pages innodb_locks_unsafe_for_binlog innodb_file_format innodb_file_format_check innodb_file_format_max innodb_large_prefix ignore_builtin_innodb skip-symbolic-links # 默认即 skip-symbolic-links. sync_frm # 8.0 版本去掉了 .frm 文件, 内置在 ibd 文件中 sql_log_bin # 仅支持会话级别设置 query_cache_xxx # 缓存相关的系统变量 metadata_locks_cache_size metadata_locks_hash_instances date_format datetime_format time_format max_tmp_tables 2.</description>
    </item>
    
    <item>
      <title>职业选择的新思考：ChatGPT是否会取代你的职位？</title>
      <link>/book/chatgpt%E7%9A%84%E5%8F%91%E5%B1%95%E4%BC%9A%E6%9B%BF%E4%BB%A3%E5%93%AA%E4%BA%9B%E8%81%8C%E4%B8%9A/</link>
      <pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/chatgpt%E7%9A%84%E5%8F%91%E5%B1%95%E4%BC%9A%E6%9B%BF%E4%BB%A3%E5%93%AA%E4%BA%9B%E8%81%8C%E4%B8%9A/</guid>
      <description>从AI回答的正确率说起   在微信上建了一个叫大聪明的chatGPT的机器人
  具体看这里: 《我在微信上用chatGPT做了个大聪明》
  中午科长说他经常会跟大聪明聊天沟通想法
  我劝他不要肓信ai的回复
  因为ai的首要目的不是“保真” ，而是“保回复质量”
    像chatGPT之类的大语言模型当然想追求答案的正确性
  但正确性对AI来说还是有点难
  他更像是个博学的学生
  什么都知道一点
  什么都能聊得起来
  但是做不到严格的“正确性”
  虽然给人的感觉是非常聪明
  但那是因为它会得多
  语言组织能力强产生形成的印象
  举例来说
  现在用得最普遍的ChatGPT3.5 和GPT-3.5 的正确率大约是58%
  ChatGPT4的正确率在64%左右，GPT-4的正确率在77%左右
   而人类借助于专业分工，常年的社会网络交流以及借助于资料阅读和工具可以做到（80%-99%)的正确率  现阶段的AI（80%正确率）  现在最顶级的AI(GPT-4)的正确率80% ，接近人类的普通专业人士 这个阶段AI只能是辅助工具 AI现阶段不能代替人类做决定 AI的每一个决定需要人工确认以保证结果是正确的 也就是说AI只是个辅助工具 用来做提升人类的工作效率 但是它的准确性不足以超过人类的信任区域 1.</description>
    </item>
    
    <item>
      <title>MySQL Group Replication(MGR集群)增加节点和迁移节点</title>
      <link>/mysql/mysqlgroupreplicationmgr%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9%E5%92%8C%E8%BF%81%E7%A7%BB%E8%8A%82%E7%82%B9/</link>
      <pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysqlgroupreplicationmgr%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9%E5%92%8C%E8%BF%81%E7%A7%BB%E8%8A%82%E7%82%B9/</guid>
      <description>MySQL Group Replication 多主结构的3节点需要切到另外三个节点上，任务需要先加3个节点到集群中，再删掉原来的3个节点。
 环境   原实例：
 172.0.2.30:3309 172.0.2.31:3309 172.0.2.32:3309    新实例：
 172.0.2.83:3309 172.0.2.84:3309 172.0.2.85:3309    修改host - 修改6台主机的/etc/hosts 172.0.2.30 dba-mysql3309-230 dba-mysql3309-230.dboop.com 172.0.2.31 dba-mysql3309-231 dba-mysql3309-231.dboop.com 172.0.2.32 dba-mysql3309-232 dba-mysql3309-232.dboop.com 172.0.2.83 dba-mysql3309-83 dba-mysql3309-83.dboop.com 172.0.2.84 dba-mysql3309-84 dba-mysql3309-84.dboop.com 172.0.2.85 dba-mysql3309-85 dba-mysql3309-85.dboop.com 旧实例上增加seed  检查状态  mysqlw -h 172.0.2.30 -P 3309 -e &amp;quot;show global variables like &#39;group_replication_group_seeds&#39;&amp;quot;; mysqlw -h 172.0.2.31 -P 3309 -e &amp;quot;show global variables like &#39;group_replication_group_seeds&#39;&amp;quot;; mysqlw -h 172.</description>
    </item>
    
    <item>
      <title>ChatGPT的胡说八道</title>
      <link>/book/chatgpt%E7%9A%84%E6%8F%90%E9%97%AE%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Thu, 30 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/chatgpt%E7%9A%84%E6%8F%90%E9%97%AE%E6%8A%80%E5%B7%A7/</guid>
      <description>前段时间我建了一个微信群，里面接入了chatgpt和百度文心一言的机器人 建群的目的有两个  让没有用过chatgpt的朋友体验一下chatGPT的魔力 我看一下大家（尤其是非技术人员）怎么跟chatgpt对话的   目前这个群里大约有200多人，有各行各业的人，技术/行政/法务/管理/学生/自由职业/艺术&amp;hellip; 多数是随着我的文章加进来的 里面很热闹，有很多奇怪的人问了chatgpt奇怪的问题  问题分类   大家在群里跟chatGPT聊天大约分成以下几类：
 打招呼好奇类：一般是在试探chatGPT能干啥 探索类：比上一类要稍懂一些，而且已经有过一定的使用经验。他们可能是在网上或者自己想出来一些奇怪的问题来试探chatGPT 问时事，问新闻：把chatGPT当成一个对话机器人，问当前时事，问股票，问彩票，问各种新闻 解决问题：问当出现什么报错时（一般是技术问题） 应该怎么做？ 写文档：总结，创意，套话&amp;hellip; 调戏chatgpt：不知道从哪学来的奇怪的话术，在那调戏chatgpt    以上这些，只有5是chatGPT擅长的。
   （未完待续)</description>
    </item>
    
    <item>
      <title>ChatGPT向左,百度文心一言向右.md</title>
      <link>/book/chatgpt%E5%90%91%E5%B7%A6%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E5%90%91%E5%8F%B3/</link>
      <pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/chatgpt%E5%90%91%E5%B7%A6%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E5%90%91%E5%8F%B3/</guid>
      <description> “ChatGPT向左,文心一言向右”:同一条赛道上的两个方向，一个深耕技术，一个用心做产品，但是他们都会成功。
 新闻连着看  3月14日，OpenAI发布GPT-4  比震惊世界的chatGPT(3.5)再次升级：创造力提升，视觉输入，更长的上下文，更强的推理能力。   3月16日，百度发布文心一言  现场演示的时候选择直接放录好的视频，而且采用邀请码的方式没有开放试用，被质疑能力不足（事实上也确实如此） 在开放的试用过程中，被用户发现文字理解有点呆（这个是预期内的，肯定会离chatgpt会有不小的距离）以及更为严重的生成图片时疑似用了国外的图片生成程序（现实是肯定用了国外的图源做训练和标记了，是否用的国外开源产品改的，还不确定）   3月24日，OpenAI发布ChatGPT Plugins  官方演示了Web浏览器，代码解释器，检索信息等方面的Demo   3月27日，百度闭门会议悄悄发布AI服务平台“文心千帆”  现场演示了：对话机器人,三分钟做ppt,数字人直播带货,订机票和酒店等操作。    现状 目前都认可的事实：  ChatGPT的技术还是遥遥领先与世界上的其他竞争对手 百度的大语言模型文言一心已经做出来了，效果虽然赶不上ChatGPT，但至少在中文领域还是可以的（我是3月21日才开始试用文心一言的，并在当天将其接入了我的微信机器人群里）  各自的方向：  技术层面 GPT在一步一步的疯狂往前探索，据说GPT-5已经在内部做安全测试了。AIGC技术这块目前是没有对手的 百度还在努力追赶，可能不远的将来能达到chatGPT初代的中文水平（GPT-3.5），一般预测在3-12个月内可以实现 产品层面 OpenAI公司开始做Plugins，也就是说提供给第三方施展的空间，做一个生态圈，走的是用技术挣钱的方向 百度从文心千帆的闭门会议可以看到一个趋势，就是他自己把AIGC周边的产品都做了，准备往卖服务，用产品挣钱的方向走 形象点说 GPT是技术大牛，开放它的能力，众多第三方开发者一起跟着它做产品。大家一起跟着吃肉喝汤。 文心一言的文心千帆则是直接面向了商家客户，来我这里有现成的产品，你用不，收点钱。 做为开发者，肯定支持GPT这种开放策略，事实上早在几个月前，各种周边的GPT衍生功能产品已经满天飞了，各路开发人员都在给它做各种补充功能：生成PPT,生成视频,翻译软件,pdf检索等等，而官方不断的开放和迭代GPT的能力，语音识别能力，图片输入能力等等。形成了比较好的良性互动 但是从百度的角度，技术上肯定追不上ChatGPT了，但是它可以在产品上下功夫，将开源社区众多开发者的主意，用自己的原生力量打磨出来，做出一个又一个实用且好用的商业化产品，也确实是个更好的选择。挣钱嘛，不寒碜。而且  1.国内短时间还没有哪家ai大语言模型技术能追上百度 。 2.挣完钱以后可以反哺技术，加快追赶的脚步。 3.这是一种很取巧但是实用的路子，百度文心千帆演示的产品中技术难度都不大，采用或者说揉合了开源的或国外现成的很多技术 4.这些产品网上的很多个人开发者都可以短时间完成开发比如:ai生成ppt,ai生成数字人,ai生成行程，但百度拥有的资源做出来的产品肯定会更加好用和精艳。    图：文心千帆企业试用用户：
接下来，我们从百度的闭门会议中发布的ai产品中看看它的技术背景
百度的AIGC产品：文心千帆 场景一：企业办公场景：3分钟制作PPT  和金山wps结合，几句话就可以生成一个精美的ppt,亮点是可以从公司网站上提取信息下来 这个技术难度不大，我在今年2月10号左右，也用chatgpt做了个类似的功能，但是实话说生成的ppt非常丑，远不如这个好看 所以该演示难度对百度来说几乎没有，但是功能非常有用 演示的视频如下：  场景二：电商服务场景：快速生成ai视频  一键生成文案，再用ai数字人技术生成直播视频 技术难度：中等，如果百度是完全自研的那么这个技术难度就有了（事实上应该不是） 开源的方案有，可以用微软的edgeTTS生成语音和字幕，用midjourney生成形象,用DID生成视频 有理由怀疑百度直接用了这些现成的方案（这也是各种数字人用的一整套技术），当然内心里我更希望更有出息一点，自己搞一套 演示的视频如下：  场景三：旅游服务场景：生成旅游规划并完成订单 场景四：金融投资场景：归纳业务评估生成投资建议 总结  百度的文心一言，技术上可以做到国内最优，且始终和国外的先进技术存在很大的落差。 百度的文心一言产品会在商业上产生很大的成功。尤其是toB业务 大语言模型AI,会极大的提高很多职业的工作效率，快跟上，别掉队。  </description>
    </item>
    
    <item>
      <title>百度文心一言发布会,未来可期:为什么我对百度充满信心</title>
      <link>/book/%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E5%8F%91%E5%B8%83%E4%BC%9A%E8%A7%82%E5%90%8E%E6%84%9F%E7%9C%8B%E5%A5%BD%E7%99%BE%E5%BA%A6/</link>
      <pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E5%8F%91%E5%B8%83%E4%BC%9A%E8%A7%82%E5%90%8E%E6%84%9F%E7%9C%8B%E5%A5%BD%E7%99%BE%E5%BA%A6/</guid>
      <description>百度文心一言发布会  3月16日14:00 百度文心一言发布会准时开始 当李厂长拿出ppt来演示，所有ai对话用录好的视频来播放时。 百度股票嗖嗖的下来了 因为一个ai产品不敢现场演示，也不敢开放试用。很难让人信服。  不看好的观点  在微博和微信群里看大家的反馈 大多数人都觉得不太看好百度的文心一言。 其核心点就在 1.于李彦宏现场露怯，主动说了不如chatGPT，其实也没人会认为百度能赶上ChatGPT 2.现场没有演示，demo部分直接放视频，这是非常明显的怕翻车 3.采用邀请码的方式，大概率只会在固定的群体里试用，没有对公众开放，也是信心不足的表现  我的观点  发布会没开之前，我在上班的路上想了几条，发布会的事   - 3月16日的发布会不会跳票，架子都搭好了，再不上去，股价还要不要了 - 李厂长将会亲自上台，演示百度all in ai不是说着玩的 - 不会开放给大家试用，需要排队等待试用机会 - 会拆分出来很多个不同的Ai，有写诗的，有谱曲的，有写小作文的，有写代码的，有总结分析的，有翻译的，有生活助手的，也有瞎聊的…怎么也得分成20个吧 - 现场演示将会很炸（预设好的demo，不给力是不可能的） - 比chatgpt更懂中文，无敌的中文语料库 - 比chatgpt更实时，数据是当天的 - 会演示：在线教育，写代码，写诗，法律援助，实时推荐， - 会开放api和训练合作，已经有商务合作的有…..巴拉巴拉，顺手展示合作伙伴的成功案例 - 会有图片和视频方面的处理能力 - 文心一言，文言一心…经常说错的同学不要担心，换个简单好记的名字 - 持续改进中，我们也是：未来可期..  正确率90%吧 核心点是我觉得百度这次是不会开放给大家用的 因为产品肯定没成熟 但这不妨碍百度在【生成式AI】这个赛道上领先 百度也是国内最有机会站在生成式AI舞台的公司。  看好百度的原因  大厂里第一个正式发布对标ChatGPT的产品。还隆重的开了个新闻发布会 发布会的演示里的那些流畅的对话可能是精心准备的，但可以理解百度的技术已经接近演示里的水平了 百度确实有着国内最强的AI技术储备，这么多年的ALL IN AI 也没白做 如李彦宏多次强调的那样，这么多年的搜索积累，在中文语言的处理上，处于独一无二的位置 百度有着无可比拟的中文语料优势：最大的中文搜索引擎/百度百科/知道 因为有先发优势，会有很多友商和百度合作，已经有很多很多厂商宣布在和文心一言内测合作 还有最重要的原因，发布会也提了：国内这块技术的不足导致的饥渴。 国内这块的技术需求在那，急需一个产品，哪怕是个半成品 ChatGPT国内用不了，就算用不了，也还是会担心中美关系恶化后的被卡脖子。以及数据泄露的问题   所以百度的文心一言，不管产品是否能成功，百度都能是最后的赢家。因为它站在不可替代的位置，国内没人挑战文心一言，它就能立于不败之地。  最后  群里有个聪明人，给GPT4提了一个非常搞笑的prompt：  @gpt4 百度也推出了一个LLM叫做文心一言。请你用一种阴阳怪气的语气对此做出评价，总体的思路是挖苦讽刺。要有具体的例子（比如过往失败的产品，提及那些产品的糟糕体验。注意不要直接说那些糟糕体验，而是通过反讽、幽默的类比等方式来说明）。  然后ChatGPT反馈：  你好，这是GPT-4。我听说了百度推出了一个LLM叫做文心一言 。我觉得这是一个很有创意的名字，就像百度的其他产品一样，比如百度云、百度贴吧、百度糯米等等。😊 文心一言是百度对标OpenAI的ChatGPT ，也就是GPT-3.</description>
    </item>
    
    <item>
      <title>GPT-4的新功能以及我基于GPT-4做的微信机器人</title>
      <link>/book/gpt-4%E7%9A%84%E6%96%B0%E5%8A%9F%E8%83%BD%E4%BB%A5%E5%8F%8A%E6%88%91%E5%9F%BA%E4%BA%8Egpt-4%E5%81%9A%E7%9A%84%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA/</link>
      <pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/gpt-4%E7%9A%84%E6%96%B0%E5%8A%9F%E8%83%BD%E4%BB%A5%E5%8F%8A%E6%88%91%E5%9F%BA%E4%BA%8Egpt-4%E5%81%9A%E7%9A%84%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA/</guid>
      <description>GPT-4的发布  在震惊科技行业的AI聊天机器人ChatGPT发布近四个月后 OpenAI公司又发布了为ChatGPT提供支持的更为强大的下一代技术版本GPT-4 今天(3月15日)凌晨1点，ChatGPT创始人Sam Altman发布了一篇推特：   正式的发布了GPT-4 Sam用五个形容词评价它：capable（adj.能力强的）、aligned（adj.符合预期的）、flawed（adj.有缺陷的）、limited（adj.能力有限的）、impressive（adj.令人印象深刻的）  GPT-4 的升级  相对于GPT3.5，训练参数量上的差别如下：  它可以生成、编辑和与用户迭代创意和技术写作任务，例如创作歌曲、编写剧本或学习用户的写作风格。 它支持生成和处理多达32,768个token（约25,000个单词）的文本，这使得它能够比以前的模型创建或分析更长的内容。 它在各种专业和学术基准测试中表现出“人类水平”的性能，例如通过模拟律师资格考试、法学院入学考试（LSAT）、研究生入学考试（GRE）量化部分和各种AP科目测试。  据 OpenAI 透露，GPT-4 通过了所有基础考试而且是高分通过。例如，GPT-4 在模拟律师资格考试的成绩在考生中排名前 10% 左右，在 SAT 阅读考试中排名前 7% 左右，在 SAT 数学考试中排名前 11% 左右。相比之下，曾经令人震撼的 GPT-3.5 ，真实得分在倒数 10% 左右，GPT-4 的强大已经可想而知。 下面这张图是GPT-4在各项考试中的表现，碾压GPT-3    它可以处理图像输入，并对场景进行识别和描述。  可以看懂图片，并找到这张图片为什么搞笑：巨大的过时VGA接口给小巧的现代智能手机充电    它可以处理表格、图形和图表等数据，并进行分析和解释。     支持更多的数据类型输入      如何用上GPT-4.  现在ChatGPT plus会员有试用，但暂时还没有开放识图功能，且有条数限制 开发者在此页面申请：https://openai.com/waitlist/gpt-4-api 加入等待列表。收到邮件后就能用     微软bing的chat已经升级到4.</description>
    </item>
    
    <item>
      <title>不要再让时间溜走了，让AI来管理你的时间！</title>
      <link>/dba/%E7%94%A8ai%E5%B8%AE%E4%BD%A0%E5%9B%9E%E7%AD%94%E6%97%B6%E9%97%B4%E9%83%BD%E5%8E%BB%E5%93%AA%E5%84%BF%E4%BA%86%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E7%94%A8ai%E5%B8%AE%E4%BD%A0%E5%9B%9E%E7%AD%94%E6%97%B6%E9%97%B4%E9%83%BD%E5%8E%BB%E5%93%AA%E5%84%BF%E4%BA%86%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>问题  如果你不知道工作时间都去哪了 或写工作周报/OKR时不知从何下手 这时候可以每天花5分钟写个工作记录 用现在最流行的AI技术帮你分类一下 或许可以帮助到你  效果  你可以看到最近一段时间的工作时间分配 也可以看到汇总的工作安排 还可以结合考勤表/OKR表进行对比  需要做的工作  每天花5分钟，写几行工作记录 每个工作记录写一行，可以加个时间 0.5h(0.5小时) 起步 可以自己写工作分类项和okr类别，也可以让AI帮你归类（我用的是chatGPT做分类）  AI分类原理  这里用的是chatGPT 将最近的50条已经分好类的工作项，当作Prompt塞给chatGPT 然后要求AI返回这个工作项的分类 prompt如下：  work_prompt=f&amp;quot;&amp;quot;&amp;quot;&amp;quot;工作内容&amp;quot;和[工作分类]的对应关系如下:{contentstr}请在以下分类中:{typestr}为 &amp;quot; %s &amp;quot; 选择一个分类&amp;quot;&amp;quot;&amp;quot;得到的报表  我们说每天的，每个人的工作内容，是无规律的：信息 当我们人为的把它按一定的格式录入下来以后，这些信息收集起来就成了：数据 有了数据，可以用各种维度的展开，对比，这时候可以做：报表 可以有很多种维度，这个月的和上个月的表 可以用A的工作和B的比 最重要的是，它会让你的工作内容变得可回溯  为什么起这个标题？  这个标题也是ai帮我生成的  </description>
    </item>
    
    <item>
      <title>带着chatGPT的NewBing</title>
      <link>/book/%E5%B8%A6%E7%9D%80chatgpt%E7%9A%84newbing/</link>
      <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%B8%A6%E7%9D%80chatgpt%E7%9A%84newbing/</guid>
      <description>NewBing的申请通过了。
应该是现在国内使用chatgpt最方便的方式了</description>
    </item>
    
    <item>
      <title>我在微信上用chatGPT做了个大聪明</title>
      <link>/book/%E6%88%91%E5%9C%A8%E5%BE%AE%E4%BF%A1%E4%B8%8A%E7%94%A8chatgpt%E5%81%9A%E4%BA%86%E4%B8%AA%E5%A4%A7%E8%81%AA%E6%98%8E/</link>
      <pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%88%91%E5%9C%A8%E5%BE%AE%E4%BF%A1%E4%B8%8A%E7%94%A8chatgpt%E5%81%9A%E4%BA%86%E4%B8%AA%E5%A4%A7%E8%81%AA%E6%98%8E/</guid>
      <description>update  2023-04-21 支持语音回复和跟读，有25种声音可选 语音的使用说明 2023-04-04 ChatGPT自带的生成图片功能太吓人了，图片上的人脸跟鬼似的（可能是为了版权考虑，把人脸做了特殊处理），看得受不了，图片部分换成了百度的文心一言  大群不再踢人，不再接收新人加入（认识的朋友还是可以加的），另起一个群接受新人加入，新起的群没有实名制要求. 两个群的权重是3:1 的关系，大群里的内容更容易得到更及时的反馈。   2023-04-02 做了以下修改  修改了机器人的回复框架，从底层重写了一版，回复速度更快（原始版本写得太2了） 群里的聊天可以分辨出是具体的人。（这个很重要&amp;hellip;) 修改了提示词，每次都要@大聪明太费劲了。现在以“请”开头的群聊，都会触发大聪明的回复 增加了生成图片的能力，例生成图片:驴肉火烧 生成语音，视频，ppt&amp;hellip;的功能也准备就绪了，炫酷有余，但实用性不佳。先试试图片的反馈。好玩就会继续跟进放开。 减少了一些功能  暂时停用了gpt4,bing,文心一言的功能，用得人少，稳定性不如GPT3.5 停用了私聊功能，不再让大聪明一对一的回复问题（不这样做的原因是，有人在私聊里发消息略微有点频繁，机器人忙着回复私聊信息，群里的消息回得卡顿了）  私聊功能是个很纠结的点，我相信一定有人更愿意在私聊窗口和机器人对话，而不喜欢在群里对话，但是对我来说这个功能就很累赘。会针对几个朋友保留这个功能     前几天在力哥的帮助和推荐下，群里开始要求实名制，甚至把我拉进去的朋友也给踢了。有朋友表示理解，也有朋友私聊我表示很生气。  支持力哥的想法。力哥是个很有想法的人，我也想学习一下他看待新事情的思维和办事方法。  核心点是&amp;quot;如果你的群里的人连改个名字都不愿意配合你，那这个群对他是没有价值的，你留他在群里干啥&amp;quot;   也希望这个群能更好玩，不要有戾气，否则就违背初衷没意思了，拉这个群就是给朋友们感受一下chatgpt的好玩的用法。 要不要多组几个群，喜欢认真的人在一个群里，喜欢瞎扯的人在另一个群里。互相不打扰。  谈正事的群里，会要求实名。 瞎聊天的群里，就各种随意。 区分是正事的群里的消息回复速度更优先一些，假如两个群里同时都有人在和大聪明交谈，那么机器人会更倾向于谈正事群（渣男算法&amp;hellip;）     关于群成本的问题 群里回复消息调用了chatgpt的api，这个确实是收费的，但是费用极低，可以忽略不计 以现在的提问频率，每天大约花费1/3个茶叶蛋的钱，这个还是负担得起的，不要慌&amp;hellip; 真正的成本在于投入时间维护和改造，好在我是个爱玩的人，这件事还挺好玩的，所以：问题不大。   2023-03-29 任何人都可以和大聪明发起一对一私聊了。不需要额外申请。 gpt4的接口报错太多，暂时禁用了，有时间再修复。 实在懒得去维护加群申请，力哥建议由他来管理群成员。 群管理员交给力哥了。 2023-03-23 修复百度文心一言回复不完整的bug ,增加文心一言生成图片功能  对于想和大聪明私聊的同学，可以一对一和大聪明发起对话了。 私聊中不需要再以 @大聪明 开头了，因为是一对一对话。 开通后会一直有效，不需要再次激活，以后任意时间给大聪明发的消息，默认都是机器人回复。 2023-03-23 21:08 此功能试用中，等稳定了，再继续开通。   2023-03-20 群满员了，现在需要加微信由管理员拉进群。 群主每天会在19:00-20:00 集中处理加群邀请  怎么加入群？  扫描这个二维码，加入群 免费使用，没有限制，只有一条，不要讨论或引导机器人讨论敏感话题免得群被封号。  加大聪明的微信，进行一对一私聊（不推荐）： 怎么提问？  把问题写出来，然后 @大聪明。等个几十秒大聪明调用GPT3.</description>
    </item>
    
    <item>
      <title>有趣的数据_ChatGPT的多语种训练数据集</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BB%BA%E8%AE%AE%E7%94%A8%E8%8B%B1%E8%AF%AD%E5%92%8Cchatgpt%E6%B2%9F%E9%80%9A/</link>
      <pubDate>Tue, 21 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BB%BA%E8%AE%AE%E7%94%A8%E8%8B%B1%E8%AF%AD%E5%92%8Cchatgpt%E6%B2%9F%E9%80%9A/</guid>
      <description>为什么建议用英语和ChatGPT沟通 为什么chatGPT中文对话时候偶尔会感觉到他在胡编瞎造 为什么chatGPT中文提问专业问题时偶尔会出现英语回复  看openai公布的GPT-3训练数据集的语言占比,中文语料只占总训练量的0.1%
   lang 语言名 训练集 占比     en 英语 181014683608 92.65%   fr 法语 3553061536 1.82%   de 德语 2870869396 1.47%   es 西班牙语 1510070974 0.77%   it 意大利语 1187784217 0.61%   pt 葡萄牙语 1025413869 0.52%   nl 荷兰语 669055061 0.34%   ru 俄语 368157074 0.19%   ro 罗马尼亚语 308182352 0.16%   pl 波兰语 303812362 0.</description>
    </item>
    
    <item>
      <title>有趣的数据_海底光缆</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E6%B5%B7%E5%BA%95%E5%85%89%E7%BC%86/</link>
      <pubDate>Mon, 20 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E6%B5%B7%E5%BA%95%E5%85%89%E7%BC%86/</guid>
      <description>周末刷推，发现有人在说中国的互联网对外连接情况，原文说中国对外连接的海底光缆就那几条，一旦打起架来，断了就成局域网
查了下各方面的数据，总结分享给大家
从图说起 全球的海底光缆的分布图 亚洲的海底光缆的分布图 中国的海底光缆的分布图 以上信息来源于一个国外的网站：https://www.submarinecablemap.com/
一些数据  海底光缆作为当代国际通信的重要手段，承担了90%的国际通信业务，是全球信息通信的主要载体。  海底电缆TimeLine  1850年盎格鲁-法国电报公司开始在英法之间铺设了世界第一条海底电缆，只能发送莫尔斯电报密码。 1888年中国第一条海底电缆是清朝时期台湾首任巡抚刘铭传，在1886年铺设通联台湾全岛以及大陆的水路电线，主要作为发送电报用途 1902年环球海底通信电缆建成。 1987年，中国台湾第一条海底电缆完成，即台湾淡水与日本长崎之间。（已停用） 1988年，中国大陆的第一条海底电缆是在1988年完成的，即福州川石岛与台湾（淡水）之间，长177海里。（已停用）  海底光缆  1988年，在美国与英国、法国之间铺设了越洋的海底光缆（TAT-8）系统，全长6700公里。传输速率为280Mb/s*3对，中继站距离为67公里。这是第一条跨越大西洋的通信海底光缆，标志着海底光缆时代的到来。 1993年12月，第一个在中国登陆的国际海底光缆系统是中国——日本（C-J）海底光缆系统。 1997年11月，我国参与建设的全球海底光缆系统（FLAG）建成并投入运营，这是第一条在我国登陆的洲际光缆系统，  中国大陆地区的海底光缆 由于光缆之间存在重合，所以实际上，中国大陆与Internet的所有通道，就是4个入口11条光缆。
中国大陆的海底光缆登陆地点分别是：
 1、青岛（2条光缆） 2、上海（7条光缆） 3、汕头（5条光缆） 4、福州（1条光缆）  青岛1:Trans-Pacific Express（TPE，跨太平洋直达）  带宽：5.12Tbps 长度：17700km 经过地区：  青岛 上海崇明岛 日本千叶县南房总市（原丸山町） 韩国庆尚南道巨济市 台湾新北市淡水区 美国俄勒冈州尼多拿海滩    青岛2:EAC-C2C  带宽：10.24Tbps 长度：36800km 经过地区：  新加坡樟宜 香港将军澳 中国大陆青岛（后期加建） 台湾八里区 菲律宾Capepisa 南韩泰安郡 日本 志摩市 常陆那珂市阿字浦    上海1：Trans-Pacific Express（TPE，跨太平洋直达）  带宽：5.</description>
    </item>
    
    <item>
      <title>探索chatgpt</title>
      <link>/book/%E6%8E%A2%E7%B4%A2chatgpt/</link>
      <pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%8E%A2%E7%B4%A2chatgpt/</guid>
      <description>  探索一下chatgpt的背景和应用场景
  以下是ppt截图
                                                                                              </description>
    </item>
    
    <item>
      <title>从一张图了解ChatGPT会改变哪些行业(职业)</title>
      <link>/book/chatgpt%E5%8F%AF%E4%BB%A5%E6%94%B9%E5%8F%98%E5%93%AA%E4%BA%9B%E8%A1%8C%E4%B8%9A/</link>
      <pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/chatgpt%E5%8F%AF%E4%BB%A5%E6%94%B9%E5%8F%98%E5%93%AA%E4%BA%9B%E8%A1%8C%E4%B8%9A/</guid>
      <description>从ChatGPT对外投资图说起  当我们讨论ChatGPT将改变哪些行业的时候,可以换个思路想 当前对ChatGPT能力理解最清楚的人是谁 肯定是ChatGPT的管理人员,董事会成员,和投资股东  拿了投资方微软的巨量资金后,ChatGPT将这部分钱用于研发和提升产品之余 也成立了openAI基金,投资了一系列的创业公司   看看ChatGPT投资了哪些公司 !
 从图上可以看到ChatGPT投资的公司,大致为成两类 上游: 可以让自己的ai进化 下游: 可以让自己的ai可以改进和提升的领域. 所以ChatGPT的管理团队认为可以有所作为的行业:  1.教育 2.记录/文书 3.法律    学生 (帮助)  由ChatGPT代写作业，已经成为美国大学的一种现象 调查发现，89%的美国大学生已经在用ChatGPT写作业。这就意味着，ChatGPT已经可以从事初级的乃至更高水平的学术研究。 对大学以下的中小学习题,chatGPT可以解答大量的基础题目,给出正确的答案和解题思路 可以用来做作业,也可以用来当成解题助手,家庭老师 同时因为ChatGPT的代写作业能力太强.相应的老师的反作弊工作也提上日程 所以我们看openAI基金投资了:Milo	家长虚拟助理  教师 (部分替代,挑战)  一方面老师们都开始担心学生使用ChatGPT这一技术作弊 另一方面,也要考虑考虑自己的工作安全,部分教师的工作(帮学生解答习题) 随着不断的进化,ChatGPT“迟早可以作为一名老师轻松地授课了”。 所以我们看openAI基金投资了:Speak	AI英语学习平台  记录/文书  录入员,会议记录类工作会被ai工具更有效率的替代 就像这张微软放出来的demo图中,会议中每个人的发言断点,会议提出来的工作项,提醒项,ai都能很好的完成 所以我们看openAI基金投资了:Mem Labs	记笔记应用   法律类工作：  律师助理和法律助理等法律行业工作人员也是在进行大量的信息消化后，综合他们所学到的知识，然后通过撰写法律摘要或意见使内容易于理解。 这和ai的训练路径是一样的,信息消化和学习,是ai最擅长的部分. 所以我们看openAI基金投资了:Harvey	Al法律顾问 参考这篇文章: https://www.lawnext.com/2022/11/stealth-legal-ai-startup-harvey-raises-5m-in-round-led-by-openai.html   会计类工作：  ChatGPT将会很轻松地把财务人员从银行对账、月末入款提醒、进销项差额提醒、增值税验证等这些枯燥重复、初级的工作中解放出来。 甚至，对于是一些专业的财务报告撰写也会带来翻天覆地的影响。 好处是这不仅极大缓解了会计人员的工作强度，而且其凭借客观、准确和及时的特点也很大程度上加强了会计信息的相关性和可靠性。 所以我们看openAI基金投资了:Kick	会计软件  其他类型工作 技术类工作：程序员们  ChatGPT可以快速生成部分基础代码,意味着一项工作在未来可以用更少的员工完成  媒体类工作：广告、内容创作、技术写作、新闻  ChatGPT可以快速生成部分基础代码,意味着一项工作在未来可以用更少的员工完成  客服人员类工作  智能客服的能力,会在ai能力提升,更多的代替人工客服.</description>
    </item>
    
    <item>
      <title>5分钟让你了解ChatGPT是什么</title>
      <link>/book/chatgpt%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/</link>
      <pubDate>Tue, 07 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/chatgpt%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/</guid>
      <description>开门见山  这是一个AI聊天机器人,2022年11月30日发布 如果你从来没有试过ChatGPT的对话 请记得要尝试去体验一下: https://chat.openai.com/chat  如果你不会科学上网 可以用这个地址体验: http://www.580top.com/dboop2023 (访问人数太多,临时加了ip限制,每个ip每天最多可提交10次左右的问题.) ​这是我业余时间为不懂技术的朋友做的一个页面 无需vpn,无需账号,直接就可以提问   火遍全网的chatGPT是什么? 专业的介绍  ChatGPT是OpenAI开发的一个大型预训练语言模型。它是GPT-3模型的变体，GPT-3经过训练，可以在对话中生成类似人类的文本响应。ChatGPT 旨在用作聊天机器人，我们可以对其进行微调，以完成各种任务，如回答问题、提供信息或参与对话。与许多使用预定义的响应或规则生成文本的聊天机器人不同，ChatGPT经过了训练，可以根据接收到的输入生成响应，从而生成更自然、更多样化的响应。  通俗的介绍: 你可以理解为它是一个聊天机器人,这个机器人会的东西比在此之前的聊天机器人更丰富一些,表现在:
 它有更强的语言理解能力 (更能听得懂人话) 它有更强的语言组织能力 (说的话更符合人类的预期) 它有强大的创作能力AIGC（利用人工智能技术自动生产内容）  以上三点都比在此之前的AI表现得更好,更丝滑,使整个ChatGPT的效果体验非常的惊人.所以突然爆火
ChatGPT是谁做的? 股权在哪家机构? 简单的说:始于Altman和Musk的10亿美元,目前受益最大的是微软.
 2015年，山姆·阿尔特曼与埃隆·马斯克、彼得泰尔、雷德霍夫曼等大佬在[罗斯伍德桑德希尔酒店]吃了一顿晚饭,决定创建一个新的人工智能实验室。 山姆·阿尔特曼（Sam Altman）: 时任知名初创公司孵化器Y Combinator负责人，现为OpenAI联合创始人兼首席执行官 埃隆·马斯克(Elon Musk): 硅谷钢铁侠,特斯拉,SpaceX &amp;hellip;  他们创立了OpenAI,马斯克和阿尔特曼立志用10亿美元的初始资金，打造对人类友好的人工智能，以非营利组织为主体，定期向公众开放AI研究成果和专利。 OpenAI研发的GPT-1、GPT-2模型均对外开源，向外部开发者共享代码和数据。 2018年，马斯克以理念不和为由宣布退出OpenAI，顺带把一些相关研发人员挖去了特斯拉。OpenAI一度被外界调侃成“特斯拉的AI技术人才输送站”。 OpenAI宣布重组，由非营利性的母公司OpenAI Inc和营利性的子公司OpenAI LP组成。至此，OpenAI也从非营利实验室转型为“利润上限”公司，这为投资者和大型科技公司的投资打开了通道，他们的回报上限为投资的 100 倍。 资本开始登场,2019年7月，微软宣布以10亿美元入资OpenAI。一个重要的前提是，微软有权将OpenAI的部分技术商业化，同时，双方达成一项多年的合作协议 2020年，GPT-3完成迭代出现了商业化用例，同年9月，微软宣布获得GPT-3模型的独家授权,其后微软多次追加投资。 2022年11月30日ChatGPT聊天机器人上线。发布仅一周的时间，就已经拥有超100万用户。发布2个月后活跃用户过亿  OpenAI的股权投资协议模式  OpenAI选择了—种新的股权投资协议模式。未来盈利后的OpenAI的利润分配将按照以下四个阶段进行。 第—阶段将优先保证埃隆马斯克、 彼得泰尔、 雷德霍夫曼等首批 投资者收回初始资本； 在第二阶段，微软将有权获得OpenAl75%的利润，直至收回其130亿美元投资； 第三阶段，在OpenAI的利润达到920亿美元后，微软在该公司 的持股比例将下降到49%, 剩余49%的利润由其他风险投资者和OpenAI的员工分享。 第四阶段，在利润达到1,500亿美元后，微软和其他风险投资者的股份将无偿转让给OpenAI的非营利基金。  国内的chatGPT能力:百度占优  chatGPT的火爆上线后,国内同类型产品的研发就已经在开展.</description>
    </item>
    
    <item>
      <title>自动化流程:数据找回(二:Oracle部分)</title>
      <link>/dba/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B_%E6%95%B0%E6%8D%AE%E6%89%BE%E5%9B%9E%E9%97%AA%E5%9B%9E_2/</link>
      <pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B_%E6%95%B0%E6%8D%AE%E6%89%BE%E5%9B%9E%E9%97%AA%E5%9B%9E_2/</guid>
      <description>续上一篇： 自动化流程:数据找回(一:MySQL数据闪回)
上一篇说的是
 1.数据找回的作用 2.MySQL数据找回自动化流程功能演示 3.自动化流程的代码配置  这篇主要介绍Oracle的数据找回自动化流程.
一些改进  改进1:Oracle数据找回功能从利用OGG改到用归档日志  上一篇的结尾,说Oracle的数据找回.要用到OGG+Kafka的方案. ​最终经过对比验证,换了更好的Oracle原生归档日志方案   改进2:增加了操作类型的筛选项,这样可以针对指定的操作(例如:delete)进行redo和undo  Oracle数据回退功能的方案对比 当尝试做Oracle数据回退功能时有以下几个方案
 Oracle数据闪回功能 ,优:自带的功能,速度快,简单 缺:需要开启闪回段,闪回的时间太短(平均约30分钟内) 全备和日志还原,优:30天内数据可回退 ,缺:慢,需要资源大,复杂不利于自动化脚本 OGG,因为我们的环境大部分重点表都做了OGG到kafka的订阅, 所以可以直接扫描kafka的信息来获取指定表的变更, 优:资源占用少,快,开发难度低 缺: 如果要回退的这张表没有做OGG订阅怎么办? 归档日志解析, 优:7天内任意时间点可回退 ,缺: 每次回退的时间段不能太长  我们完成了方案3和方案4的代码开发工作,最终选用了方案4: 归档日志解析
 脚本化实现难度小 7天内任意时间点,能满足90%的业务需求 回滚速度快,举个例子:  研发提交流程,要求看A数据库里Order表在周三上午10:00-10:30的所有修改动作,提交申请后,大约10秒内就可以完成这个流程.   覆盖的范围大,线上/测试/开发 环境的所有库表都支持 占用资源少,对线上影响小  Oracle数据回退功能演示  在测试环境还原一次全库的历史快照只需要 0.4秒,非常的快速!!!  小结 当数据库有了这个功能以后:
 研发可以在改错数据的时候,快速提交流程回退数据 当有奇怪的线上数据问题时,可以很方便的生成redoSQL,重放过去某个时间段的数据修改 数据库有大量写入时,快速的定位是哪张表,什么样的SQL引起的  </description>
    </item>
    
    <item>
      <title>自动化流程:数据找回(一:MySQL数据闪回)</title>
      <link>/dba/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B_%E6%95%B0%E6%8D%AE%E6%89%BE%E5%9B%9E%E9%97%AA%E5%9B%9E/</link>
      <pubDate>Mon, 30 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B_%E6%95%B0%E6%8D%AE%E6%89%BE%E5%9B%9E%E9%97%AA%E5%9B%9E/</guid>
      <description>功能介绍 数据找回,又名数据闪回,误删除回滚,误操作回滚,数据重做&amp;hellip;
解决的问题  数据库里的数据被误删除/更新,需要定位修改的SQL语句,影响范围,回滚误操作,此时怎么办?
  方法1: DBA用前一天的数据库备份还原出来,然后找到更新前的SQL(这个通常需要花费几个小时的时间) 方法2: 如果发现误操作的时间比较及时,一般7天以内,可以通过解析数据库日志(MySQL的binlog,Oracle的archivelog)比较快速的找到误操作的SQL及时回滚  两个方法,都需要DBA参与
   方法 依赖文件 可恢复范围 操作用时     方法一 全备 1个月内 1-5小时   方法二 日志 7天 0.5小时    其中方法2,需要DBA找到并登录目标服务器,解析数据库日志,其中有研发和DBA的沟通时间,手动处理时间,我们可以通过自动化流程,让研发人员自助填写需求,在dba审批通过后系统自动执行.
自动化流程的作用  1.提高工作效率,不再需要DBA手动去查日志,解析日志&amp;hellip; 将原先几十分钟的工作,变成自动化 2.减少误操作,减少手动处理问题时的手滑,操作命令写错了的运维风险 3.减少沟通成本,自助化服务无需(dba和研发)反复沟通细节 4.一般发现有数据更新错了,需要很快的得到响应,自助化流程可以减少非工作时间联系DBA的响应时长 5.便于统计哪些项目成员或业务线出现这种误更新的失误.  功能预览 设计了以下的表单填写页面,用于收集信息 设计了两步审批  负责人审批: 如果该库有具体的DB负责人,则DB负责人审批.否则由提交申请的用户上级审批 DBA审批:DBA审批时会看到将要自动化执行的命令,并且可以在执行前修改部分参数.(通常默认的命令是最优的)  后端执行  执行是异步的.执行过程中会展示运行日志 执行完成后,会展示以下三块内容 1.后台运行日志 (用于DBA排查自动化任务时是否有异常) 2.SQL内容概览 (展示符合条件的表共有多少次ins/upd/del操作. 用于验证是否符合预期,该概览目前只支持MySQL,Oracle的还没实现) 3.解析出来的SQL文件 (最重要的部分)  代码实现 step1:表单代码 &amp;lt;step id=&amp;quot;1&amp;quot; name=&amp;quot;数据找回&amp;quot; displaytype=&amp;quot;input&amp;quot;&amp;gt;&amp;lt;form&amp;gt; &amp;lt;input name=&amp;quot;dbid&amp;quot; title=&amp;quot;选择数据库&amp;quot; inputtype=&amp;quot;select_group&amp;quot; desc=&amp;quot;&amp;quot; defaultvalue=&amp;quot;&amp;quot; &amp;gt;&amp;lt;style&amp;gt;&amp;lt;div-class&amp;gt;span24&amp;lt;/div-class&amp;gt;&amp;lt;input-class&amp;gt;input-normal,input-small,input-normal,input-normal&amp;lt;/input-class&amp;gt; &amp;lt;connstr&amp;gt;link:db_monitor&amp;lt;/connstr&amp;gt;&amp;lt;sqlstr&amp;gt;select b.</description>
    </item>
    
    <item>
      <title>将没有parentid的二维表转换成json的树状结构(python版)</title>
      <link>/ops/%E5%B0%86%E6%B2%A1%E6%9C%89parentid%E7%9A%84%E4%BA%8C%E7%BB%B4%E8%A1%A8%E8%BD%AC%E6%8D%A2%E6%88%90json%E7%9A%84%E6%A0%91%E7%8A%B6%E7%BB%93%E6%9E%84python%E7%89%88/</link>
      <pubDate>Fri, 06 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B0%86%E6%B2%A1%E6%9C%89parentid%E7%9A%84%E4%BA%8C%E7%BB%B4%E8%A1%A8%E8%BD%AC%E6%8D%A2%E6%88%90json%E7%9A%84%E6%A0%91%E7%8A%B6%E7%BB%93%E6%9E%84python%E7%89%88/</guid>
      <description>需求  数据库里有这样的二维表  (id,country,province,city) (1,&amp;quot;a&amp;quot;,&amp;quot;aa&amp;quot;,&amp;quot;aa1&amp;quot;), (&amp;quot;2&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;aa&amp;quot;,&amp;quot;aa2&amp;quot;), (&amp;quot;3&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;bb&amp;quot;,&amp;quot;bb1&amp;quot;), (&amp;quot;4&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;bb&amp;quot;,&amp;quot;bb2&amp;quot;), (&amp;quot;5&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;bb&amp;quot;,&amp;quot;bb3&amp;quot;), (&amp;quot;6&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;cc&amp;quot;,&amp;quot;cc1&amp;quot;), 转换成在json中可用的树状结构
[{ &amp;quot;id&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;a|aa&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;aa&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: 1, &amp;quot;parent_id&amp;quot;: &amp;quot;a|aa&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;aa1&amp;quot;, &amp;quot;leaf&amp;quot;: true }, { &amp;quot;id&amp;quot;: &amp;quot;2&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;a|aa&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;aa2&amp;quot;, &amp;quot;leaf&amp;quot;: true }] }] }, { &amp;quot;id&amp;quot;: &amp;quot;b&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;b&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;b|bb&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;b&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;bb&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;3&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;b|bb&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;bb1&amp;quot;, &amp;quot;leaf&amp;quot;: true }, { &amp;quot;id&amp;quot;: &amp;quot;4&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;b|bb&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;bb2&amp;quot;, &amp;quot;leaf&amp;quot;: true }, { &amp;quot;id&amp;quot;: &amp;quot;5&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;b|bb&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;bb3&amp;quot;, &amp;quot;leaf&amp;quot;: true }] }] }, { &amp;quot;id&amp;quot;: &amp;quot;c&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;c&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;c|cc&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;c&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;cc&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;6&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;c|cc&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;cc1&amp;quot;, &amp;quot;leaf&amp;quot;: true }] }] }]  本来以为很好写的一小段,写起来发现还挺麻烦的  难点  二维表转json tree 还是比较常见的写法,但是这个二维表里没有parentid,所以上下级关系需要用country,province 两列来对齐生成  代码  我写了一个python版的实现   class jsontree_str_(): def __init__(self) -&amp;gt; None: pass def get_jsonstr_parentid(self,rows,columns): sb_rows=[] columnsi=len(columns) if len(rows)==0 or columnsi&amp;lt;3: return sb_rows dict_ids={} for row in rows: for i in range(1,columnsi): idstr=&amp;quot;|&amp;quot;.</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(四):BinLog</title>
      <link>/mysql/mysql%E7%9A%84binlog%E6%97%A5%E5%BF%97/</link>
      <pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84binlog%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续：
 MySQL的7种日志(一):概况 MySQL的7种日志(二):RedoLog MySQL的7种日志(三):UndoLog MySQL的7种日志(四):BinLog  1.什么是binlog  又名:MySQL归档日志,MySQL二进制日志 记录所有数据库表结构变更（DDL例如CREATE、ALTER TABLE…）以及表数据修改（DMLINSERT、UPDATE、DELETE …）的所有操作。 默认情况下，二进制日志并不是在每次写的时候同步到磁盘。因此，当数据库所在地操作系统发生宕机时，可能会有最后一部分数据没有写入二进制日志文件中，这会给恢复和复制带来问题。  2.binlog的作用  时间点的恢复：某些数据的恢复需要二进制日志，例如，在一个数据库全备文件恢复后，用户可通过二进制日志进行即时点（point-in-time）恢复。 主从复制：通过复制和执行二进制日志使一台远程的 Mysql 数据库（一般称为 slave）与一台 MySQL 数据库（一般称为 master）进行实时同步。 变更审计：用户可以通过二进制日志中的信息来进行审计，回溯是否对数据库的修改。 误操作回滚：当误修改(ins/upd/del)发生时,可以用binlog解析出修改前后的语句,用于快速回滚 异构数据同步：通过解析binlog,可以将MySQL的变更通知到异构数据源(kafka,es,mongo,redis,mq&amp;hellip;) 事务存储引擎的崩溃恢复。MySQL采用事务的两阶段提交协议。当 MySQL 系统发生崩溃时，事务在存储引擎内部的状态可能为 prepared 和 commit 两种。对于 prepared 状态的事务，是进行提交操作还是进行回滚操作，这时需要参考 binlog：如果事务在 binlog 中存在，那么将其提交；如果不在 binlog 中存在，那么将其回滚，这样就保证了数据在主库和从库之间的一致性。  3.binlog 和 redolog 区别  适用对象不同： binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用 redolog 是 InnoDB 引擎特有的 写入内容不同：  binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：语句 ROW：记录行数据最终被修改成什么样了 MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redolog 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；   写入方式不同： binlog 是可以追加写入的。“追加写” 是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志 redolog 是循环写的，空间固定会被用完 作用不同  4.</description>
    </item>
    
    <item>
      <title>mongodb性能监控指标详细解释</title>
      <link>/dba/mongodb%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/mongodb%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A/</guid>
      <description>常用监控项及说明 当我们监控mongodb实例时，大约有300多项的监控指标，通常我们可以关注以下的20项指标就够了
   监控项 说明     mongodb_memory 内存占用（MiB）   mongodb_mongod_op_latencies_latency_total 累计操作耗时（毫秒）   mongodb_mongod_op_latencies_ops_total 累计操作次数   mongodb_op_counters_total 累计接收的操作请求次数（即使操作不成功也会增加）   mongodb_connections 连接数   mongodb_mongod_metrics_cursor_open 打开游标数量   mongodb_mongod_metrics_document_total 累计文档操作次数   mongodb_mongod_global_lock_current_queue 当前排队等待获取锁的操作个数   mongodb_mongod_metrics_query_executor_total 查询和查询计划评估过程扫描的（索引或文档）条目总数   mongodb_asserts_total 累计断言错误次数   mongodb_mongod_metrics_get_last_error_wtime_num_total 累计getLastError操作数量   mongodb_mongod_wiredtiger_cache_bytes 当前缓存数据大小（byte）   mongodb_mongod_wiredtiger_cache_bytes_total 写入或读取的缓存数据大小（byte）   mongodb_mongod_wiredtiger_cache_pages 当前缓存页数量   mongodb_mongod_wiredtiger_cache_evicted_total 累计缓存移除页数量   mongodb_extra_info_page_faults_total 累计缺页中断次数   mongodb_ss_network_bytesOut 累计发送网络流量（byte）   mongodb_ss_network_bytesIn 累计接收网络流量（byte）   mongodb_mongod_replset_member_replication_lag 副本集成员主从延迟（秒）    mongodb_memory  mongodb_memory 指标表示 MongoDB 数据库实例使用的内存量。这个指标可以帮助监控系统管理员查看 MongoDB 的内存使用情况，并对系统的内存进行优化。 resident 和virtual resident 指的是进程在物理内存中占用的空间，即进程实际使用的物理内存。 virtual 指的是进程在虚拟内存中占用的空间，即进程所占用的总内存，包括物理内存和交换空间。 通常来说，如果 resident 值很大，说明进程实际使用的物理内存很多，这可能表示系统的内存不足，或者进程的内存使用不合理。如果 virtual 值很大，说明进程占用的总内存很多，这可能表示进程在使用较多的交换空间，或者系统的总内存不足。 总之，resident 和 virtual 指标可以帮助你了解进程对内存的使用情况，从而为进行性能优化提供重要的参考信息。 获取代码：  例：mongodb_memory{job=&amp;quot;mongodb&amp;quot;, service=&amp;quot;mongodb&amp;quot;, team=&amp;quot;dba&amp;quot;, type=&amp;quot;resident&amp;quot;}34957mongodb_memory{job=&amp;quot;mongodb&amp;quot;, service=&amp;quot;mongodb&amp;quot;, team=&amp;quot;dba&amp;quot;, type=&amp;quot;virtual&amp;quot;}49537mongodb_mongod_op_latencies_latency_total   mongodb_mongod_op_latencies_latency_total 指标中的 read、write、command、transactions 分别表示 MongoDB 数据库实例中的不同操作类型的平均响应时间。</description>
    </item>
    
    <item>
      <title>《超越期待》读书笔记</title>
      <link>/book/%E8%B6%85%E8%B6%8A%E6%9C%9F%E5%BE%85%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%B6%85%E8%B6%8A%E6%9C%9F%E5%BE%85%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</guid>
      <description>所谓商人就是做生意的人，我把自己当作从商资本，从十几岁开始就一直在经营自己。做生意，要先从让对方了解自己开始。学会建立自我品牌。 为对方提供超越期待的东西，让更多人开心、感动，才能收获惊喜。 &amp;mdash;-《超越期待》【日】松浦弥太郎 著，人民邮电出版社
 看闲书  2022年11月24日18:30 感恩节 在盒马上订了一只烧鸡和火锅显示还在配送中 窝在沙发上看了本日本人写的闲书（非计算机技术的书我都叫它闲书） 《超越期待》作者是一个成功的日本商人松浦弥太郎 用半自传体的方式写了作者对待人生和生意的一些经营原则 很久没看非技术类的书了 看进去了就写个读书笔记  01.人生就是一场生意  人生就是一场生意，生意的本质就是看清自己，尽可能地帮助他人。 事实上，“利他”的背后其实是利己，有舍才有得。 有人认为“利他”是心灵鸡汤，实际上这是人生另一种心界，无私的心界。 超越期待这4个字，不仅仅是让他人感受到我们可以提供给对方惊喜的东西， 更重要的一点是，我们先要超越自己、先要让自己走出人生的困惑。 只有这样，我们才能带给他人足够的价值，才能为他人提供超越期待的惊喜。 慢慢积累个人品牌，从赋能他人做起吧。  02.带着经营者意识  哪怕只是公司普通职员经营者意识也很重要 经营者意识和公司职员看待事物的角度完全不同 在时间的利用上 经营者意识很少去想如何能让自己轻松一点 更多的是想着怎样将潜力发挥更大 并积极采取行动 在同事关系上 经营者就算是工作中有不满 对同事有抱怨 也要思考如何顺利的把事情完成 减少对人表达负面情绪 经营者意识不是以公司为家 工作的意义不止是个人的成长 工作的意义还在于这份工作对公司的贡献和价值 学会用经营者的角度去看问题 “利他”的同时让自己“物尽其用”成长  03.不发言即是风险  在一些会议上或类似的场合 有些人喜欢一言不发或者说些无关紧要的话 没有人会去批评不发言的人 很多时候人们会意识不到这样会有负面评价 不发言 不敢表达自己的观点的原因 可能是觉得自己的观点不重要 或是担心自己轻浮的发言带来负面印象 这时候不发言或附合是最保险的策略 但是这样的结果 1.失去了表达自己观点的机会 2.失去了交流沟通的机会（越是陌生多的场合，这种交流越重要） 3.失去了别人了解你的机会 在很多的人生教育里 都告诉我们沉默不语是件好事 它代表的是知道一切但是不炫熠 这更适合的是生活 但在需要更多主动沟通交流的工作场所 并不是一件非常好的方式 在陌生的场合勇敢的表达自己的观点 坦诚的讲出来 哪怕你还没有整理好思路也不要怕  04.</description>
    </item>
    
    <item>
      <title>Redis的缓存一致性/缓存溢出/缓存雪崩/缓存穿透/缓存击穿</title>
      <link>/dba/redis%E7%BC%93%E5%AD%98-%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E6%BA%A2%E5%87%BA%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F%E5%87%BB%E7%A9%BF/</link>
      <pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/redis%E7%BC%93%E5%AD%98-%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E6%BA%A2%E5%87%BA%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F%E5%87%BB%E7%A9%BF/</guid>
      <description>尝试用一个例子来描述高并发系统下的缓存设计，一边举例子一边描述和解决以下问题。
 为什么要用缓存? 缓存一致性问题? 缓存溢出问题? 缓存雪崩问题? 缓存穿透问题? 缓存击穿问题?  问题：  假设我们有5000万条商品信息存储在数据库中，现在这些信息要展示给用户看，我们需要做什么？
 答案1:直连数据库  代码中直接访问数据库，读取数据，展示给用户看，这个方法可以吗？ 答案是：访问量少的时候可以，系统访问量大了就崩了。 事实上大多数的内部系统和ToB业务，访问量不大，直接用数据库就解决问题了 如果业务访问量上来了，这时候频繁访问数据库，就会造成很明显的瓶颈。 这也是大多数“古典“网站和系统，用户访问一多就崩溃的原因 在设计系统的时候没有考虑：高访问量，高并发 一般认为预计访问量有超过2000次/秒，直连数据库的方案就不太建议了 为了避免数据库被打崩，我们就需要考虑在数据库和代码层之间加上一个缓存 有很多种缓存，下面以用得最多的Redis来举例子  答案2:加缓存（例如Redis)  现在我们用了Redis在数据库和业务之间做缓冲 需要访问一个商品的时候  1.业务传过来一个商品id 2.在redis中查找是否有这个id的信息，有就直接返回 3.如果redis中没有找到，去数据库里读取，读取到了信息存入redis，并返回给用户   因为多了一层redis，程序性能得到了极大的优化 访问变快了（纯内存的redis比MySQL要快很多） 不会因为大量的访问被堵死了（单节点的Redis可负担的简单QPS大约是10万，MySQL大约是0.4万） 现在系统的瓶颈解决了，那么接着往下想 如果此时数据库的信息被更新了，Redis中的缓存信息怎么办？ 可能有同学认为，数据库更新了，也把Redis信息同步更新/或删除了不就行了 事实上你细想一下，就没那么简单了 这就是引出了一个问题：缓存一致性问题  缓存一致性问题  当修改一条商品信息，MySQL和Redis缓存都需要修改，两者之间会有先后顺序，可能导致数据不一致。
  当我们需要修改商品时，需要考虑3个问题：  1.先更新缓存还是先更新数据库？ 2.更新缓存的时候,是更新(update)缓存，还是删除(delete)缓存？ 3.怎么更新缓存保证一致性？    1、先更新缓存还是先更新数据库？  如果先更新缓存，写数据库失败，则缓存为最新数据，数据库为旧数据，缓存为脏数据。 之后其他查询马上进来就会拿到这个数据，但是这个数据在数据库中是不存在的。 数据库中不存在的数据缓存并返回给客户端是没有意义的。 所以不能先更新缓存。只能是：DB First  2、更新缓存的时候,是更新(update)缓存，还是删除(delete)缓存？  这里推荐是修改商品的时候，直接删除(delete)缓存 原因是update缓存通常比delete缓存需要更多的资源 为了得到一条商品的完整信息，可能会join几张表得到一个json，组装起来set到redis中的代价，会比直接del一个rediskey要大得多 而在一个高并发系统中，我们要尽可能的保证整个修改是尽可能快的完成(代价是一次缓存失效)  3.</description>
    </item>
    
    <item>
      <title>MySQL中间件对比:ProxySQL/MaxScale/ShardingSphere</title>
      <link>/mysql/mysql%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Thu, 03 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%AF%B9%E6%AF%94/</guid>
      <description>MySQL中件间对比 目标：对比以下3款MySQL中件间
 ProxySQL MaxScale ShardingSphere  一.Proxy基础能力 兼容性  ProxySQL:良好 MaxScale:mairdb更友好，MGR支持不好 ShardingSphere:良好  发行方  ProxySQL:sysown MaxScale:mairdb ShardingSphere:京东  发布/更新/生态  ProxySQL:c++, 5.1k stars,最新版v2.4.4 1个半月前发布 MaxScale:c,1.6k stars,最新版22.08.2 3个半月前发布 ShardingSphere:java,17.6k stars,最新版5.2.1 16天前发布  运维便利性  ProxySQL:简单 MaxScale:普通 ShardingSphere:复杂（配置文件多且复杂）  二.Proxy高可用架构支持 主从架构下的从库故障  如何探测到   ALL: 每N秒主动探测一次
  如何响应故障   ALL:探测不到，主动处理: - ShardingSphere:从路由表中标记下线 - proxysql:从group中标记下线 - MaxScale:标记下线
  切换后状态   ALL:复制拓扑可用
  影响时长   ALL:1-5 秒</description>
    </item>
    
    <item>
      <title>翻到一个旧网站580top.com的尸体</title>
      <link>/book/580top%E7%BD%91%E7%AB%99/</link>
      <pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/580top%E7%BD%91%E7%AB%99/</guid>
      <description> 整理一个废弃的移动硬盘 发现以前做的一个网站的文件 10几年前的我审美也是在线的 网站风格很清新 现在看来也不过时 虽然核心的动态文件，数据库都没了 但是生成的静态文件还在 于是我把它复活了   快速旧网站导航 SQLServer Oracle MySQL  MySQL基础知识 MySQL高可用性和HA MySQL性能与优化 MySQL故障与解决 MySQL源码解读   其他数据库 系统运维 DBA考试 DBA话题  </description>
    </item>
    
    <item>
      <title>MySQL高可用组件之ProxySQL</title>
      <link>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Bproxysql/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Bproxysql/</guid>
      <description>ProxySQL是什么 介绍  MySQL一款开源的中间件的产品 支持读写分离 支持 Query 路由功能 支持动态指定某个SQL进行缓存 支持动态加载（无需重启ProxySQL服务） 故障切换和SQL过滤功能。  ProxySQL初始化 安装 wget https://github.com/sysown/proxysql/releases/download/v2.4.4/proxysql-2.4.4-1-centos7.x86_64.rpm rpm -ivh proxysql-2.4.4-1-centos7.x86_64.rpm 产生报错： warning: proxysql-2.4.4-1-centos7.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 8217c97e: NOKEY error: Failed dependencies: gnutls is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28()(64bit) is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28(GNUTLS_1_4)(64bit) is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28(GNUTLS_3_0_0)(64bit) is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28(GNUTLS_3_1_0)(64bit) is needed by proxysql-2.4.4-1.x86_64 执行： yum install -y gnutls rpm -ivh proxysql-2.4.4-1-centos7.x86_64.rpm 检查安装情况  rpm -ql proxysql /etc/logrotate.</description>
    </item>
    
    <item>
      <title>MySQL高可用组件之orchestrator</title>
      <link>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Borchestrator/</link>
      <pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Borchestrator/</guid>
      <description>orchestrator是什么 介绍  GitHub公司的用go语言编写并开源的一个MySQL高可用管理工具 MySQL高可用性和复制拓扑管理工具，支持复制拓扑结构的调整，自动故障转移和手动主从切换等。 提供Web界面展示MySQL复制的拓扑关系及状态，通过Web可更改MySQL实例的复制关系和部分配置信息 同时也提供命令行和api接口，方便运维管理。 相对比MHA来看最重要的是解决了管理节点的单点问题，其通过raft协议保证本身的高可用。  特点  自动发现MySQL的复制拓扑，并且在web上展示。 重构复制关系，可以在web进行拖图来进行复制关系变更。 检测主异常，并可以自动或手动恢复，通过Hooks进行自定义脚本。 支持命令行和web界面管理复制。  功能限制  slave不能手动提升为master 不支持多源复制 不支持并行复制 不支持与PXC联合使用  注意事项  对主机名依赖严重，习惯用ip来管理实例的，需要注意确保主机名可解析 主从拓扑结果目前不支持两个实例互为主备或环形结构 自动故障转移，只负责将主从切换了，把从设置为主，其他的变更（dns或proxy变更以及运维信息的变更等需要自己写hook脚本）  orchestrator的工作原理 orchestrator的探测机制  orchestrator会每隔InstancePollSeconds（默认5s）时间用以下SQL去被监控的实例上读取实例状态  show global status like &#39;Uptime&#39; select @@global.hostname, ifnull(@@global.report_host, &#39;&#39;), @@global.server_id, @@global.version, @@global.version_comment, @@global.read_only, @@global.binlog_format, @@global.log_bin, @@global.log_slave_updates show master status show global status like &#39;rpl_semi_sync_%_status&#39; select @@global.gtid_mode, @@global.server_uuid, @@global.gtid_executed, @@global.gtid_purged, @@global.master_info_repository = &#39;TABLE&#39;, @@global.binlog_row_image show slave status select count(*) &amp;gt; 0 and MAX(User_name) !</description>
    </item>
    
    <item>
      <title>2022年视觉下的元宇宙</title>
      <link>/book/2022%E5%B9%B4%E8%A7%86%E8%A7%89%E4%B8%8B%E7%9A%84%E5%85%83%E5%AE%87%E5%AE%99/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/2022%E5%B9%B4%E8%A7%86%E8%A7%89%E4%B8%8B%E7%9A%84%E5%85%83%E5%AE%87%E5%AE%99/</guid>
      <description>01 什么是元宇宙  元宇宙就是「metaverse」 verse 是宇宙 meta 来自遥远的古希腊。 metaphysics，翻译叫做形而上学 我们生活的现实宇宙就是 universe 对应的虚拟宇宙（元宇宙）就是 metaverse 所以元宇宙就是说区别真实存在的宇宙，我们用意念幻想出来的一个虚拟的宇宙世界（可以理解成人们虚拟出来的一个平行世界） 北京大学陈刚教授、董浩宇博士这样定义元宇宙：“元宇宙是利用科技手段进行链接与创造的，与现实世界映射与交互的虚拟世界，具备新型社会体系的数字生活空间。” 电影《头号玩家》  02 元宇宙概念的形成   1981年年弗诺·文奇的《真名实姓》和 1984 年的威廉·吉布森的《神经漫游者》科幻小说构建了科幻的世界框架被称为赛博朋克（Cyberpunk）
  1992年的科幻小说《雪崩》，作者 Neal Stephenson 描绘了一个虚拟的平行数字世界（metaverse），人们用阿凡达（Avatar）在其中生活、工作、娱乐。
  《雪崩》创造了 metaverse 这个单词来命名 cyberspace。
  一般人们认为1992年的小说《雪崩》是元宇宙的起源,因为它创造了：metaverse这个词
  这本小说引入国内时，译者将 metaverse 翻译成了「超元域」。所以「元宇宙」这个概念，只是科幻小说中对于人类未来的一种假设
  2003年游戏《Second Life》发布，它在理念上给我们部分解放了现实世界所面临的窘境，这句话怎么理解。就是我们在现实世界中最痛苦的一件事是不能快速调整自己的身份，而在虚拟世界当中，我们可以通过拥有自己的分身来实现，所以《Second Life》给了我们过一种新生活的可能性。
  2021年是元宇宙元年。
  2021年初，Soul App提出构建“社交元宇宙”。
  2021年3月，被称为元宇宙第一股的罗布乐思（Roblox）正式在纽约证券交易所上市；
  2021年5月，微软首席执行官萨蒂亚·纳德拉表示公司正在努力打造一个“企业元宇宙”；
  2021年8月，英伟达宣布推出全球首个为元宇宙建立提供基础的模拟和协作平台；
  2021年8月，字节跳动斥巨资收购VR创业公司Pico；
  2021年10月28日，美国社交媒体巨头脸书（Facebook）宣布更名为“元”（Meta），来源于“元宇宙”（Metaverse）；</description>
    </item>
    
    <item>
      <title>Python入门之书上没有的东西</title>
      <link>/ops/python%E5%85%A5%E9%97%A8%E4%B9%8B%E4%B9%A6%E4%B8%8A%E6%B2%A1%E6%9C%89%E7%9A%84%E4%B8%9C%E8%A5%BF/</link>
      <pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/ops/python%E5%85%A5%E9%97%A8%E4%B9%8B%E4%B9%A6%E4%B8%8A%E6%B2%A1%E6%9C%89%E7%9A%84%E4%B8%9C%E8%A5%BF/</guid>
      <description>分享分  为Python零基础同学做了一场入门分享 准备了1个小时的ppt 现场因为有很多演示，共花了90分钟 还好今天没人过来抢会议室。  ppt 说重点：  三天打鱼两天晒网的学习Python是不现实的，每天抽小半个小时敲代码，坚持下去，一两个星期就能入门。 最先要学的不是基础知识，而是环境配置和工具的选择 基础知识不用啃得太死，差不多理解了有印象就行，后面随着练习会不断强化，自然就学会了 练习为王，千万不要死看书，不动手。 多写，多练习，遇到报错顺着问题解决，一开始有卡住的找朋友同事帮忙，后面要学着自己搜索 多利用搜索引擎，随着技能的提升，会不断的有新问题出现。 要针对一个具体的目标和小型项目来进行练习。  </description>
    </item>
    
    <item>
      <title>万物不如MySQL_万物皆可Join</title>
      <link>/dba/%E4%B8%87%E7%89%A9%E7%9A%86%E5%8F%AFsql/</link>
      <pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E4%B8%87%E7%89%A9%E7%9A%86%E5%8F%AFsql/</guid>
      <description>当前数据库架构越来越复杂
  数据库MongoDB,Redis,Es,Kafka,Postgresql&amp;hellip;
  加上传统的关系型数据库（MySQL,Oracle,SQLServer)
  你是否因为各种数据库的查询语言不同而头晕眼花，到处撞墙！
   你是否各种分库分表后，不同的数据库之前没办法join联合查询而一蹶不振   你是否业务同学发给你一个excel，让你查这些订单的明细而不知所措，来回倒腾。   你是否在焦急的等待着BI大数据同事帮你把不同数据源的表都抽到一起才能join出想要的数据？   怎么办？怎么办？ 没办法!!! 拆开的数据库没办法放在一台服务器上 各种数据库也没办法统一成一种 大数据部门的同步任务正在走流程 走完的流程，他们也不能保证数据同步任务不中断 Excel不是数据库不能用SQL 怎么办？怎么办？ 这种混乱就没人能治吗？ 不要让这些问题挡住你前进的脚本 dboop平台的统一查询平台横空出世 不再区分数据库类型 所有的数据库种类都支持MySQL语法 是的，你没有听错 不管什么类型的数据库 统统只需要记住MySQL语法了 Oracle,SQLServer,MongoDB,kafka DBA在运维的每一种数据库 都可以当成MySQL一样使用了  kafka当成MySQL Mongo当成MySQL  而且这些表都是可以互相join ,union 的   现在我们来休验一下这神奇的黑科技 第一步 我们有个这样的excel 第二步 把excel上传到平台上 第三步 得到一个可以查询的excel文件 第四步 用excel join MySQL 这就是我们说的： 几个问题 问题1:查询会不会影响线上业务  绑定了dba的高可用架构系统，可以自动路由到专门给bi取数服务的专用只读实例上。不会对线上应用产生影响 理论上bi抽数进程会和它产生资源抢占，但是因为bi抽数多数是凌晨进行，两个并不冲突
 问题2:查询的性能怎样  快，非常快，普通的单表查询0.</description>
    </item>
    
    <item>
      <title>说能源</title>
      <link>/book/%E8%AF%B4%E8%83%BD%E6%BA%90/</link>
      <pubDate>Wed, 10 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%AF%B4%E8%83%BD%E6%BA%90/</guid>
      <description>山顶的人工水库  昨天在群里讨论股票 讨论化工能源的前景时 说起周末去北京郊区爬山 山顶上有个巨大的人工水库 这个水库的作用是 用电低峰时把水从山下抽到山顶上来 等想发电的时候 就放水发电 在感叹电力储能设备如此窘迫的时候 几个同学都不信有这么傻的事情 从经济价值/能量损失/投资收益 都觉得不可思议 是王多鱼的傻项目 要不是亲眼看见 也会怀疑这种储能方案的可行性 初见时也是觉得这种方案有点傻 后来搜了一下相关资料 这个抽水蓄能的方案 尽然是当前电力储能的最优解 无论是储能效率 还是绿色环保尽然还都占了不少优势 大毛和二毛的战争还在打得要死要活的 欧洲的能源依赖于俄罗斯 所以制裁的时候反而会伤到自身的能源安全 同时全世界的能源价格都在爆涨 所以，来说说能源吧  战争的尽头是能源  远古时候人类的战争：食物 再后来战争为了：土地 而现代战争就成了：能源 有了能源人类可以生存 有了充足的能源可以生存得很好 有了无尽的能源 人类可以毁天灭地 可以削山填海无所不能 可以在室内种大麻 可以填海造田 可以在沙漠里人造绿洲 如果有无尽的能源 人类可以把海水都给冻成冰 也可以把南极变成大火炉 可以去想去的任何地方 以前学政治课本的时候 里面描绘的物质极大丰富的主义 只要有无尽的随时可以取用的能源  能源的尽头是电力  可以用的能源可以有煤，石油，天然气 也可以是不太能直接用的太阳，风，势能 就像所有的程序需要统一接口一样 未来的能源会越来越趋像于统一的接口 那就是电 第二次工业革命是电力 人类开始使用电 第三次工业革命是计算机技术 人类更加依赖电 迟迟没有到来的第四次大变革 觉得可能的方向有 1.核能技术的快速成熟和民用 2.生物技术的快速发展（人类生命大幅延长，智力开发更进一步） 3.储能设备的革命 4.人工智能的革命性突破（是真的代替超越人类，不是现在这种半吊子） 1,3是人类的电力供给提升 2,4是人类对电力需求增大 还是会围绕着能源来进行 而且会越来越标准 也就是人类对烧煤，烧气的需求会越来越少 而对电的依融会越来越大 而事实上现有的油气煤 除掉油用来做化工提炼外 主要的作用也是用来燃烧发热 很大的比例已经用来发电了 油气煤最终会变成电为人类服务 之所以我们还在用煤/用气 是因为转化成本和储存成本还过高  电力的难题是储能  电和石油，天然气不一样 它生产出来很快就得用掉 暂时没有好的储存方式 这也就导致用电高峰时缺电 用电低峰时电力过剩 前些年去草原自驾游 看到到处都有巨大的风电在转 和当地人聊天说这些电会统一到电站调好了再并网 风电和现在正火的光伏电站 都会有个问题 不稳定 风的大小，太阳光的大小 都是不可控因素 而需求是相对稳定的 但是供给是非常不稳定的 其实水电也有这种情况，只是好控制一些 当然现在发电的主力还是烧煤 所以这些绿色发电的不稳定还不明显 如果未来这些绿色能源进一步提升 那电力储能问题会更加明显 说起能源安全的时候 会发现 如果电能可以低成本的长期保存 那么人类的能源会缓解很多 可以在能源充足的时候（水/风/太阳） 充分发电储存 而可利用的途径其实非常非常多 只是这些电的稳定性不高 所以不好利用 从早些年的铅酸电池 到现在的锂电 再到电动车，电动汽车的普及 只是人类储电的一点小进步 就可以让街上跑了这么多绿牌车 如果储能技术有突破的话 未来的能源结构会发生更明显的调整 人类的能源开采能力会提升（开采出来直接存储） 人类的能源利用率会提升（低峰的电力可以储存） 人类的能源焦虑会减少  图1:电力储能市场装机结构 2021 年，我国已投运电力储能项目累计装机 4610 万千瓦，占全球市场总规模的 22%，同比增长 30%。</description>
    </item>
    
    <item>
      <title>定时收集存储过程函数视图信息入库(Oracle,MySQL)</title>
      <link>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%87%BD%E6%95%B0%E8%A7%86%E5%9B%BE%E4%BF%A1%E6%81%AForaclemysql/</link>
      <pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%87%BD%E6%95%B0%E8%A7%86%E5%9B%BE%E4%BF%A1%E6%81%AForaclemysql/</guid>
      <description>需求 下午接到运维转来的一个权限申请流程：大数据部门研发要求开通保垒机权限。以方便在保垒机上安装SQL客户端去查看存储过程和视图的内容
保垒机直连数据库查询，这种不可控的方式，早在去年我就把这个历史问题给禁止掉了，没想到现在还有人要求开通
经过沟通得知，对方想查看Oracle数据库里的一些老的存储过程的代码。而DBA平台上只有表结构相关的数据字典，没有存储过程和视图的数据字典
所以，别慌，不就这点需求吗，马上就可以加上。
 为什么DBA平台上的数据字典不包括存储过程和视图？ 因为存储过程/函数/视图 也是被我禁掉的，研发人员上线不可以写存储过程和视图。 所以就没想过要在DBA运维平台上做这块功能
 但是因为
 历史原因，以前的Oracle数据库上已经存在很多的视图和存储过程 第三方原因，公司采购的一些第三方服务和软件，带了存储过程和视图 这些被禁止使用的数据库对象，也需要做统一维护  拆解  这些数据库对象的信息用定时任务收集线上的表结构到本地，存为两份 一份入库，做为快照信息，展示给用户。 一份落本地文件，上传到git，用git做版本管理   为什么不在用户请求查看某个数据库对象的信息时，实时查询给用户？
 1.因为历史原因，我们有的库有几万个数据库对象，当用户选择一个库时，list列表加载很慢，所以一开始设计的时候，我们做了快照 2.一份快照，还可以用作数据库对象的git版本管理   建表 在dboop库中建表
 CREATE TABLE `info_objects` ( `objectid` int NOT NULL AUTO_INCREMENT, `dbid` int NOT NULL DEFAULT &#39;0&#39;, `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT &#39;&#39;, `object_name` varchar(255) NOT NULL DEFAULT &#39;&#39;, `object_type` varchar(64) NOT NULL DEFAULT &#39;&#39;, `object_text` longtext, `cstatus` smallint NOT NULL DEFAULT &#39;1&#39;, `dba_freshtime` datetime NOT NULL DEFAULT &#39;1990-01-01 00:00:00&#39;, PRIMARY KEY (`objectid`), UNIQUE KEY `idx_infoobjects_id` (`dbid`,`object_name`,`object_type`), KEY `idx_info_objects_time` (`dba_freshtime`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 实现数据字典收集入库 建Oracle采集任务  &amp;lt;!</description>
    </item>
    
    <item>
      <title>MySQL的事务隔离和MVCC</title>
      <link>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8Cmvcc/</link>
      <pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8Cmvcc/</guid>
      <description>0.前言:为什么要写这篇文章？ 事务隔离和mvcc的重要性 不同于很多MySQL的原理，只需要DBA掌握，事务对于研发人员也是必须掌握的知识点和原理。并发程度越高，数据库里的锁和事务越明显，越重要。所以：数据库事务和mvcc是研发和DBA都要熟练掌握的另一方面的原因是现有的资料对mvcc写得不够直观 现有的对mvcc原理的讲解停留在画图阶段，我觉得光画图还不够，要实打实的一个字节一个字节的看MySQL真实的数据文件是怎么实现的。利用自研的MySQL数据文件分析工具（ 参考：innodb存储格式 )。可以很直观的把mvcc实现的底层逻辑给展示出来。
 以下两篇文章，可以协助你更好的理解本章节的内容
  MySQL行格式(compact,redundant,dynamic,compressed) ) MySQ事务id:trx_id )  环境准备  MySQL版本:8.0.22 事务隔离级别:REPEATABLE-READ (默认隔离级别)  建一张表dboopuser并insert几条数据 drop table dboopuser; create table dboopuser( userid int unsigned not null primary key , age smallint unsigned not null default 0, username varchar(20) not null default &#39;&#39;, userimg varchar(255) not null default &#39;&#39; ) ENGINE=InnoDB COMMENT=&#39;测试user表--用于mvcc测试20220727&#39; ; insert into dboopuser(userid,age,username,userimg) values(9527,25,&#39;cccccccccc&#39;,&#39;http://www.dboop.com/img/user/2002_innodbtrx_527.jpg&#39;); insert into dboopuser(userid,age,username,userimg) values(9528,15,&#39;dddddddddddddd&#39;,&#39;http://www.dboop.com/img/user/2002_innodbtrx_528.jpg&#39;); insert into dboopuser(userid,age,username,userimg) values(9529,25,&#39;eeeeeeeeeeeeeeeee&#39;,&#39;http://www.</description>
    </item>
    
    <item>
      <title>google和twitter的镜像</title>
      <link>/book/google%E5%92%8Ctwitter%E7%9A%84%E9%95%9C%E5%83%8F/</link>
      <pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/google%E5%92%8Ctwitter%E7%9A%84%E9%95%9C%E5%83%8F/</guid>
      <description>今天在大学的群里，F同学发了一个截图说王思聪在说
我说这是假消息，因为王思聪的推特账号不是这个sicongwang001名字，然后抓了张图
对***上网这件事也是有很多人不太擅长，于是做了两个镜像，提供给大家一个随时随地可以访问外网的跳转。
镜像站如下：
 Google（用来查资料): 地址就不公开了，有风险，有需要的单独找我吧 Twitter (用来看新闻): 地址就不公开了，有风险，有需要的单独找我吧  enjoy it</description>
    </item>
    
    <item>
      <title>show engine innodb status 工具化实现</title>
      <link>/mysql/mysql%E7%9A%84showinnodbstatus/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84showinnodbstatus/</guid>
      <description>为什么要写这个工具 当MySQL出现性能问题时，DBA经常得去innodb status ，
但是innodb status的输出信息非常丰富也很复杂。滚了几个屏幕的指标，像这样的得翻好几页，几百行的结果。
 哪些是重要的指标 指标具体代表什么意思 指标的值是否正常  非常考验DBA的眼力。
考虑到以上的不方便，写了个小脚本把这些指标提取出来，并将指标对应的中文意思和合理取值范围做了详细的备注。
实现思路  当用户选中MySQL实例，并点击show engine innnodb statutus按钮时 后台进程去该实例执行 show engine innnodb statutus 语句 返回结果做正则筛选，将各种指标和值保存在一个字典中 提前准备好一个指标的字典，字典里记了该值的中文说明和取值范围（取值范围这个还没做好） 两个字典一合并，输出一个分好类的可视化结果  指标提取和定义 脚本内容是定义了一个数据字典去翻译这些指标
{ &amp;quot;background_thread&amp;quot;:(&amp;quot;后台进程:除掉用户请求的活动会话，MySQL后台进程也会定时的进行一系列工作。&amp;quot;,[(&amp;quot;master_thread_loops_active&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;后台master线程avtive执行次数合计值&amp;lt;/b&amp;gt;,后台master线程的每次循环时会选择一种状态来执行(active、shutdown、idle),active次数/idle次数 比值越高，代表系统的写操作越繁忙。&amp;quot;), (&amp;quot;master_thread_loops_idle&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;后台master线程idle执行次数合计值&amp;lt;/b&amp;gt;,和上一个指标连起来看,idle次数越高，代表系统的写操作越少。所以该指标值越大，系统写资源越充足&amp;quot;), (&amp;quot;master_thread_log_flush_and_writes&amp;quot;,&amp;quot;Bytes&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;后台master线程刷新redo日志&amp;lt;/b&amp;gt;,定期刷新redo日志，和参数innodb_flush_log_at_timeout控制刷新时间&amp;quot;) ] ) ,&amp;quot;bufferpool_memory&amp;quot;:(&amp;quot;缓冲池:有关已读和已写页面的统计信息。可以从这些数字中获得缓冲池的使用情况。&amp;quot;,[ (&amp;quot;total_large_memory_allocated&amp;quot;,&amp;quot;Bytes&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;分配给InnoDB Buffer Pool的总内存&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;dictionary_memory_allocated&amp;quot;,&amp;quot;Bytes&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;分配给InnoDB数据字典的内存&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;buffer_pool_size&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;分配给IBP的内存，单位pages&amp;lt;/b&amp;gt;,每页16k&amp;quot;) ,(&amp;quot;buffer_pool_hit&amp;quot;,&amp;quot;/1000&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool命中率&amp;lt;/b&amp;gt;每1000次请求有*次命中buffer pool,非常重要&amp;quot;) ,(&amp;quot;free_buffers&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool Free List 总大小，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;database_pages&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool LRU List 总大小，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;old_database_pages&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool old LRU 总大小，单位pages(冷端)&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;modified_db_pages&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool中脏页的数量，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pending_reads&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;等待读入Buffer Pool的页数量，单位pages&amp;lt;/b&amp;gt;,理论上不应该有等待队列&amp;quot;) ,(&amp;quot;pending_writes_lru&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;LRU中buffer中等待被刷的脏页数，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pending_writes_flush_list&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在checkpoint期间要刷新的缓冲池页数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pending_writes_single_page&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在缓冲池中写入挂起的独立页的数目&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_made_young&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;热点页数&amp;lt;/b&amp;gt;,在缓冲池LRU list中年轻的总页数(移动新页面到sublist的头部)&amp;quot;) ,(&amp;quot;pages_made_not_young&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;old sublist中的page数，冷端的page数&amp;lt;/b&amp;gt;,在缓冲池LRU列表中不年轻的页面总数(保留旧页面在sublist中，不改变)&amp;quot;) ,(&amp;quot;pages_made_young_per_sec&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒LRU链中被young的page数&amp;lt;/b&amp;gt;,oungs/s度量标准仅用于old pages，基于对page的访问次数，而不是页的数量。对页进行多次访问都会被计算。如果见到非常低的值，可能需要减小延迟或增加old page LRU list 的比例。增大后，页面需要更长的时间才会移动到尾部，这就增加了再次访问page，从而使他们made young的可能性增大&amp;quot;) ,(&amp;quot;pages_made_non_young_per_sec&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒LRU链中未被young的page数&amp;lt;/b&amp;gt;，可以一定程度上看出库的繁忙程度和命中率,Not young，如果在执行大表扫描时未看到较高的non-young和non-youngs/s，请增加innodb_old_blocks_time。&amp;quot;) ,(&amp;quot;pages_read&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool中读取的page总数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_created&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在bufferpool中创建的page数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_written&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool写入的page数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_read_per_sec&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool中读取的page数/秒&amp;lt;/b&amp;gt;, 比较重要，代表库的繁忙程度&amp;quot;) ,(&amp;quot;pages_created_per_sec&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在bufferpool中创建的page数/秒&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_written_per_sec&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool写入的page数/秒&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_read_ahead&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒平均预读操作次数&amp;lt;/b&amp;gt;k&amp;quot;) ,(&amp;quot;evicted_without_access&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒驱逐的page数&amp;lt;/b&amp;gt;k&amp;quot;) ,(&amp;quot;random_read_ahead&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒钟随机预读操作的次数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;lrn_len&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;LRU的长度&amp;lt;/b&amp;gt;&amp;quot;) ] ) .</description>
    </item>
    
    <item>
      <title>MySQL复制参数_slave_rows_search_algorithms及无主键表的处理</title>
      <link>/mysql/mysql%E5%A4%8D%E5%88%B6%E5%8F%82%E6%95%B0slave_rows_search_algorithms/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%A4%8D%E5%88%B6%E5%8F%82%E6%95%B0slave_rows_search_algorithms/</guid>
      <description>0.起因 线上MySQL实例，今天报大量延时，且卡住不动。（表现为seconds_behind_master不断上涨，从库gtid不动）
同时二级从库有复制SQL进程报错
[ERROR] [MY-010584] [Repl] Slave SQL for channel &#39;&#39;: Could not execute Update_rows event on table 「表名」; Can&#39;t find record in &#39;「表名」&#39;, Error_code: 1032; handler error HA_ERR_END_OF_FILE; the event&#39;s master log mysql-bin.000****, end_log_pos *******, Error_code: MY-001032 1.排查问题 排查问题时
 确认该实例上的从库不提供线上实时业务访问（业务可以接受延时）。不需要做从库切流量动作 先是看了一下从库的多线程复制是database级的，开启多线程复制到logical_clock ,问题并没有恢复       set global slave_parallel_type=&amp;lsquo;logical_clock&amp;rsquo;; set global slave_parallel_workers=4; start slave sql_thread;```
 排除掉是线程数不够的原因 发现processlist中是在等Applying batch of row changes (update)  确定是卡在sql进程，再看relaylog确实持续增长800M(表示该实例写入不频繁)   解析relaylog 发现是普通的update语句大约有8000次左右  这个量级的update且是row模式，理论1分钟内就追上了。   查看锁datalocks，发现有大量的行数50几万，都是同一个表的  slave的sql进程不应该有这个量级的行锁。   查看表结构发现这个表是无主键的表，里面大约有50几万条记录，无主键无索引  2.</description>
    </item>
    
    <item>
      <title>图数据库nebula源码编译安装</title>
      <link>/dba/nebula%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/</link>
      <pubDate>Wed, 29 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/nebula%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/</guid>
      <description>环境准备 当前机器环境centos7
硬件要求    类型 要求     CPU 架构	x86_64   内存 4 GB   硬盘 10 GB，SSD    环境依赖（针对centos7)    软件名称 版本 备注     glibc 2.17 及以上 执行命令ldd &amp;ndash;version检查版本。   g++ 8.5.0 及以上 执行命令g++ -v检查版本。   cmake 3.14.0 及以上 执行命令cmake &amp;ndash;version检查版本。    yum安装准备 cd /etc/yum.repos.d/mv CentOS-Base.repo CentOS-Base.repo.backwget -O CentOS-Base.repo http://mirrors.</description>
    </item>
    
    <item>
      <title>图数据库nebula性能监控</title>
      <link>/dba/nebula%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7/</link>
      <pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/nebula%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7/</guid>
      <description>目的 nebulaGraph官方自带的Nebula Dashboard 监控工具里的监控指标和筛选粒度已经足够丰富。但基于以下原因，还是在此基础上自己做了层监控
 缺少关键指标的定义，对除DBA以外的用户不友好。我们希望把性能数据让研发同学也能看到 缺少一个对所有服务器的横向对比和集中展示Dashboard 与我们现有的DBA监控不在一个平台，需要多平台切换使用 没有性能指标评分，同样的指标，没有给出是否合格的标准和提示 官方已经提供了可以直接读取性能数据的接口 http://%s/stats，在些基础上二次开发监控的难度会很低，预估开发工作量1pd ,实际开发工作量1.5pd  1.指标筛选 官方提供的性能接口里有几百项指标，我们从graph,storage,rockdb 三个层面，筛选了60几个重点的，需要关注的指标，筛选的依据是
 是否能从指标定位到资源或性能问题（响应时间等） 是否在排查问题出现时，有助于定位异常（命令次数，qps等) 是否有类似的指标已经存在。  最终，挑选了以下指标 2.编写收集代码（Python） 这部分因为有官方接口，所以直接请求就可以了
for nodeid,nodehost in grpahlist: dict_result=self.get_nebula_stats(nodehost) #入库 dict_result def get_nebula_stats(self,nodehost) dict_result={} urlstr=&amp;#39;http://%s/stats?format=json&amp;#39;%(nodehost,) mlist=self.get_urldata(urlstr) for dictc in mlist: for k,v in dictc.items(): if k in dict_graph: keyname=dict_graph[k][0] dict_result[keyname]=v return dict_result 3.性能评价 这是非常重要的一步，沿用我在2005年做的数据库性能模型的方法（参考：https://github.com/51ak/DatabaseRating/）
 对我们的每一项指标，我们需要对其取值范围进行判断，给其打分：优，良，中，劣。 标记为:weight 对每一项指标，对其权重做标记（0-5）,标记为：height 由weight和height计算出这个实例的健康程度 但是我们的nebula服务只有一个集群，不需要太细化，所以我们只做了weight标记  做weight标记的逻辑是定义如下的一个区间列表
dict_graph={ &amp;#34;num_active_queries.sum.60&amp;#34;:(&amp;#34;num_active_queries&amp;#34;,(-4,5,10,50,1000000)), &amp;#34;num_active_sessions.sum.60&amp;#34;:(&amp;#34;num_active_sessions&amp;#34;,(-4,5,10,50,1000000)), &amp;#34;num_opened_sessions.rate.60&amp;#34;:(&amp;#34;num_opened_sessions&amp;#34;,(-4,100,500,10000,10000000)), &amp;#34;num_queries.rate.60&amp;#34;:(&amp;#34;num_queries_rate&amp;#34;,(-4,100,500,10000,10000000)), &amp;#34;num_queries.sum.60&amp;#34;:(&amp;#34;num_queries_sum&amp;#34;,(-4,5000,50000,900000,100000000)), &amp;#34;num_sentences.rate.60&amp;#34;:(&amp;#34;num_sentences_rate&amp;#34;,(-4,100,500,10000,10000000)), &amp;#34;num_sentences.sum.60&amp;#34;:(&amp;#34;num_sentences_sum&amp;#34;,(-4,5000,50000,900000,100000000)), &amp;#34;query_latency_us.</description>
    </item>
    
    <item>
      <title>图数据库nebula实时慢日志收集和展示</title>
      <link>/dba/nebula%E6%85%A2%E6%9F%A5%E8%AF%A2%E7%9B%91%E6%8E%A7/</link>
      <pubDate>Fri, 17 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/nebula%E6%85%A2%E6%9F%A5%E8%AF%A2%E7%9B%91%E6%8E%A7/</guid>
      <description>目的 因为我们的图数据库从neo4j社区版转到nebula graph方向。最近在项目压测的时候，一开始很平稳，运行一段时间后，NQL会越来越慢，发现性能巨差。nebula经常卡死，表现为：
 nebula-storaged.service和nebula-graphd.service服务经常挂掉。 业务侧反馈执行什么NQL都很慢 nebula show query里发现有大量（300-500个）running的进程。平时很快的NQL也卡在那  系统资源
 内存紧张 ，大量的虚拟内存被占用 io,cpu也较高  在做了一些参数调优后，发现状态有所缓解，但是一段时间后，还是会慢慢卡死，从现象上推测是有一种或几种慢NQL多次执行后，把系统资源消耗完了，导致大面积的堵塞。所以面临的需求还是要有个慢查询排查工具。
参考上一次做oracle慢日志收集展示的方法， https://www.dboop.com/oracle/oracle%E6%80%8E%E6%A0%B7%E5%AE%9E%E6%97%B6%E6%94%B6%E9%9B%86%E5%B1%95%E7%A4%BA%E6%85%A2%E6%9F%A5%E8%AF%A2/
变化的是：
 因为nebula的集群就一个，不需要做oracle慢日志的大表套小表，数担据量不大就建了一张monitor_nebula_slow表存放数据. nebula抓到的慢查询里的NQL是没有去参数化的，需要自己做去参数化，把相同类型的NQL，标识为同一个md5id   1.建一张表，每隔1分钟（时间可调，但我们的场景1分钟足够了） CREATE TABLE `monitor_nebula_slow` (`logid` int unsigned NOT NULL AUTO_INCREMENT,`SessionID` varchar(50) NOT NULL DEFAULT &#39;&#39;,`ExecutionPlanID` varchar(50) NOT NULL DEFAULT &#39;&#39;,`User` varchar(50) NOT NULL DEFAULT &#39;&#39;,`Host` varchar(50) NOT NULL DEFAULT &#39;&#39;,`StartTime` datetime DEFAULT NULL,`DurationInUSec` int unsigned NOT NULL DEFAULT &#39;0&#39;,`Status` varchar(50) NOT NULL DEFAULT &#39;&#39;,`Query` varchar(5000) NOT NULL DEFAULT &#39;&#39;,`_timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,`md5id` varchar(64) NOT NULL DEFAULT &#39;&#39;,PRIMARY KEY (`logid`),KEY `ix_monitor_nebula_slow` (`_timestamp`)) ENGINE=InnoDB AUTO_INCREMENT=13231 DEFAULT CHARSET=utf8mb32.</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(三):UndoLog</title>
      <link>/mysql/mysql%E7%9A%84undo%E6%97%A5%E5%BF%97/</link>
      <pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84undo%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续：
 MySQL的7种日志(一):概况
  MySQL的7种日志(二):RedoLog
 1.什么是undolog  undo：撤销或取消，以撤销/回滚操作为目的，返回指定某个状态的操作。 undolog：数据库事务开始之前，会将要修改的记录存放到 Undo 日志里，当事务回滚时或者数据库崩溃时，可以利用Undo日志，撤销未提交事务对数据库产生的影响。 undolog在事务开始前产生；事务在提交时，并不会立刻删除undolog，innodb会将该事务对应的 undo log 放入到删除列表中，后面会通过后台线程purge thread进行回收处理。 undolog属于逻辑日志，记录一个变化过程。例如执行一个delete，undo log会记录一个insert；执行一个update，undo log会记录一个相反的update。  2.undolog的作用  实现事务的原子性 当事务回滚时或者数据库崩溃时，利用Undo日志，撤销未提交事务对数据库产生的影响。事务处理过程中，如果出现了错误或者用户执行了 ROLLBACK 语句，MySQL 可以利用 Undo Log 中的备份将数据恢复到事务开始之前的状态。 实现多版本并发控制（MVCC） Undo Log 在 MySQL InnoDB 存储引擎中用来实现多版本并发控制。事务未提交之前，Undo Log 保存了未提交之前的版本数据，Undo Log 中的数据可作为数据旧版本快照 供其他并发事务进行快照读。（构建read view视图）  3.undolog的存储 3.1 物理存储位置 找到具体存放的位置 MySQL5.6.3 之前的版本undolog存储在系统共享表空间里，后续的版本推荐存话在单独的文件中
mysql&amp;gt; show global variables like &#39;%undo%&#39;; +--------------------------+-------------------------+ | Variable_name | Value | +--------------------------+-------------------------+ | innodb_max_undo_log_size | 1073741824 | | innodb_undo_directory | /data/mysql3306/innolog | | innodb_undo_log_encrypt | OFF | | innodb_undo_log_truncate | ON | | innodb_undo_tablespaces | 2 | +--------------------------+-------------------------+ 5 rows in set (0.</description>
    </item>
    
    <item>
      <title>redis高可用模式双比</title>
      <link>/dba/redis%E7%9A%84%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB/</link>
      <pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/redis%E7%9A%84%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB/</guid>
      <description>redis高可用模式 常见的高可用模式对比：
主从复制时代 主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(Master)，后者称为从节点(Slave)；数据的复制是单向的，只能由主节点到从节点。
主从复制的优点 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。
主从复制的工作原理  1）若启动一个Slave机器进程，则它会向Master机器发送一个“sync command”命令，请求同步连接。 2）无论是第一次连接还是重新连接，Master机器都会启动一个后台进程，将数据快照保存到数据文件中（执行rdb操作），同时Master还会记录修改数据的所有命令并缓存在数据文件中。 3）后台进程完成缓存操作之后，Maste机器就会向Slave机器发送数据文件，Slave端机器将数据文件保存到硬盘上，然后将其加载到内存中，接着Master机器就会将修改数据的所有操作一并发送给Slave端机器。- Slave出现故障导致宕机，则恢复正常后会自动重新连接。 4）Master机器收到Slave端机器的连接后，将其完整的数据文件发送给Slave端机器，如果Mater同时收到多个Slave发来的同步请求，则Master会在后台启动一个进程以保存数据文件，然后将其发送给所有的Slave端机器，确保所有的Slave端机器都正常。  Sentinel哨兵时代 哨兵(sentinel)：是一个分布式系统，用于对主从结构中的每台服务器进行监控，当出现故障时通过投票机制选择新的 Master 并将所有 Slave 连接到新的 Master。所以整个运行哨兵的集群的数量不得少于3个节点。
从Redis2.8版本起，提供了一个稳定版本的Sentinel哨兵来解决高可用的问题，它的思路是启动奇数个Sentinel的服务来监控Redis服务器来保证服务的可用性。 为了保证监控服务器的可用性，我们会对Sentinel做集群部署，Sentinel既监控所有的Redis服务，Sentinel之间也相互监控。 Sentinel本身没有主从之分，地位是平等的，只有Redis服务节点有主从之分。 Sentinel通过Raft共识算法，实现Sentinel选举，选举出一个leader，由leader完成故障转移。
Raft共识算法： https://www.dboop.com/dba/raft%E5%8D%8F%E8%AE%AE/ 哨兵的核心功能：在主从复制的基础上，哨兵引入了主节点的自动故障转移。
Sentinel的作用  监控：哨兵会不断地检查主节点和从节点是否运作正常。 自动故障转移：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。 通知（提醒）：哨兵可以将故障转移的结果发送给客户端。  Sentinel的结构 哨兵结构由两部分组成，哨兵节点和数据节点。
 哨兵节点：哨兵系统由一个或多个哨兵节点组成，哨兵节点是特殊的redis节点，不存储数据。 数据节点：主节点和从节点都是数据节点。  Redis Cluster 时代 redis集群是在redis 3.0版本推出的一个功能，其有效的解决了redis在分布式方面的需求。当遇到单机内存，并发和流量瓶颈等问题时，可采用Cluster方案达到负载均衡的目的。并且从另一方面讲，redis中sentinel有效的解决了故障转移的问题，也解决了主节点下线客户端无法识别新的可用节点的问题，但是如果是从节点下线了，sentinel是不会对其进行故障转移的，并且连接从节点的客户端也无法获取到新的可用从节点，而这些问题在Cluster中都得到了有效的解决。
key与slot的关系是永远不会变的，会变的只有slot和Redis节点的关系。 如果想让很多个key同时落在同一个节点怎么办呢，只需要在key里面加入{hash tag}即可。 Redis在计算槽编号的时候只会获取{}之间的字符串进行槽编号计算，如下所示：
user{666}base=&amp;hellip; user{666}fin=&amp;hellip;
Redis-Cluster 特点  无中心结构。 数据按照slot存储分布在多个节点，节点间数据共享，可动态调整数据分布。 可扩展性，可线性扩展到1000个节点（官网推荐不超过1000个），节点可动态添加或删除。 高可用性，部分节点不可用时，集群仍可用。通过增加Slave做standby数据副本，能够实现故障自动failover，节点之间通过gossip协议交换状态信息，用投票机制完成Slave到Master的角色提升。 降低运维成本，提高系统的扩展性和可用性。  Redis Cluster的作用  （1）数据分区 数据分区(或称数据分片)是集群最核心的功能。  集群将数据分散到多个节点，一方面突破了Redis单机内存大小的限制，存储容量大大增加；另一方面每个主节点都可以对外提供读服务和写服务，大大提高了集群的响应能力。</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(二):RedoLog</title>
      <link>/mysql/mysql%E7%9A%84redo%E6%97%A5%E5%BF%97/</link>
      <pubDate>Fri, 13 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84redo%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续上一篇： MySQL的7种日志(一):概况
上一篇我准备写MySQL日志还是2个月前的事，这两个月里生活发生了天翻地覆的变化，都没时间更新。
昨天跟朋友聊天立了flag说今天要续写一篇
于是中午吃饭时在纸上画了一个流程图。来介绍下MySQL里的RedoLog
1.几个问题 redolog和binlog一样记录的是数据修改后的记录。区别是什么，存在的意义是什么？  如果不要redolog，直接修改数据行不行？ 答：可以，但是随机读写性能差 先写redolog还是先改数据？答：先写内存里的数据，再写redolog，再写binlog，再写磁盘里的数据 先写redolog还是先写binlog? 答：先写redolog,再写binlog 如果写完redolog，还没来得写binlog时就停电了,怎么办？答：修改不要了，从undolog中回滚数据 如果写完redolog,binlog时还没来得数据落盘就停电了,怎么办？答：重做redolog，提交数据。修改有效  redolog和undolog的关系  答：redolog用来恢复丢失数据（恢复到最后一次提交位置）也称之为前滚操作，undolog是用来回滚到之前的版本，称之为回滚操作  relaylog的作用  答：redolog是用来做崩溃恢复使用的，这种崩溃恢复不需要我们人为的参与，MySQL自己内部自己实现了这种崩溃恢复的功能，我们只管享受这种功能给我们带来的服务即可，这种服务给我们的感受就是：MySQL数据库异常宕机的时候，重启服务之后，数据库中之前提交的记录都不会丢失数据仍然可以正常恢复，不管这种提交的记录是否已经更到具体的表所对应的磁盘page也中。  2.修改数据的流程  当我们要更新一条数据时，比如有一条SQL update userinfo set name=&#39;dboop&#39; where name=&#39;张三&#39;; 最直接的方法：从磁盘上找到对应的数据库文件，把它修改完存放到磁盘中。  方法是可以的，很多简单的程序修改文件也是用的方法，但是性能差。   而数据库中一般会有以下几种方式来写入数据修改  按页组织数据，一些关联近的数据存放在一个页中，MySQL中默认一页是16k 读取和修改数据都是需要先把页加载到内存中,MySQL是放到innodb_buffer_pool中 先改内存，再合适的时候再写入磁盘 先改日志再改数据 日志也是先写内存中的日志buffer，再合适的时候刷入磁盘    下图是简化版的一个数据修改，真实的流程比这复杂很多，这里的数据修改不只是update，按页组织的insert/update/delete操作都是对页修改
3.Redolog在数据库意外崩溃时的作用 当故障发生时，数据库意外当机，有部分内存中已修改的页（脏页）没来得及刷新到磁盘里。
在写入redo log时，会顺便记录XID，即当前事务id。在写入binlog时，也会写入XID。
如果在写入redo log之前崩溃，那么此时redo log与binlog中都没有，是一致的情况，崩溃也无所谓。
如果在写入redo log prepare阶段后立马崩溃，之后会在崩恢复时，由于redo log没有被标记为commit。于是拿着redo log中的XID去binlog中查找，此时肯定是找不到的，那么执行回滚操作。
如果在写入binlog后立马崩溃，在恢复时，由redo log中的XID可以找到对应的binlog，这个时候直接提交即可。
总的来说，在崩溃恢复后，只要redo log不是处于commit阶段，那么就拿着redo log中的XID去binlog中寻找，找得到就提交，否则就回滚。
在这样的机制下，两阶段提交能在崩溃恢复时，能够对提交中断的事务进行补偿，来确保redo log与binlog的数据一致性
4.Redolog的刷盘 4.</description>
    </item>
    
    <item>
      <title>钱也分很多种</title>
      <link>/book/%E9%92%B1%E4%B9%9F%E5%88%86%E5%BE%88%E5%A4%9A%E7%A7%8D/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E9%92%B1%E4%B9%9F%E5%88%86%E5%BE%88%E5%A4%9A%E7%A7%8D/</guid>
      <description>选中上下文，就知道是什么钱了。
中国 . 元 中国澳门 . 元 中国台湾 . 元 中国香港 . 元 美国 . 美元 美国 . 美元_背面 英国 . 英镑 韩国 . 圆 日本 . 元 德国 . 马克 俄罗斯 . 卢布 法国 . 法郎 欧洲 . 欧元 葡萄牙 . 埃斯库多 瑞士 . 法郎 荷兰 . 盾 西班牙 . 比塞塔 加拿大 . 元 意大利 . 里拉 印度 . 卢比 澳大利亚 . 元 新加坡 . 元 泰国 . 铢 越南 .</description>
    </item>
    
    <item>
      <title>我为什么要反对DBA参与业务(出报表/改数据)</title>
      <link>/dba/%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%AF%B9dba%E6%94%B9%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%AF%B9dba%E6%94%B9%E6%95%B0%E6%8D%AE/</guid>
      <description>上午有业务人员在钉钉上call我
 A：要修改库里的一批资产的属性 我：这事为啥找我? A：有同事说数据库的事情都要找DBA。 我：DBA不直接改数据,谁跟你这样说的，拉上他一起来说说。 A：不愿意透露这个人是谁，   首先表明态度：坚决反对DBA参与(出报表/改数据)等业务行为，DBA可以提供对应的平台和工具。
 以下行为
 DBA知道很复杂的业务逻辑，知道怎么把一个人客户的帐单做拆分（比业务和产品人员都知道的更清楚） 业务同学邮件发过来一批订单号，DBA同学可以帮他同时修改这些订单号关联的7，8张表的数据和状态（逻辑很复杂，但DBA知道怎么改得滴水不漏） 定期给业务人员出一些报表，这些报表是DBA用很复杂的SQL和各种临时表算出来的。逻辑很复杂，只有DBA能写出来 很多业务和产品上没来得及做的功能，业务会直接发工单给DBA，DBA写SQL帮业务临时完成 都是越界的，DBA不需要也不应该对业务“深入”的了解。  为什么要这样？ 从3个方面说：
 DBA职责 职业分工 数据安全性  一.DBA职责 DBA的首要任务是保护数据，维护高可用访问(而不是主动修改数据)
 制定并实施数据库安全规范/管理规范/访问规范等。 持续改进优化高可用架构（当发生硬件/网络/软件故障时，可以快速恢复） 建立自动监控系统,及时有效处理各种报警 建立自动检查，备份，HA，远程容灾和远程备份系统 发现协助业务优化数据库的库表结构，SQL写法等，共同提升数据库性能  二.职业分工 专业的人做专业的事
 出数据和导报表的工作，有大数据部门的同事，有数据分析师。 修改数据的工作，有各业务线的研发和产品人员，他们设计的系统，他们最理解怎么改，DBA可以提供- SQL上线平台，他们自助完成 如果DBA比大数据的同更懂得出报表，比业务线的研发人员更了解应该写SQL改数据是没有必要的， DBA可以分成:业务DBA和运维DBA,通常我们说的DB都是运维DBA,权限很大。如果有业务DBA会限制他的权限，不会有运维DBA这么大的权限。  三.数据安全性  DBA作为数据的最终维护人员，修改数据是最直接暴力的方式 普通员工没有直连数据库做操作的权限，只能通过上线代码和指定的平台(DBA提供的dboop平台)   DBA不参与业务(出报表/改数据),是否意味着DBA从工作中摘了出去? 也不是
 DBA提供数据查询平台,对短而小的数据查询导出需求,研发定位问题的查询，提供一个快速查询平台（有完整的权限管理，日志审计，行为约束，敏感数据脱敏） DBA提供便利的数据库SQL上线自助流程，方便研发人员快速上线SQL，有完整的日志，SQL审核,数据备份,权限控制,快速回滚。 DBA不需要了解表中的每个字段具体含义，每个表之间的关联，只需要知道这个库存放的数据是否重要，属于哪个业务线，负责人是谁，省下来的精力去做真正运维的工作  DBA有权限可以任意修改库里的数据，但是这个不合规，不应该这样做。正确的做法是提供一个平台，让真正了解业务的研发/产品/数据分析人员，去查询，去出报表，去改数据。</description>
    </item>
    
    <item>
      <title>数据库多环境SQL上线</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%9A%E7%8E%AF%E5%A2%83sql%E4%B8%8A%E7%BA%BF/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%9A%E7%8E%AF%E5%A2%83sql%E4%B8%8A%E7%BA%BF/</guid>
      <description>数据库多环境分类 MySQL数据库按环境区分有以下几种数据库  正式环境数据库：又称生产环境 验证环境数据库：上线验证环境，介于测试和生产之间的一个环境 测试环境数据库：又细分为   test0测试环境：默认测试环境 test1测试环境: test1测试环境简称t1环境，和test0环境是并行关系 test2,test3&amp;hellip; 未来会有更多的测试环境  开发环境：又称Dev环境，用于研发本地调试代码  Oralce测试环境参考MySQL的测试环境 数据库多环境上线的问题  SQL上线（建表/改表，改数据等） 需要在每个环境下都提个流程，研发的重复工作量大 研发可能会忘掉部分环境下执行某个SQL，造成各种环境和线上环境的差异越来越大  解决方式  提供一个多环境SQL上线流程，研发可以勾选每次上线要执行的环境 提供一个多环境数据库表结构对比工具 提供一个不同环境的数据库对象自动同步工具  多环境SQL上线流程 这里只演示MySQL多环境上线流程，Oracle的多环境上线流程和这个一模一样
步骤1.选择数据库 步骤2.填写上线内容  注意1：这里可以自由勾选要执行的数据库环境，红色的是线上环境，绿色的是测试和验证环境，蓝色的是开发环境 注意2：所有环境都会执行SQL验证，所有环境通过验证，才可以进入下一步  步骤3.流程审批  如果通过了系统的SQL审核，会出现如下图的界面，进入流程审批 3.负责人审批是必选步骤 4.测试审批是用户可选步骤 5.DBA审批会根据用户提供的SQL内容，系统判定是否有dba介入（建表/改表/大范围修改数据DBA会审批，否则会自动跳过这一步）  步骤4.用户执行  审批完成以后，由发起人自由选择在合适的时间，点击上线按扭   步骤5.执行结果  执行完成后，会显示每个环境的执行结果。   后台配置 这个流程也是在dboop平台上用xml配置的，后台界面如下</description>
    </item>
    
    <item>
      <title>温柔的话</title>
      <link>/book/%E6%B8%A9%E6%9F%94%E7%9A%84%E8%AF%9D/</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%B8%A9%E6%9F%94%E7%9A%84%E8%AF%9D/</guid>
      <description>如果你半夜醒了，就给我打电话，不要一个人玩手机。 没事，我不听他们说，我听你说。 我不会让你一个人安静的，你的小脑瓜是想不通的，我会哄你到服软，偏向你，让你心安。 不忙，没事，你说，我在 累了就向后倒，我在 其实，每次道别之后，我都有悄悄回头过。 别逞强，你是小朋友，扛不住可以哭。 你负责温柔，我负责守护你的温柔。 我爱你，你可以一遍又一遍地向我确认。 我们会吵架，但是我们会和好，我们会有误会，但是我们都会解释清楚，我们的爱只增不减。 你问我说爱有没有期限，山脚的野草一年又一年，我想陪你的不止是北方的冬天。 我不知道我什么时候开始喜欢你的，我只是记得见到你，我就突然不着急回家了。 你放心，我除了干饭就是想你。 回首亘年漫月里 所有怦然心动 你仍拨得头筹 我不是来讲道理的，我是来给我的小朋友撑腰的。 你什么都愿意和我讲，就是我最有安全感的时候。 如果你感到辛苦，那就转过来到我怀里躲一躲吧， 你在我这里永远有台阶下，永远有纸巾擦泪，永远有最真诚的鼓励和最柔软的怀抱。 你不是我权衡利弊后的选择，而是我怦然心动后，明知不可为而为之的坚定，这是我对这份感情最大的诚意。  这些都是网上看到的
这是我最近记下来的
我以前还记了很多笑话
放在一个文本里
想背下来散步的时候
说给她听
可惜记性太差
每次都在那想半天
挤不出来一个完整的笑话
而我这辈子说过最温柔的话是
不管你能不能理解我做的 也不去想你为什么要这样对我的 我依然会等你到... 就像年轻时候发生过的那样 你如果也还是没来 我就开始新的生活 那是我内心最柔软的时刻</description>
    </item>
    
    <item>
      <title>Sysbench做压力测试</title>
      <link>/ops/sysbench%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Sat, 12 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/ops/sysbench%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/</guid>
      <description>0.环境 1.安装sysbench  curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash yum -y install sysbench 2.执行压测 10.10.0.1 上执行 sysbench /usr/share/sysbench/oltp_read_write.lua --tables=5 --table_size=2000000 --mysql-user=dba --mysql-password******* --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-db=dbatest prepare sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=dba --mysql-password=****** --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-db=dbatest --tables=5 --table_size=2000000 --threads=300 --time=120 --report-interval=60 run &amp;gt;&amp;gt; /root/sb/proxy300.log sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=dba --mysql-password=****** --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-db=dbatest --tables=5 --table_size=2000000 --threads=20 --time=120 --report-interval=10 run sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=dba --mysql-password=****** --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-db=dbatest --tables=5 --table_size=2000000 --threads=20 --time=120 --report-interval=10 run sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=dba --mysql-password=****** --mysql-host=127.</description>
    </item>
    
    <item>
      <title>DBA的工作评价标准</title>
      <link>/dba/dba%E5%B7%A5%E4%BD%9C%E8%83%BD%E5%8A%9B/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/dba%E5%B7%A5%E4%BD%9C%E8%83%BD%E5%8A%9B/</guid>
      <description>每大项有4-5个评价标准，分别给分，卓越(4.5),优秀(4),一般(3.5),待改进(3),较差(2.5) 综合得分计为工作能力得分
 这里不区分级别，资深DBA,高级DBA,DBA 按同一标准打分。
01.责任感和主动性 1:设定工作目标  卓越:有高度的责任感，自己设立挑战目标，并实现目标；在工作责任和任务上挑战自己的极限，为公司创造最大价值 优秀:以极大的责任心去 设立非常挑战性目标；主动要求承担更多责任和工作 一般:给自己设立挑战性目标；严格要求自己；主动要求承担责任和工作 待改进:给自己设立容易达到的目标，不严格要求自己；不主动要求工作和责任 较差:不设立目标，不要求自己；推托工作和责任  2:工作效率效果  卓越:效率非常高，紧迫感非常强，今日事今日毕。全力以赴保证完成任务。树立标准，积极地影响他人 优秀:效率非常高，紧迫感非常强，今日事今日毕。全力以赴保证完成任务 一般:效率高，紧迫感强，在限定时间能完成任务 待改进:做事效率不高，紧迫感不足，有时不能在限定时间能完成，偶尔出错 较差:做事拖沓、无紧迫感、无责任心，不能在限定时间能完成工作，时常出错  3:承担责任  卓越:有自我批评，自我激励的能力；追求卓越，在责任感/主动性上是员工典范 优秀:勇于承认错误而不找借口，积极改正错误；正面积极地影响他人 一般:承认错误不找借口，积极改正错误 待改进:有时不承认错误，有时责怪别人 较差:不承认错误，抱怨和指责别人；态度、言行消极地影响他人  4:奉献精神  卓越:愿意为公司/团队利益而牺牲个人利益 优秀:在任何情况下，关注公司/团队利益多于个人利益 一般:在大多数情况下，关注公司/团队利益多于个人利益 待改进:能平衡个人和公司/团队利益 较差:关注个人利益多于公司和团队  02.解决问题能力 1:解决问题  卓越:洞察工作和流程中的潜在问题，预先就可能出现的问题提出解决方案，避免问题的出现 优秀:解决用户问题时经常能让用户非常满意 一般:解决用户问题时经常能让用户满意 待改进:解决用户问题时有时能让用户满意 较差:不能够解决用户问题，不能让用户满意  2:履行工作职责  卓越:超出职责范围的解决公司面临的困难，并超额完成任务 优秀:能够履行职责并超额完成任务 一般:能够履行职责并完成任务 待改进:有时能够履行职责并完成任务 较差:不能够履行职责并完成任务  3:处理复杂问题  卓越:综合运用逻辑和直觉以获得最好的解决办法；快速和正确地解决问题；认识到产生问题的根本原因,一劳永逸的从系统或流程或人员上彻底解决 优秀:通过分析各种复杂数据，找出关键问题，得出理性的结论并付诸于行动，取得预期目标 一般:能够分析和解决复杂问题；能就新出现的问题提出合理解决办法 待改进:面临复杂问题时所提出方案不是充分必要；但会处理比较简单的问题 较差:在面对复杂问题时需要帮助；做事无轻重缓急  4:态度信心和持续改进  卓越:是某一领域的专家，有必胜信心，永远追求卓越 优秀:把持续改进和提高作为工作和人生态度；面对困难和挑战总是有办法 一般:有不解决问题不罢休的、积极的、有办法态度 待改进:碰到自己无法解决的问题积极向上反映；遇到挫折有时不能快速调解恢复积极心态 较差:对所碰到的问题无动于衷，不向上反映，不去寻求解决方案；遇到挫折容易丧失信心  03.</description>
    </item>
    
    <item>
      <title>MySQL复制故障修复_无主键表大事务卡住</title>
      <link>/mysql/mysql%E5%A4%8D%E5%88%B6%E6%95%85%E9%9A%9C%E8%A7%A3%E5%86%B3_%E5%A4%A7%E4%BA%8B%E5%8A%A1%E5%8D%A1%E4%BD%8F/</link>
      <pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%A4%8D%E5%88%B6%E6%95%85%E9%9A%9C%E8%A7%A3%E5%86%B3_%E5%A4%A7%E4%BA%8B%E5%8A%A1%E5%8D%A1%E4%BD%8F/</guid>
      <description>0.故障现象 生产环境MySQL复制报警，现象
 从库复制延时越来越大,gtid一直停留在固定的地方不变 从库的relaylog越来越大，1G以上，但是增长不明显。 从库当前没有业务访问，不存在资源紧张 主库上最近一段时间没有明显的大批量写入  1.原因定位  从上面的现象卡，基本上可以推断是大事务卡住了， 我看的时候法爷已经把relaylog解出来了，也很明显的看到有很多的delete。 根据以上推断我们去主库上查时间点的日志，发现了： 一个SQL是 delete from t1 where c1=&#39;&#39; 删除了65万行数据 于是问题定位：生产环境的windows机器上有同事用navicat删除了线上MySQL的数据。  简单的一个SQL ，但是因为一些原因综合在一起引起了雪崩
 不幸的是：这张表是个没主键的表，导致从库追日志进程卡住，无法正常执行 幸运的是：这些从库没有业务访问，没有造成实际影响  2.安全规范 首先：生产环境的windows机器安装navicat访问数据库这种行为，肯定是不被允许的，
但是因为“历史原因”我们依然有少量同学（不超过10人）有这种特殊需求。
原计划是3月底推动消除的，
经过此事以后，DBA会加快推进禁止在生产环境安装数据库客户端连接数据库这个规范。
有时候就是这样，觉得这个地方可能有风险，我们排个期来解决，通常就会没等到期限就先暴出来了
 问：为什么我们不用限制账号访问来源的方法？ 答：因为一些原因,加ip限制代价太大，且不利于未来的docker虚拟化。
 3.问题修复 共有3个从节点，我和另外两个DBA用了三种不同的方式来修复
方法一：我用的方法，就很暴力的在从库上reset master 再set 跳过这个事务 use db_test; truncate table t1; stop slave ; reset master; set @@GLOBAL.GTID_PURGED=&#39;59939d78-de2d-11eb-ac46-e43d1a074d20:16020676&#39; start slave; 方法二：法爷用了相对温和的方法，模拟一个事务的方法。 use db_test; stop slave; SET gtid_next=&#39;59939d78-de2d-11eb-ac46-e43d1a074d20:16020676&#39; truncate table t1; SET gtid_next=&#39;automatic&#39;; start slave; 我和法爷讨论了一下，相对来说这个是更安全的方法。保证了事务的连续，偷换了一个事务的内容</description>
    </item>
    
    <item>
      <title>MySQL5.7升级到8.0(一):SQL语法变化</title>
      <link>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B41sql%E8%AF%AD%E6%B3%95/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B41sql%E8%AF%AD%E6%B3%95/</guid>
      <description>研发：MySQL5.7升级到8.0(一):SQL语法变化
  DBA：MySQL5.7升级到8.0(二):配置和参数
   Note：这里面是升级到8.0,需要开发人员参与修改部分或注意部分
   不常用 废弃了 GROUP BY 分组的排序 ASC 和 DESC, 存储过程中包含此语法的无法正常执行;
  不常用 最新版可能不支持 &amp;amp;&amp;amp;, ||, ! 的语法, 需要使用标准 SQL 的 AND, OR, NOT 进行替换;
  不常用 外键的名字在整个 schema 中必须唯一;
  常用 支持公共表表达式cte, 窗口函数 不再支持5.6，5.7 的土方法实现递规这种写法废了！不能再用了
 SELECT * FROM(SELECT @rn:= CASE WHEN @id = id THEN @rn + 1 ELSE 1 END AS rownum,@id:= id as id, volume, dateFROM(SELECT * from table_001 WHERE fdate &amp;lt;= &#39;2022-02-16&#39; ORDER BY id, date DESC) a ,(SELECT @rn=0, @id=0) b )aWHERE rownum &amp;lt;= 5   不常用 支持备份锁(backup lock)</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(一):概况</title>
      <link>/mysql/mysql7%E7%A7%8D%E6%97%A5%E5%BF%97/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql7%E7%A7%8D%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 和其他关系型数据库一样
MySQL也重度依靠数据库日志来完成一系统的功能。
总结了以下7种重要的日志。今天先简单给这7兄弟做个整体介绍。
接下来会分三个章节分别说清楚（binlog/redolog/undolog 这三位不省心的兄弟在做什么).
1.日志的分类 Binlog:  MySQL最重要的日志（没有之一），记录了所有的DDL和DML语句(除了数据查询语句select、show等)，以事件形式记录 DBA依赖它做：高可用方案，异构数据迁移，备份和恢复，误更新回滚。。。。等等 ，是整个MySQL的灵魂 研发/大数据人员依赖Binlog做数据订阅，数据同步  relaylog:  它是依赖于binlog的日志，格式也和binlog一样。 是MySQL复制进程把“别的实例的binlog&amp;quot;复制到本地后，就叫做relaylog. 作用是为了MySQL高可用复制服务的一种日志。  Slowlog:  慢查询日志用来记录在MySQL中响应时间超过阀值的语句，则会被记录到慢查询日志中。由long_query_time参数控制，默认值10秒， 一般线上环境，我们设置为:0.2秒 或0.5秒 两种标准 一般DBA通常会用脚本将日志收集归类，分析后对部分规则产生报警。 这个日志是文本类型的，打开就能看到，比较简单，很容易理解，也很有用  genlog:  一般轻易不开启，开启以后，会将所有经过的SQL都记录到日志里，非常费资源。 定位奇怪的问题时会用到。审计的时候也能用到。 不建议打开，定位问题后，要及时关闭  errorlog:  数据库产生warning,error时会打印的日志，实例启动失败了，或者实例崩溃了必看的日志。平时做好监控。建议开启死锁print，在errorlog中也能看到死锁信息。  redolog：  可能是最难理解的一个日志了，不同于上面的那些日志，redolog是innodb存储引擎的日志，不是MySQL自身的日志 redolog经常会和binlog/undolog搞混 记住最重要的一点：redolog是为了数据库突然关机或崩溃的时候用的。它的作用是：为了不丢失修改。 redolog通常是物理日志，记录的是数据页的物理修改（区别于binlog的逻辑修改)，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页(恢复数据页，且只能恢复到最后一次提交的位置)。  undolog:  undo和redonlog一样也是是innodb存储引擎的日志，用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。 它的作用除了和redolog一起保证数据库突然关机或崩溃的时候，数据不丢失，不混乱。它还是MVCC事务特性的重要组成部分。  小结： 简单介绍完7种日志，其中的三个日志（binlog/redolog/undolog) 涉及知识点非常多，会分别写一篇，慢慢聊。
&amp;ndash; done</description>
    </item>
    
    <item>
      <title>Oracle实时慢日志收集和展示</title>
      <link>/oracle/oracle%E6%80%8E%E6%A0%B7%E5%AE%9E%E6%97%B6%E6%94%B6%E9%9B%86%E5%B1%95%E7%A4%BA%E6%85%A2%E6%9F%A5%E8%AF%A2/</link>
      <pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/oracle/oracle%E6%80%8E%E6%A0%B7%E5%AE%9E%E6%97%B6%E6%94%B6%E9%9B%86%E5%B1%95%E7%A4%BA%E6%85%A2%E6%9F%A5%E8%AF%A2/</guid>
      <description>Oracle怎样实时收集展示慢查询？ 当网上搜相关问题或问一个身边的OracleDBA ，通常会有两种类型的答案
 看awr报告 扔过来一段SQL脚本。   类似于MySQL的慢日志方案：Slowlog--logstash--&amp;gt;es--&amp;gt;MySQL--&amp;gt;web页
但Oracle没有现成的slowlog可以用。
 所以我们先后采用尝试了以下三种方法。
方法一：定时收集awr报告 我们去年和黄老师一起尝试了定时生成awr报告，获取topSQL入库后，再收集展示的方法，这个方案需要对OracleAWR的缓存表理解得很深入，黄老师哼哼哧哧花了一个月时间，最终完成了统一收集，并通过web页展示，但总体效果不好。 主要缺点在于
 不能做到实时，甚至不能准实时（取决于多长时间生成一次awr报告） 只能每个实例取topSQL 方案呆板且不理性 总结：实现难度大，效果差  方法二：Oracle中间件记录慢查询 去年年底的时候我们在写一个Oracle中件间，业务访问Oracle数据库需要连接到Proxy上，然后在Proxy上记录查询日志，这种方法理论上是完全可以的，但是当我去尝试去实现这个功能的时候发现很困难，因为我们采用了端口中转的方式，可以抓到客户端和服务端的通信包，但是不能对应上这些通信包的对应关系。这就导致计算SQL执行时间这一步没法实现。 然后我们架构组也在做jdbc层的中件间，这个是100%可以轻松实现的，但只适用于java程序，对于非java代码访问数据库就无能为力了 总结：收集得很准，但开发工作量大，需要很长的时间
方法三：定时直接读取正在执行的SQL 这个方案是我们年前做了一个Oracle长时间无响应SQL的报警功能（超过60秒没执行完的SQL会发钉钉报警），然后一次故障处理时，法爷说这个功能改一下刚好可以做慢查询收集整理。
于是我们快速的试了下这个方案（代价很小，花了半个小时就完成了，任务配置和报表展示）
 新建存放慢SQL信息的表一个字典表/一个慢SQL表（5分钟） 配置一个每分钟去所有Oracle实例上收集的任务（15分钟） 配合已有CMDB信息，完成慢SQL和研发负责人的对应，写一个SQL （5分钟） 用这个SQL配置出来一张可展示的报表，设置成全体研发可见（5分钟） 总结：零开发工作，只要建个表，配置一个定时job和报表，就可以完成，效果不错  得到这样的一张这样的：实时展示Oracle慢查询的报表 &amp;ndash; done</description>
    </item>
    
    <item>
      <title>分布式一致性协议:raft协议</title>
      <link>/dba/raft%E5%8D%8F%E8%AE%AE/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/raft%E5%8D%8F%E8%AE%AE/</guid>
      <description>关于raft的起源和历史  raft协议是一种分布式强一致性协议  为什么要有一致性？  1.数据不能存在单个节点（主机）上，否则可能出现单点故障 2.多个节点（主机）需要保证具有相同的数据。   都有哪些一致性协议（算法）  Paxos ：强一致性，由Lamport出品，例如：腾讯的PhxSQL，阿里的OceanBase数据库 Raft ：强一致性，由Paxos改进而来，例如：redis的sentinel,etcd数据库 用的是raft协议 ZAB ：强一致性，由Paxos改进而来，例如：ZooKeeper Gossip协议：弱一致性或者叫最终一致性，例如：rediscluster协议     2014年，由斯坦福大学Diego的一篇200多页的博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》提出的一种全新的一致性协议。英文好的可以去看看。网上也有好多翻译成中文的 虽然核心协议上基本都是师继Paxos协议，基于多数派的协议。但是 Raft 一致性协议的贡献在于，定义了可易于实现的一致性协议的事实标准。模块化拆分以及设计简化。使分布式协议更加容易理解和实现  名词一： 复制状态机(Replicated state machines)  为了简化和便于理解，raft协议提出了复制状态机的概念，将集群中的节点都当成一个复制状态机，每个状态机只有三种状态。 复制状态机(Replicated state machines) ： 将集群中的每个服务器看做一个状态机, 它们接收外部的指令, 进行状态的改变, 所谓保持分布式一致性即是保证集群中所有状态机的状态一致性。 在任何时候，每个服务器都处于以下三种状态中的一种: 领导人(leader): 处理所有客户端的交互，日志复制同步，任何时候最多有一个领导人 跟随者(follower): 完全被动（不发出RPC，响应传入的RPC） 候选人(candidate): 用于选举新领导者 我们用图来解释这三种状态的变化关系  名词二： 任期和选举 为了判断过时的信息，过时的leader，raft协议提出了任期(term)的概念
 时序被分割为多个领导者任期 每个任期最多1个领导者 有些任期没有领导者（如：上图上的第2个阶段选举失败，但是任期值还是会加1） 每个服务器维护当前任期的值  1.什么时候开始选举  一个正常运行的系统，领导者必须不断的发送心跳（AppendEntries RPC）以保持其领导者的地位 如果在electionTimeout时间内（一般是100-300ms)，跟随都未收到RPC: 跟随者假设领导者已经崩溃，开始新的选举  2.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（四）:性能测试和改进</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B44/</link>
      <pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B44/</guid>
      <description>续： 从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》 从零写一个兼容MySQL/Oracle的Proxy中件间（二）:SQL捕获和改写 从零写一个兼容MySQL/Oracle的Proxy中件间（三）:MySQL协议捕获和转发
1.过去的三个文章我们实现了以下功能]  Oracle登录捕获：捕获了Oracle通信协议中的用户登录包 Oracle用户解析：抓到了用户传用户名和密码的内容（密码是加密串） SQL请求包：同时通过对比，确定了用户发送SQL请求的通信包 OracleSQL日志：分析这些包，把SQL语句拿出来，记到日志里。 OracleSQL改写：用户发起的SQL 经过中间层改写到了服务端收到的是另一个SQL执行返回结果。 MySQL兼容：增加配置文件，使中间件可以支持两种数据库 MySQL协议解析：将经过proxy的MySQL包里的SQL语句解析出来，记录到日志  在没更新的这几天里我又偷偷完成了配置变更等小功能。现在中件间其实已经在理论上可以发布使用了
在投入使用前，在测试环境对这个半成品的中件间做了些基准测试。
在测试环境上生成了5张表，每张表200万行数据，对其进行直连和proxy模式压测。
以下是测试报告： 结论是：加了Proxy，性能下降了14% ，在情理之中，一般的SQL中间层因为多了层中转，响应时间会降低20ms左右。tps/qps在不做连接池的情况下会下降10%。 分析性能下降的原因：
因为在proxy存把经过的网络包都拆开来分析其中的内容，且把SQL语句存在日志里，这些步骤是比较费资源和时间的。
为了提升Proxy性能，降低中间层的性能影响，我们加了个配置参数
cat /data/proxy/conf/proxy3308.cnf [basic] logfile = /data/proxy/log/3308.log daemon = true [proxy] proxytype = mysql bind = 0.0.0.0:3308 server = 127.0.0.2:3308 isssl = false iscatchquery = false #增加是否“拆包” false时，Proxy进入高性能模式 iscatchlogin = false maxsquerylsize = 16384 当 iscatchquery=false时，Proxy进入高性能模式
if Iscatchquery { #只有iscatchquery为true时才解析包。 switch ProxyType { case &amp;quot;mysql&amp;quot;: log.Printf(&amp;quot;mysql:sqlPipeMySQL\n&amp;quot;) sqlPipeMySQL(srcCon, dstCon) case &amp;quot;oracle&amp;quot;: log.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（三）:MySQL协议捕获和转发</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B43/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B43/</guid>
      <description>续： 从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》 从零写一个兼容MySQL/Oracle的Proxy中件间（二）:SQL捕获和改写
1.过去的两天我们实现了以下功能]  Oracle登录捕获：捕获了Oracle通信协议中的用户登录包 Oracle用户解析：抓到了用户传用户名和密码的内容（密码是加密串） SQL请求包：同时通过对比，确定了用户发送SQL请求的通信包 OracleSQL日志：分析这些包，把SQL语句拿出来，记到日志里。 OracleSQL改写：用户发起的SQL 经过中间层改写到了服务端收到的是另一个SQL执行返回结果。   MySQL兼容：增加配置文件，使中件间可以支持两种数据库 MySQL协议解析：将经过proxy的MySQL包里的SQL语句解析出来，记录到日志  开始动手：
步骤一：中件间可以同时支持MySQL和Oracle 中件间的配置应该放在哪，理论上是想放在MySQL或zk里，当配置有变更的时候，中件间获得变更，但这个实现有点麻烦，可能得写好久，就先一个本地的配置文件
准备一个配置文件
proxy] proxytype = mysql bind = 0.0.0.0:1106 server = 10.26.*.*:3307 isssl = false iscatchquery = true iscatchlogin = false maxsquerysize = 4096 [proxybak] #proxytype = oracle #bind = 0.0.0.0:1106 #server = 10.26.*.*:1521 #isssl = false #iscatchquery = true #iscatchlogin = false #maxsquerylsize = 4096 然后在通信进程中收到包时处理
func (t *Proxy) pipeSend(dstCon, srcCon *Conn, chSend chan int64) { defer pipeClose(dstCon) switch ProxyType { case &amp;quot;mysql&amp;quot;: log.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（二）:SQL捕获和改写</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B42/</link>
      <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B42/</guid>
      <description>续上一篇： 从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》
0.前言 昨天的文字里写开发这个中间件的原由和要解决的问题，有朋友留言
网上有现成的开源中间件为啥不用。
 答：网上有很多MySQL的中件间，Oralce目前还没有可以免费使用的中件间. 这可能就是开源和闭源的差别。
 Oracle自带的功能已经可以实现想要的功能（高可用/审计日志）
 答：
 Oracle官方的高可用方案RAC，无疑是非常非常非常优秀的,但我们现有的硬件不支持做跨机房RAC,以及我们迁移时需要proxy中间层来降低业务中断时间。 Oracle的审计日志太笨重/不支持慢日志/不支持SQL黑名单。   1.昨天我们实现了以下功能]  捕获了Oracle通信协议中的用户登录包 抓到了用户传用户名和密码的内容（密码是加密串） 同时通过对比，确定了用户发送SQL请求的通信包   SQL日志：分析这些包，把SQL语句拿出来，记到日志里。 SQL改写：用户发起的SQL 经过中间层改写到了服务端收到的是另一个SQL执行返回结果。  开始动手：
步骤一：从Oracle通信包中分解出SQL语句 已知有以下两种head的包是在传递SQL
0x1 0xf 0x0 0x0 0x6 0x0 0x0 0x0 0x0 0x0 0x11 0x6b 0x4 0xa5 0x10 0x0 0x0 0x35 0x1c 0x0 0x0 0x1 0x0 0x0 0x0 0x3 0x5e 0x5 0x61 0x80 0x0 0x0 0x0 0x0 0x0 0x0 0xfe 0xff 0xff 0xff 0x1 0x0 0x0 0x0 0x6 0x0 0x0 0x0 0x0 0x0 0x3 0x5e 0x6 0x61 0x80 0x0 0x0 0x0 0x0 0x0 0x0 0xfe 0xff 0xff 0xff 0xff 0xff 0xff 0xff 0x24 0x0 0x0 0x0 0xfe 0xff 0xff 0xff 0xff 0xff 0xff 1.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B41/</link>
      <pubDate>Wed, 05 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B41/</guid>
      <description>0.前言 MySQL由于开源的原因，有各式各样的中件间Proxy ，极大的丰富了做高可用或迁移的方案，习惯了MySQL生态圈的灵活和便利，Oracle官方不开源代码和协议，没有中间件proxy，显得很笨重。
比如以下的方案就会很不好办：
 实时抓取Oralce的访问SQL日志 慢日志捕获和收集 高可用中件间Proxy在故障时自动切换 SQL访问黑名单。  基于以上的一些困难，打算自己从头写一个兼容MySQL/Oracle的中件间，希望从中件间层同时支持两种数据库。方便我们做数据库的高可用管理和从Oracle到MySQL的迁移。
这个计划是在年前的2021年最后一次组内会议上提出来的构想。元旦放假期间我就一直在想这事怎么搞
问题的难点在于：Oracle的client/server端通信没有文档的说明，没人能说清楚Oracle是怎么交互的。
这两天用最原始的方法抓包，一个包一个包的去看，找到包的规律，分析它的通信协议。竟然发现这个方法可行
1.步骤 1.写一个Python脚本去连接（192.168.1.1:1521）上的Oracle  #!/usr/bin/env python ## coding: utf-8 import cx_Oracle conn = cx_Oracle.connect(&#39;dboopreader/dbooppassword@192.168.1.1:1521/testdb&#39;) print(&amp;quot;连接成功&amp;quot;) conn.close() print(&amp;quot;连接关闭&amp;quot;) 通过wireshark抓包，发现一次简单的连接，有38个通信包。
2.捕获这些包，发现它的规律 挨个点开这些包，发现了一些有用的信息，然后发现wireshark的包看起来不方便， 本地模拟一个端口1522端口，劫持这些请求，打印出来，得到如下这种的tcp包
抓到:127.0.0.1到192.168.1.1的包 二进制展示如下: 0.0x7 0xaf 0x0 0x0 0x6 0x0 0x0 0x0 0x0 0x0 0x2 0x54 0x3 0x54 0x3 0x3 0x2a 0x6 0x1 0x1 20.0x1 0x6f 0x1 0x1 0xc 0x1 0x1 0x1 0x1 0x1 0x1 0x1 0x7f 0xff 0x3 0xe 0x3 0x3 0x1 0x1 40.</description>
    </item>
    
    <item>
      <title>分布式数据库</title>
      <link>/dba/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/</guid>
      <description>分布式CAP原则 CAP定理是NOSQL数据库的基石，指的是在一个分布式系统中，Consistency(一致性)、Availability(可用性)、Partition tolerance(分区容错性)这三者只能满足其中两个，不能全部满足，这个结论是实践的总结，但是理论上，如果网络很好的情况下，三个特性可以同时满足。现实中的分布式系统因为网络的原因，很有可能会出现延迟或丢包等问题，因此必须要实现分区容忍性，一致性和可用性之间只能二选一。 对于传统的数据库来说，需要强一致性；而NoSQL系统一般注重性能和扩展性，而非强一致性。
 Consistency(一致性)：分布式数据系统中的所有的数据备份，在同一时刻值都相同。也就是同一时刻所有机器上的数据保持一致。 Availability(可用性)：当集群中的一部分节点故障后，集群整体还能响应客户端的请求。也就是说，每个用户的请求都能收到正常的响应，并且响应时间在用户可接受范围内。 Partition tolerance(分区容错性)：尽管节点之间网络通信丢失或延迟了任意数量的消息，但是整个系统仍然在正常运行。  可以有三种组合：
 CA：指的是单点集群。 CP：舍弃了可用性，可用性指的是高性能，所以性能不是很高。 AP：是弱一致性，一般的NoSQL数据库对一致性的要求不是很高  分布式的一致性  为什么要有一致性？  1.数据不能存在单个节点（主机）上，否则可能出现单点故障 2.多个节点（主机）需要保证具有相同的数据。   都有哪些一致性协议（算法）  Paxos ：强一致性，由Lamport出品，例如：腾讯的PhxSQL，阿里的OceanBase数据库 Raft ：强一致性，由Paxos改进而来，例如：redis的sentinel,etcd数据库 用的是raft协议 ZAB ：强一致性，由Paxos改进而来，例如：ZooKeeper Gossip协议：弱一致性或者叫最终一致性，例如：rediscluster协议    数据分片 当数据量过于庞大，单机难以支撑时，会面临扩展瓶颈，那么就需要将数据进行拆分，分散在多个数据库实例上。
数据分片是指将数据全局划分为相关的逻辑片段，有水平切分、垂直切分、混合切分三种类型。
 水平切分:可以简单地理解为按照数据行进行切分，即一部分行放在某数据库，另外一部分放在另外的数据库实例。比如可以按照时间、地区拆分，亦或是根据hash进行拆分。 垂直切分：垂直拆分可以简单理解为按照表进行分类，将表分布在不同的节点上,基本目标是将使用频繁的属性聚集在一起 混合切分：水平切分与垂直切合的结合。  数据分片的基本原则
 完备性条件 可重构性条件 不相交性条件  垂直分片后将数据组合起来需要执行连接运算，比水平分片后的数据组合要困难一些。
分布式查询处理及优化 分布式数据库需要考虑查询问题，其需要做的就是把一个分布式数据库上的高层次查询映射为本地数据库上的操作，最后通过网络通信，将操作结果汇聚起来。
相对于集中式数据库，分布式数据库还要考虑额外的几个问题：
 选择最优站点查询 数据传送方式 站点之间交换数据的问题 相对于集中式的查询目标，分布式需要多考虑一项 “通信开销代价”  对水平分片的优化
 尽量把选择条件下移到分片的限定关系处，再把分片的限定关系与选择条件进行比较，去掉它们之间存在矛盾的相应片断。 如果最后剩下一个水平片断，则在重构全局关系的操作中，就可去掉“并”操作.  对垂直分片的优化
 把垂直分片所用到的属性集，与查询条件中的投影操作所涉及的属性相比较，去掉无关的垂直片断。 如果最后只剩下一个垂直片断与查询有关时，去掉重构全局关系的**“连接”**操作（至少可以减少“连接”操作的次数）  基于半连接算法的查询优化</description>
    </item>
    
    <item>
      <title>数据库备份管理制度</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E7%AE%A1%E7%90%86%E5%88%B6%E5%BA%A6/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E7%AE%A1%E7%90%86%E5%88%B6%E5%BA%A6/</guid>
      <description>备份策略  数据库备库节点上启用定时任务，每天0:10～6:10 全备（或增备)文件 至本地sas盘，每个实例压缩成一个备份文件。 生成的备份文件调用接口上传状态并断点续传止主备份备机 数据库备份在本地sas盘上保留1-3天的备份文件 辅备份机在0点-22点保持当天的备份文件夹和主备份机的同步 主备份机每天22:00 将当天的接收到的备份文件移至 yyyyMMdd 目录下 所有备份机每天23:00删除30天以上的过期备份文件  每个月的第一次数据库全备，永不过期删除（例如：如果db每天一次全备，则每月的1号备份永久保留）    备份周期  MySQL：每天1次全备，15分钟同步一次binlog日志，全备和日志保留30天 Oracle:每周1次全备,其余天数增备，实时保存日志，备份和日志保留30天 MongoDB:每天1次全备，全备保留30天  备份脚本（源机） 传输脚本（辅备份机） </description>
    </item>
    
    <item>
      <title>孤岛备份机和勒索病毒</title>
      <link>/dba/%E5%AD%A4%E5%B2%9B%E5%A4%87%E4%BB%BD%E6%9C%BA%E5%92%8C%E5%8B%92%E7%B4%A2%E7%97%85%E6%AF%92/</link>
      <pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%AD%A4%E5%B2%9B%E5%A4%87%E4%BB%BD%E6%9C%BA%E5%92%8C%E5%8B%92%E7%B4%A2%E7%97%85%E6%AF%92/</guid>
      <description>勒索病毒 什么是勒索病毒？  勒索病毒就是那种中毒后 加密你的文件（通常是aes加密算法) 提示你去支付一些电子货币才能解开文件的一种病毒 通常是要求支付一定数量的比特币 像下面这种  怎么写一个勒索病毒？  如果让我写一个简单的勒索病毒 我可能会这样写 0.像指定的服务器（控制机）请求一个aes公钥 1.用这个公钥挨个给本地文件加密  1.1 遍历本地所有文件 1.2 给每个文件头加上一个特殊标记（不用多，10来个字节就行） 1.3 挨个用公钥加密所有文件   2.提示用户文件加密了，要求给钱 3.如果收到钱了就给他一个解密的代码 4.解密代码这样写  4.1 遍历本地所有文件 4.2 判断是否有特殊标记 4.3 如果有，则是加密文件 4.4 用私钥去解开这个文件   当然真实的勒索病毒会更加严谨，我只是描述一下思路 我也从来没写过  中了勒索病毒怎么办？  不差钱方案：给钱，然后寄希望于对方的人品。 运气好方案：这是个常见的普通勒索病毒，网上有很多的工具可以尝试解一下 报警：造成重大损失的可以公开报警，交给安全部门处理，当然这个破案的难度有点大，数据可能还是找不回来 补救方案：用备份来救命。  如果有备份，可以恢复文件，那这时候就基本上可以依靠本身的备份体系来恢复大部分损失（还是会有不可挽回的损失）    勒索病毒和备份体系的攻防  聪明的勒索病毒会攻击备份体系 1.本机备份：中了勒索病毒以后，本机备份几乎是99%也会中毒，几乎没啥用了 2.异机备份：如果是个人电脑中毒，很难会感染到备份机，但是如果是机房里的服务器中毒了，那么病毒极有可能会感染备份机。 3.异机房备份：同异机备份，主要还是一个服务器内网环境。 如何防止勒索病毒攻击备份体系呢？ 这就是我们接下来下说的孤岛备份机方案  孤岛备份机 什么是孤岛备份机？  它是一个特殊的备份机 1.它不和普通的服务器连网 2.本地不开任何端口，任何其他服务器不能请求它的任何服务 3.只和指定的一台机器直连（通常这台机器是个普通的备份机） 4.它只以视为“这台普通备份机”的备份机 5.它会定时拉取普通备份机上的指定目录 6.</description>
    </item>
    
    <item>
      <title>Linux服务器共享目录Centos7</title>
      <link>/ops/linux%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/linux%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB/</guid>
      <description>0.环境 服务器：
 10.10.0.1 源机器 10.10.0.2 目标机器1 10.10.0.3 目标机器2 目标： 将10.10.0.1机器上的 /public/downloadnew 文件夹共享给10.10.0.2/3两台机器  三台机器共享写入 /data/www/dboop/static/download目录
1.安装nfs yum -y install nfs-utils rpcbind # 开机启动 systemctl enable rpcbind.service systemctl enable nfs-server.service # 重启服务 systemctl restart rpcbind.service systemctl restart nfs-server.service 2.共享设置 10.10.0.1 上执行 mkdir /public/downloadnew ln -s /public/downloadnew /data/www/dboop/static/download -f vim /etc/exports #输入 /public 10.10.0.2(insecure,rw,sync,no_root_squash) 10.10.0.3(insecure,rw,sync,no_root_squash) exportfs -rv 10.10.0.2/3 上执行 vim /etc/fstab #输入 10.10.0.1:/public /mnt/public nfs defaults 0 0 mkdir /mnt/public mount -a df -h ln -s /mnt/public/downloadnew /data/www/dboop/static/download -f 到此 10.</description>
    </item>
    
    <item>
      <title>MySQL的锁:innodb锁粒度详解</title>
      <link>/mysql/mysql%E9%94%81_innodb%E9%94%81%E7%B2%92%E5%BA%A6/</link>
      <pubDate>Sat, 13 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E9%94%81_innodb%E9%94%81%E7%B2%92%E5%BA%A6/</guid>
      <description>锁定义 lock_rec_not_gap锁  Record Locks
 A record lock is a lock on an index record. For example, SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; prevents any other transaction from inserting, updating, or deleting rows where the value of t.c1 is 10. Record locks always lock index records, even if a table is defined with no indexes. For such cases, InnoDB creates a hidden clustered index and uses this index for record locking.</description>
    </item>
    
    <item>
      <title>MySQL binlog 问答</title>
      <link>/dba/mysqlbinlogquestion/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/dba/mysqlbinlogquestion/</guid>
      <description>binlog 是什么?  MySQL 的二进制日志 ,不是纯文本类的 记录的是数据库变动的日志(insert,update,delete,create,replace,grant &amp;hellip;.) 不包括 select,set 等  binlog 重要吗？是不是一定要开？  几乎是最重要的MySQL日志，严谨点说是最重要的之一 如果没有特殊情况，一定要开！为什么？你再往后看&amp;hellip;)  binlog 有什么作用？  高可用同步，经常用它来同步主库和从库的数据。 它本身就是记录了数据库变化的日志，放在那让你看也是它作为“日志”的作用 恢复指定时间点的数据, 想把数据库恢复到指定时间眯，得靠它 回滚数据 ，误删除数据时用到 审计 变更捕获到其他平台(kafka,es或其他数据库)  binlog 怎么开启，放在哪，怎么存储的？ binlog组提交  MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数 引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程： flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）； sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）； commit 阶段：各个事务按顺序做 InnoDB commit 操作； 上面的每个阶段都有一个队列，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。 对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率。  主从复制是怎么实现？ MySQL 集群的主从复制过程梳理成 3 个阶段：  写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引擎中的数据。  具体详细过程如下：  MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。  24、什么时候 binlog cache 会写到 binlog 文件？ 在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。</description>
    </item>
    
    <item>
      <title>丐帮PK赛</title>
      <link>/dba/gaibangpk/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/dba/gaibangpk/</guid>
      <description>[ 丐帮pk赛：帮派里选出来最睿智的两个人，1vs1，互相出题，答题时长不限，方法不限,结果正确了，得1分，否则对方得一份。统计最终得分。 ]
当前得分 p:3 / k:8  第一轮.地铁站走楼梯问题 题目:从知春路地铁站走楼梯上来，共有30级台阶，阿怂每次上台阶要么是1层，要么是2层，问：有多少种方法阿怂可以爬完这30级台阶？
 出题人:P 答题人:K 答题时间:用时17分钟 答题结果:正确 正确答案:1346269 解答:每一层楼梯的方法不是他决定的,是他上一级台阶有几种方法和上上一级台阶有几种方法,加一块就是他有几种方法. 图解： 代码：https://github.com/51ak/golearn/blob/master/day11_%E7%88%AC%E6%A5%BC%E6%A2%AF/main.go   第二轮.病毒感染率问题 题目:假设新冠病毒检测covid-19的感染率为1%，而新冠病毒检测检测准确率为99%,如果阿怂做了一次检查，被通知结果为阳性，那么阿怂真正感染的概率是多少？
 出题人:K 答题人:P 答题时间:用时2分钟 答题结果:失败! 答成了99% 正确答案:50% 解答:假设有10000人，感染率为1%,那么其中有100为感染，9900位健康,其中100感染者里，有99位真感染，1人误诊，9900位健康者里，有99位因为误诊是感染的，9801位是健康的。于是：被告知感染的人里，有99位真正的感染者和99位健康的人。你感染了被诊断出来的几率是0.01*（1-0.01）=0.0099，假如你没感染被诊断出来的几率是0.99*0.01=0.0099所以实际上依然健康的概率为0.0099/（0.0099+0.0099）=0.5 贝叶斯理论   第三轮.收保护费问题 题目:阿怂要到一条街上收保护费，老大交待这次不能同时收相邻的两个捕子的保护费，收了第1号铺子的就不能收第2号，可以跳过去收3或者4号。假如从1号铺子开始每个铺子能收到的保护费分别是4, 3, 5, 4, 5, 23, 7, 8, 9, 7, 7, 8, 12, 15, 17, 13, 4, 9, 12, 17, 13, 5, 0, 7, 14, 7, 9, 8, 9, 30 元，问：阿怂最多能收到多少元钱？
 出题人:P 答题人:K 答题时间:用时18分钟 答题结果:166算错了一个数 正确答案:167 解答:当收到第N个铺子时，可以选择收和不收，  选择收，那么这个铺子时的最大金额就是N-2间铺子的最大金额+第N间铺子 选择不收，那么收到这个铺子时的最大金额就是第N-1间铺子的最大金额 所以在N号铺子时，怂哥最多能拿到的钱就是上面两个值取个最大值，把这个值人做个记号，然后接着去下个铺子做同样的事，结果就一路推出来了。 在第N号铺子，只需要回头看看N-2，N-1号铺子门口的最大金额标记和这个铺子的钱就知道这个铺子的记号。 图解： 代码：https://github.</description>
    </item>
    
    <item>
      <title>MySQL的drop/truncate Table影响分析和最佳实践</title>
      <link>/dba/droptable/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/dba/droptable/</guid>
      <description>0.前言 MySQL上直接Drop张大表,会有什么影响，能否直接写个 drop table ; 或者 truncate table ; 甚至是delete * from 如果这张表足够大，比如1亿行记录，drop 的时间需要多久，期间我的MySQL是否能正常访问？
首先明确一点，现在讨论的是要删掉的大表一定是没人访问的表，否则如果这张表仍然还有被高频的访问，你敢直接删那基本上就是茅坑里点灯，找死！ 如果MySQL版本是5.5.23以下，直接DROP一张大表，也是守着茅坑睡觉，离死不远。 好，现在明确了这张表肯定没人访问了，你的MySQL版本也足够新，并不表示你就远离了茅坑，但如果这张表足够大，仍然有被崩到的风险。
大表：我们定义为5000万行以上，或者单表文件大于100G
我们要讨论的是innodb存储引擎,myisam等存储引擎，DROP 表又快又安全
1.drop table 的风险和避免方法 Drop table 要做的主要有3件事：  把硬盘上的这个文件删了 把内存中的这个库已经加载加来的Page删了，腾出空间 把MySQL元数据字典中这张表关联信息删了  可能会引起的风险有3种：  MySQL长时间阻塞其他事务执行，大量请求堆积，实例假死。(锁) 磁盘IO被短时间大量占用，数据库性能明显下降(IO) 内存里的page大量置换，引起线程阻塞，实例假死（内存)  解决和避免的方法3种：  io占用的问题，对这个表建一个硬链，使Drop table 表的时候并没有真的去磁盘上删那个巨大的ibd文件，事后再用truncate的方式慢慢的删除这个文件，如果是SSD盘和卡,drop table后再直接rm文件也没问题 内存和IO占用的问题，升级MySQL版本   MySQL 5.5.23 引入了 lazy drop table 来优化改进了drop 操作影响(改进，改进，并没有说完全消除!!!拐杖敲黑板3次)
  MySQL5.7.8 拆分了AHI共用一个全局的锁结构 btr_search_latch
  MySQL8.0 解决了truncate table 的风险
   道路千万行，低峰第一条。选择低峰时间段，找个夜深人静，月黑风高的时候是更好的选择。  2.</description>
    </item>
    
    <item>
      <title>MySQL的行格式(Compact、Redundant、Dynamic和Compressed)</title>
      <link>/mysql/mysql%E7%9A%84%E8%A1%8C%E6%A0%BC%E5%BC%8Fcompact_redundant_dynamic_compressed/</link>
      <pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E8%A1%8C%E6%A0%BC%E5%BC%8Fcompact_redundant_dynamic_compressed/</guid>
      <description>前言  MySQL的默认存储引擎innodb是按16k大小的page来组织存储数据的 MySQL的*.ibd 数据文件，大小一定是能被16kB整除的 在逻辑上innodb是按btree来组织数据存储的 针对每一行具体的数据，共有4种存储方式可供选择：Compact、Redundant、Dynamic和Compressed 其中：Redundant 已经被淘汰了，不建议使用 Compact/Dynamic/Compressed 用的是同一个原理，只在细节上有点变化，不影响其实现逻辑 所以我们说行格式的时候，就可以从compact格式来分析，后两种是compact格式的变种   以下原理部分，都只说compact行格式。(?因为compact是基础，后两种都是基于它衍生出来的)
 行格式在哪里看，怎么修改行格式 查看 mysql&amp;gt; show table status like &#39;%dbooptest%&#39; \G *************************** 1. row *************************** Name: dbooptest Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 9 Avg_row_length: 1820 Data_length: 16384 Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: 2020-06-10 20:22:49 Update_time: 2020-06-10 20:22:49 Check_time: NULL Collation: utf8mb4_unicode_ci Checksum: NULL Create_options: Comment: 测试 1 row in set (0.</description>
    </item>
    
    <item>
      <title>MySQL的事务id:trx_id</title>
      <link>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1id_trx_id/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1id_trx_id/</guid>
      <description>什么是事务id(trx_id)  可以理解为MySQL官方存储引擎innodb维护的一个全局自增变量:max_trx_id, 一个6字节长度的整数。(max_trx_id如果一直增长，理论上也是有溢出的可能性的，超过2的48次方后，会重新从0开始，这时候会破坏事务的顺序规则) 每当一个事务开始时，需要申请一个新的trx_id值时，就获取max_trx_id的最新值，然后将max_trx_id值加1。  事务id的作用  主要是用来记录事务开始的顺序 会用在各种事务冲突和mvcc中  如何查看事务id 查看当前事务的trx_id  select TRX_ID from INFORMATION_SCHEMA.INNODB_TRX where TRX_MYSQL_THREAD_ID = CONNECTION_ID() 查看当前的事务id列表（活动)  select TRX_ID from INFORMATION_SCHEMA.INNODB_TRX 查看当前的事务id列表（活动+非活动) 看innodb status 的TRANSACTIONS 部分
show engine innodb status \G # 找到这一部分 TRANSACTIONS 部分 TRANSACTIONS Trx id counter 2419 -- 当前最大事务 ID Purge done for trx&#39;s n:o &amp;lt; 2419 undo n:o &amp;lt; 0 state: running but idle History list length 0 LIST OF TRANSACTIONS FOR EACH SESSION: ---TRANSACTION 421658589187480, not started 0 lock struct(s), heap size 1136, 0 row lock(s) ---TRANSACTION 421658589186624, not started 0 lock struct(s), heap size 1136, 0 row lock(s) ---TRANSACTION 421658589185768, not started 0 lock struct(s), heap size 1136, 0 row lock(s) 上面是我搭的测试环境，所以没有活跃事务， 需要注意的是几个事务id都非常大（例：421658589187480） 这个后面会解释说明</description>
    </item>
    
    <item>
      <title>MySQL原理_innodb存储格式详解(二)</title>
      <link>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A32/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A32/</guid>
      <description>MySQL用page页来管理和存储数据文件，这些page页是如何被组织起来的，真实的数据(data,index) 真实是怎么存放在ibd文件和内存中的呢，了解存储格式，将帮助我们更好的理解MySQL是如何工作的，从而更好的理解其他数据库知识点（索引，MVCC,等等），本视频中我将利用Python脚本把这些难以理解的数据页解析出来并图形化展示给大家，望大家知其然而知其所以然
视频较长，分两段录制，录制时音量较小，注意控制下声音大小。
  下面是分析.idb文件的脚本，执行结果的明细部分</description>
    </item>
    
    <item>
      <title>MySQL原理_innodb存储格式详解(一)</title>
      <link>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A3/</guid>
      <description>MySQL用page页来管理和存储数据文件，这些page页是如何被组织起来的，真实的数据(data,index) 真实是怎么存放在ibd文件和内存中的呢，了解存储格式，将帮助我们更好的理解MySQL是如何工作的，从而更好的理解其他数据库知识点（索引，MVCC,等等），本视频中我将利用Python脚本把这些难以理解的数据页解析出来并图形化展示给大家，望大家知其然而知其所以然
视频较长，分两段录制，录制时音量较小，注意控制下声音大小。
  核心问题：MySQL（*innodb)是如何组织，存储表数据的？
innodb单表最大能到多少，为什么？
为什么innodb数据文件的大小始终可以被16384整除？
int 和bigint 差别有多大?
varchar(10) 和varchar(100) 差别有多大？varchar(1000)呢? TEXT 呢?
1页(page)=基本单位，存储和读取的核心，每页大小默认：16k
1区(extend)=64页,64*16k=1M
1组(space)=256区, 256*1M=256M
每个page都有个编号，整型最大 2^32 x 16k = 64T （单表大小上限）
下面是分析.idb文件的脚本，执行结果截图 </description>
    </item>
    
    <item>
      <title>我收藏的一些安全(黑科技)硬件</title>
      <link>/book/%E6%88%91%E6%94%B6%E8%97%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%89%E5%85%A8%E7%A1%AC%E4%BB%B6/</link>
      <pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%88%91%E6%94%B6%E8%97%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%89%E5%85%A8%E7%A1%AC%E4%BB%B6/</guid>
      <description>2020年春节 冠状病毒肆虐
憋在家里觉得是个机会有大把时间
重构一个以前自己写的一个项目代码
战斗的第二天
笔记本因为长时间不关机
硬盘烧了
无法补救的那种
虽然有备用电脑
但离开了个人熟悉的软件和环境
好多事情还是没法做
比如
每周四都会在公众号上连载的MySQL视频
就被迫中断了
昨晚我仰望星空
在想宇宙的奥秘和人类&amp;hellip;
以及软件和硬件的依赖
再牛的软件还是靠硬件支撑的
而我对硬件基本是白痴
日常工作中凡是硬件相关的
都尽量交给更合适的同事完成
只有个人电脑和数码产品
才会花精力去折腾
但作为一个硬件白痴
也收藏了一些有趣的硬件
以前周末会经常去中关村的鼎好，E世界
瞎逛
买了很多无用的小东西
类似于女人对口红的追求
迷恋板卡和黑科技
今天跟大家分享3个有趣的跟安全有关的硬件
提前声明
不可以学坏了，做坏事，否则后果自负
1.PM3 呃，这是git上一个很酷炫项目的硬件载体
华强北给焊出来了
众所周知街边配钥匙的会配一些小区门禁卡，电梯卡，员工卡,甚至一些加密卡
那么pm3能做的会更多
对应的卡片种类极大丰富
最重要的是破解加密卡，你知道插上usb连上电脑以后那个小卡片就是个裸着的
2.摩托罗拉c118 上古神器
2g网络的缺陷魔鬼被这个摩托罗拉这个低端手机给释放了
连上电脑，关联上附近的信号塔，开始短信嗅探
浓浓的黑科技感觉
当然现在是4G，5G时代了
3.大功率WiFi信号接收器 呃，这个是wifi信号定向（发送）接收器
但是因为wifi协议的缺陷
尤其是早期路由器的wps_pin 码漏洞
介绍下这个Pin漏洞（基本上几个小时一定会破解一个无线wifi密码）
有定向信号接收器外挂在室外（物理外挂）
周边1公里的wifi都能破
好在这几年pin漏洞都补上了
但抓包暴力依旧还在
Wifi密码照样不经打
Wifi信号放大器会增加这种能力的范围和强度
所以Wifi密码一定要很复杂
Wifi爆破成功就进内网（然后&amp;hellip;）
然后，
再说这个硬件
它不止是接收信号放大
也可以把发送信号放大
想想这又可以怎么搞事
发散一下思维
不可以学坏了，做坏事，否则后果自负
不可以学坏了，做坏事，否则后果自负
不可以学坏了，做坏事，否则后果自负
2020-1-30 晨 北京顺义</description>
    </item>
    
    <item>
      <title>windowsCluster集群脑裂问题最佳实践</title>
      <link>/sqlserver/windowscluster%E9%9B%86%E7%BE%A4%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/sqlserver/windowscluster%E9%9B%86%E7%BE%A4%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</guid>
      <description>故障描述  WinSvr2008R2/Windows 故障转移集群 意外中断1分钟左右后自行恢复 导致SQLServer alwayson集群 中断访问，连接卡住无法提供访问  错误日志 从日志上来看，当时应该在这个时段节点dboopawo91和dboopawo92两台服务器发生了网络错误，我们看到在日志中显示，两台服务器互相联不通对方了，所以在他们的日志中显示，由于这些机器都无法联通，所以他们都被从群集中踢出。
------------------------------------------------------------------------------ 节点dboopawo91报由于网络问题联系上不dboopawo92。 --------------------- 11/06/2015 12:13:02 AM Critical dboopawo91.server.dboop.com 1135 Microsoft-Windows-FailoverClustering Node Mgr NT AUTHORITY\SYSTEM Cluster node &#39;dboopawo92&#39; was removed from the active failover cluster membership. The Cluster service on this node may have stopped. This could also be due to the node having lost communication with other active nodes in the failover cluster. Run the Validate a Configuration wizard to check your network configuration.</description>
    </item>
    
  </channel>
</rss>
