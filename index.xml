<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Classic</title>
    <link>/</link>
    <description>Recent content on Classic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 27 Mar 2025 00:00:00 +0000</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>做了一个去O的工具：异构数据验证对比</title>
      <link>/oracle/%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%8E%BBo%E7%9A%84%E5%B7%A5%E5%85%B7%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E8%A1%8C%E7%BA%A7%E9%AA%8C%E8%AF%81%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Thu, 27 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>/oracle/%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E5%8E%BBo%E7%9A%84%E5%B7%A5%E5%85%B7%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E8%A1%8C%E7%BA%A7%E9%AA%8C%E8%AF%81%E5%AF%B9%E6%AF%94/</guid>
      <description>工具有什么用  当我们用一些第三方同步工具同步数据时，同步过程中可能会出现：数据不一致 这时候需要一些数据对比，来验证数据同步是否按预期进行 很早以前我写过类似的功能。但是一直没有做异构数据同步支持 异构数据是指源数据库和目标数据库不是同一种数据库 例如，从Oralce实时同步数据到MySQL 这是一个常见的异构数据同步场景 如何来验证和修复这个数据同步 于是我开发了这个工具，用来解决这个问题  支持异构数据源对比  支持MySQL&amp;lt;&amp;mdash;-&amp;gt;MySQL 支持Oracle&amp;lt;&amp;mdash;-&amp;gt;MySQL 支持Oracle&amp;lt;&amp;mdash;-&amp;gt;Oracle 支持MySQL&amp;lt;&amp;mdash;-&amp;gt;Oracle  双向数据对比  能比较出源库有没有的数据(生成Detelte语句) 能比较出源库没有，但是目标库有的数据(生成Detelte语句) 能比较出主键相同，但是其他列有差异的数据(生成Update语句)  双向生成修复SQL  可以生成目标库的redo SQL 也可以生成源库的Undo SQL  可配置的时间精度对比  对不同数据的不同时间精度都用同一维度对比(默认精确到分钟:YYYY-MM-DD HH:MM) 对不同精度的小数格式化支持(会去掉0.6000后面的000 )  列默认值支持  通常用于一边是Null，一边是Not Null的默认值  白名单支持  支持对列级别的白名单（这一列不参与对比） 支持对值级别的白名单 (包含)  支持表结构变形后的对比  支持源表和目标表表结构不同（要求主键是唯一的，其他变形在sql可控范围内）  对比速度  可按表级别并发执行，单表对比速度约：1万-3万行/秒  </description>
    </item>
    
    <item>
      <title>数据报告_全球云计算市场(2025年3月)</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%85%A8%E7%90%83%E4%BA%91%E8%AE%A1%E7%AE%97%E5%B8%82%E5%9C%BA%E4%BB%BD%E9%A2%9D/</link>
      <pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%85%A8%E7%90%83%E4%BA%91%E8%AE%A1%E7%AE%97%E5%B8%82%E5%9C%BA%E4%BB%BD%E9%A2%9D/</guid>
      <description>关键要点  研究表明，全球主要云基础设施服务提供商的市场份额数据截至2025年3月显示，AWS占30%，Azure占21%，Google Cloud占12%，Alibaba Cloud占4%，Oracle占3%，SalesForce、IBM Cloud和Tencent Cloud各占2%。 这些数据涵盖平台即服务（PaaS）、基础设施即服务（IaaS）以及托管私有云服务，总计79%，其余21%可能由其他较小提供商持有。 证据倾向于显示云市场竞争激烈，AWS和Microsoft Azure领先，其他公司也在特定领域有增长。  市场份额概述 全球云计算市场由几大主要玩家主导，AWS以30%的市场份额位居第一，其可靠性和安全性广受认可。Microsoft Azure紧随其后，占21%，得益于其与生产力工具的整合。Google Cloud以12%的份额位居第三，特别是在AI和数据分析领域表现强劲。此外，Alibaba Cloud在4%的市场份额中表现出色，尤其在中国市场，而Oracle、SalesForce、IBM Cloud和Tencent Cloud各占2%至3%，服务于特定行业和地区。
当前趋势 云计算市场正在快速发展，AI集成、边缘计算和多云/混合云策略是2025年的主要趋势。这些趋势可能会影响未来市场份额的分布，促使公司投资新技术以保持竞争力。
 详细报告 引言 云计算已经彻底改变了企业和个人存储、管理和处理数据的方式。它提供了可扩展、灵活且成本效益高的解决方案，成为当今数字时代不可或缺的一部分。了解主要云服务提供商的市场份额对于企业和利益相关者制定云策略至关重要。本报告基于2025年3月的市场研究数据，分析全球云基础设施服务提供商的市场份额，包括平台即服务（PaaS）、基础设施即服务（IaaS）以及托管私有云服务。
市场份额数据 以下表格展示了主要云基础设施服务提供商的市场份额：
   服务提供商 市场份额     🇺🇸 亚马逊AWS 30%   🇺🇸 微软Azure 21%   🇺🇸 谷歌云 12%   🇨🇳 阿里云 4%   🇺🇸 甲骨文 3%   🇺🇸 Salesforce 2%   🇺🇸 IBM云 2%   🇨🇳 腾讯云 2%   🇨🇳 华为云 2%   🇨🇳 百度云 1%    注意： 这些提供商的总市场份额为79%，表明其余21%的市场可能由其他较小提供商持有。</description>
    </item>
    
    <item>
      <title>有趣的数据_三国.蜀国五虎将数据盘点</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E4%B8%89%E5%9B%BD.%E8%9C%80%E5%9B%BD%E4%BA%94%E8%99%8E%E5%B0%86%E6%95%B0%E6%8D%AE%E7%9B%98%E7%82%B9/</link>
      <pubDate>Sat, 01 Mar 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E4%B8%89%E5%9B%BD.%E8%9C%80%E5%9B%BD%E4%BA%94%E8%99%8E%E5%B0%86%E6%95%B0%E6%8D%AE%E7%9B%98%E7%82%B9/</guid>
      <description>五虎将基础信息表    武将 字 籍贯 使用武器     关羽 云长 河东解良（今山西运城） 青龙偃月刀   张飞 翼德 幽州涿郡（今河北涿州） 丈八蛇矛   赵云 子龙 常山真定（今河北正定） 龙胆亮银枪   马超 孟起 扶风茂陵（今陕西兴平） 蛇矛   黄忠 汉升 南阳（今河南南阳） 大刀(弓箭)    五虎将官职晋升表    武将 初次任职 最高职位     关羽 偏将军 前将军   张飞 中郎将 右将军   赵云 牙门将军 镇东将军   马超 偏将军 左将军   黄忠 议郎 后将军    五虎将数据 五虎将斩杀敌将表    武将 斩杀人数 被斩杀者     关羽 18 程远志、华雄、管亥、车胄、荀正、颜良、文丑、孔秀、孟坦、韩福、卞喜、王植、秦琪、蔡阳、杨龄、夏侯存、于禁、庞德   张飞 8 邓茂、高升、曹豹、纪灵、吕翔、夏侯兰、夏侯杰、周善   赵云 24 麴义、裴元绍、高览、张武、吕旷、淳于导、夏侯恩、晏明、钟缙、钟绅、邢道荣、刘晙、马汉、慕容烈、焦炳、朱然、金环三结、韩瑛、韩琪、韩琼、韩瑶、韩德、曹遵、苏颙   马超 5 王方、李蒙、李通、马玩、梁兴、杨柏   黄忠 4 邓贤、韩浩、夏侯渊、史迹    五虎将斩杀名将表  这里的“大将”定义是具有较高军事地位或显著个人武力的人物。    武将 斩杀大将数 被斩杀大将     关羽 5 华雄、管亥、颜良、文丑、庞德   张飞 1 纪灵   赵云 2 高览、韩德   马超 0 -   黄忠 1 夏侯渊      五虎将在名场名中的表现评分    武将 高光表现 表现评分（满分10分）     关羽 温酒斩华雄 8   张飞 据水断桥 7   赵云 长坂坡救主 9   马超 潼关之战 6   黄忠 定军山斩夏侯渊 10    总结  赵云:杀将最多，手法最快。云大怒，上前一枪捅死 关羽:名将终结者。吕布领盒饭以后，二爷看谁都是插标卖首尔 黄忠:斩夏侯渊这一项就拉平甚至是追平了其他4虎的差距，夏侯渊级别最高哟！夏侯司令员～。  </description>
    </item>
    
    <item>
      <title>DeepSeek学习资料包整理打包下载</title>
      <link>/ai/deepseek%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E5%8C%85%E6%95%B4%E7%90%86%E6%89%93%E5%8C%85%E4%B8%8B%E8%BD%BD/</link>
      <pubDate>Tue, 18 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>/ai/deepseek%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99%E5%8C%85%E6%95%B4%E7%90%86%E6%89%93%E5%8C%85%E4%B8%8B%E8%BD%BD/</guid>
      <description>在工作和生活中，撰写文案常为刚需。以下是我使用DeepSeek的一些技巧和文案范例，助你高效应对文案需求。
一、DeepSeek使用技巧  关键词设置：精准设置关键词可获取更符合需求的文案结果。 模板功能：利用模板快速构建文案框架，节省时间。  二、Deepseek指令合集  涵盖营销推广、 产品介绍、 活动邀请等多场景，为文案撰写提供灵感参考。 借助这些技巧和范例，大家能更好地运用DeepSeek提升文案撰写能力。  三、视频教程  如何利用国内现有平台做方案创作的视频教程，看看就好   以下这些资料都是网上公开渠道收集整理，非原创。
 ├─DeepSeek使用技巧 │ └─有用的，建议看 │ GenAI：国内外AI细分赛道与产品分析（198页）.pdf │ GenAI：国内外AI细分赛道与产品分析（198页）.pptx │ 浙江大学：2025年DeepSeek行业应用案例集解锁智能变革密码（153页PPT）.pdf │ 入门教程：DeepSeek 15天指导手册（25页word).pdf │ 北京大学：DeepSeek系列-DeepSeek与AIGC应用（99页PPT）.pdf │ 北京大学：DeepSeek系列-提示词工程和落地场景（86页PPT）.pdf │ 尚硅谷教育：deepseek-r1论文-中文翻译版（14页word).pdf │ 清华大学第三版：DeepSeek+DeepResearch：让科研像聊天一样简单（86页PPT）.pdf │ 清华大学第二版：DeepSeek赋能职场（36页PPT）.pdf │ 清华大学第一版：DeepSeek：从入门到精通（104页PPT）.pdf │ 尚硅谷教育：第三方平台-硅基流动部署DeepSeek R1（10页word）.pdf │ 尚硅谷教育：本地算力部署DeepSeek详细流程（19页word）.pdf │ 清华大学第四版：普通人如何抓住DeepSeek红利（65页PPT）.pdf │ 厦门大学：大模型概念、技术与应用实践（140页PPT）.pdf │ └─没啥用的，随便看看 │ 1000个DeepSeek神级提示词，让你轻松驾驭AI.docx │ 3个DeepSeek隐藏玩法，99%的人都不知道！.docx │ deepseek 应该怎样提问.docx │ Deepseek 高效使用指南.docx │ Deepseek不好用，是你真的不会用啊！.docx │ DeepSeek小白使用指南，99% 的人都不知道的使用技巧（建议收藏）docx.docx │ DeepSeek最强使用攻略，放弃复杂提示词，直接提问效果反而更好？.</description>
    </item>
    
    <item>
      <title>有趣的数据_婚恋APP数据盘点</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%A9%9A%E6%81%8Bapp%E6%95%B0%E6%8D%AE%E7%9B%98%E7%82%B9/</link>
      <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%A9%9A%E6%81%8Bapp%E6%95%B0%E6%8D%AE%E7%9B%98%E7%82%B9/</guid>
      <description>近年来，约会和婚恋方式发生了巨大的变化，尤其是在在线平台的影响下。让我们通过一些关键数据来分析当前婚恋APP的普及率以及中国市场的主要平台情况。
全球婚恋APP普及率 根据Statista Market Insights的数据，婚恋APP的普及率因国家而异，主要受人口规模与文化差异的影响。以下是一些主要国家的婚恋APP普及率：
   国家 普及率     🇺🇸 美国 17.7%   🇫🇷 法国 11.2%   🇰🇷 韩国 10.8%   🇩🇪 德国 9.0%   🇧🇷 巴西 7.8%   🇬🇧 英国 6.3%   🇨🇳 中国 5.8%   🇮🇳 印度 1.9%    从数据来看，美国和法国的普及率显著高于其他国家，尤其是美国，婚恋APP的使用已经成为许多人日常生活的一部分。相比之下，中国的普及率为5.8%，虽然相对较低，但市场规模仍然庞大。
中国婚恋市场概况 在中国，婚恋APP的市场竞争激烈，主要平台占据了大部分市场份额。以下是一些重要平台的市场占有率概况：
   APP 市场份额(预估%)     百合佳缘 37%   珍爱网 22%   有缘网 12%   伊对 6%   对缘 5%   青藤之恋 4%   一线姻缘 3%   钻石婚恋 2%   MarryU 3%    可以看到，百合佳缘与珍爱网占据了市场的绝大部分份额，三大平台合计市场占有率接近70%。这些平台的主打特点包括大规模的用户群体与完善的线上线下服务，而一些新兴平台则通过差异化策略赢得了年轻群体的青睐。</description>
    </item>
    
    <item>
      <title>2000元就能本地部署AI大模型？7种DeepSeek配置对比</title>
      <link>/ai/deepseek%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%AE%89%E8%A3%85%E4%BD%A0%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B/</link>
      <pubDate>Fri, 07 Feb 2025 00:00:00 +0000</pubDate>
      
      <guid>/ai/deepseek%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%AE%89%E8%A3%85%E4%BD%A0%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B/</guid>
      <description>《DeepSeek R1的低成本奇迹——557万美元如何改变AI行业？》
 DeepSeek火出圈的原因有两个：一是强大的推理能力，二是廉价的成本 目前为止DeepSeek是最省显卡的AI模型 随着官方的DeepSeek服务器被用户挤爆。 经常会遇到：服务器繁忙，请稍后再试。严重影响使用 这时候就会有些聪明人想：能不能在自己的电脑上安装一个私人的DeepSeek，只给自己提供访问 答案是：可以的 那多少钱的电脑可以跑动一个私有的AI大模型呢 答案是：最低配置是2000块钱的台式电脑就可以！  访问DeepSeek的几种方法 方法1: 访问DeepSeek官网 通过DeepSeek的官方网站，用户可以在线使用DeepSeek模型进行各种任务，无需安装任何本地软件。
方法2: 下载DeepSeek手机APP 用户可以通过手机APP访问DeepSeek，随时随地使用模型进行任务。
方法3: 本地安装DeepSeek服务 用户可以在本地计算机上安装DeepSeek服务，直接在本地使用模型进行计算和任务处理。
三种访问方式的对比：    特征 访问DeepSeek官网 手机APP 本地DeepSeek     成本 免费 免费 需要硬件资源和安装费用   易用性 容易 容易 较难   数据存储 云端 云端 本地   隐私保护 取决于厂商 取决于厂商 最大隐私保护   更新与维护 自动更新 自动更新 需手动更新   计算资源 依赖厂商 依赖厂商 取决于本地硬件配置     方法1（访问官网）：日常办公或在家有电脑 方法2（手机APP）： 拿着手机出门移动办公或在家使用。 方法3（本地安装服务）：适合需要处理敏感数据或高性能需求的用户，但需要一定的技术知识，并且受限于本地硬件的性能和更新维护。  哪些职业的人需要部署本地AI 在涉及到敏感数据和隐私保护的职业中，很多从业人员更倾向于使用本地部署的AI系统，以确保数据安全。以下是一些更适合访问本地AI的职业：</description>
    </item>
    
    <item>
      <title>2024年终总结:时间过得真快</title>
      <link>/book/2024%E5%B9%B4%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/2024%E5%B9%B4%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</guid>
      <description>时间  2025年01月28 农历腊二十九除夕 今年的过年好早 过完了元旦没几天 春节就这样来了 时间过得太快了 这一年我总是有这种感觉 时间过得好快啊 听过一个说法 年龄越大的人 越会感觉时间过得快 春去秋来 花开花谢 这世间万物在不断的轮回 而我手里攥着的时间 正悄悄从指缝间溜走 舍不得 生命的意义是什么 活着的价值是什么 老实说 我不知道 也不觉得能给世界带来什么影响 就这么平淡的活着 经历着一点点的小事 努力让自己活得舒服点开心点 好像就很满足了 早上醒来尝试问了三个问题 这一年过得累吗？ 不累 这一年过得开心吗？ 蛮开心的 这一年成长了吗？ 成长了 所以我可以很认真的说： 过去的一年时间里 是很放松满足的一年 充实紧凑且有意义  健康  过去的一年 最值得炫耀的事：健康 练出来了马甲线 背部的肌肉正在加厚 各项运动指标都在提升 五公里跑步最好成绩：23分钟 二十公里跑步最好成绩：110分钟 二十公里骑行最好成绩：54分钟 三峰环穿最好成绩：390分钟 徒步登山：327公里 力量训练：4041分钟 一串冰冷的数字后面 是一颗斗志昂扬的心脏 是克服拖延症的决心 是拖着疲惫的身体继续的毅力 也是挥洒汗水后的释然 也是站在山顶看风景时的 身体更轻盈，精力更旺盛 甚至对生活的掌控感会也大大增强 喜欢沿途的风景 也喜欢正在努力的自己  工作  坦率的说 过去一年里我的本职工作 我自己是不满意的 虽然我们用了更少的人 高标准的完成了现有的工作 还凭着一已之力 完成了公司下游数据加密 可能放在别的团队是件很大的项目 但是扪心自问 还是可以做得更好 并没有把100%的精力投入工作中 倒也不是去摸鱼贪玩了 分了一些精力在做ai相关的方向 这一年里 不停的在尝试整合 想在ai方向有所突破 在公司里也尝试了智能客服 智能报单等试验 结果都不好 反而是自己用ai提高自己的工作效率 搞得风生水起的 但只限于自己 做不出来一个像样的产品 也找不到用ai对工作上有用的方向 有一点是肯定的 在ai这个不属于我工作领域范畴内的事情上 投入了不少的精力 只是没啥产出  生活  这一年的生活非常自律 做自己认为对的事 改变一些不好的习惯 很多方面都做得不错 也在努力修复自己的“低情商” 对家人温和一些 对朋友大方一些 及时的对周围的人表达谢意 对同事不要太苛刻 主动合群一些 老张年纪越来越大 虽然还是经常担心他的身体 好在这一年都没啥大事 回去了两趟 看起来他的生活有些乱 我也不能说什么 他说习惯了老家的生活 大城市生活不习惯 这个问题暂时还没有解决的方法 也是最困扰我的问题 中年男人的压力 除掉应对自己不再年轻 还需要考虑家庭 上有老，下有小 不至是说说而已 压力是实在的 这一年去了大同 乌兰察布 无锡 上海 常州 保定 雄安 旅行对我来说越来越不重要 美好的风景也都看过了 也经历过了 简单的生活 就足够美好  结语  18:10 外面稀稀拉拉的有点鞭炮声 一会春晚的倒计时就要开始了 站在年岁的分水岭上 用汗水分辨晨昏 以脚步丈量年轮 在这个除夕 回首看这集讲过去的一年时间 把时间印在登山路上的石阶上 把时间留在晨跑呼出的白气里 把时间藏在火山口裹紧的雨衣下 把时间挂在海边那顶孤独的帐篷旁 时间是生命的秒表 滴答声中 我们走过了又一年 我们老了一岁 我们也成长了很多 这一年 我们没有辜负时间 这一年过得不错  2025-01-28 北京.</description>
    </item>
    
    <item>
      <title>中美芯片博弈之三：中国的AI芯片公司盘点</title>
      <link>/book/%E4%B8%AD%E7%BE%8E%E8%8A%AF%E7%89%87%E5%8D%9A%E5%BC%88%E4%B9%8B%E4%B8%89%E4%B8%AD%E5%9B%BD%E7%9A%84ai%E8%8A%AF%E7%89%87%E5%85%AC%E5%8F%B8%E7%9B%98%E7%82%B9/</link>
      <pubDate>Fri, 24 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E4%B8%AD%E7%BE%8E%E8%8A%AF%E7%89%87%E5%8D%9A%E5%BC%88%E4%B9%8B%E4%B8%89%E4%B8%AD%E5%9B%BD%E7%9A%84ai%E8%8A%AF%E7%89%87%E5%85%AC%E5%8F%B8%E7%9B%98%E7%82%B9/</guid>
      <description>在中美芯片博弈的背景下，中国AI芯片公司正逐渐崭露头角，成为全球芯片产业的重要力量。本文将盘点国内顶尖的AI芯片厂商，分析它们的市场表现和技术实力，展望中国芯片产业的未来。
AI芯片厂商上市情况    企业 上市进度     海光信息 已上市   寒武纪 已上市   景嘉微 已上市   壁仞科技 上市准备中   天数智芯 上市准备中   摩尔线程 上市准备中   华为 未上市   燧原科技 未上市    AI芯片厂商主要产品    企业 主要GPU产品     华为 Ascend系列（如Ascend 910B、910C）   海光信息 DCU100   寒武纪 MLU系列（如MLU370、MLU590）   景嘉微 JM系列（如JM9230）   壁仞科技 BR系列（如BR100、BR104）   燧原科技 云燧T20/T21训练卡，云燧i20推理卡   天数智芯 天垓100   摩尔线程 MTT系列（如S80、S4000）    1.</description>
    </item>
    
    <item>
      <title>有趣的数据_各地区资源排名(五):危险固体废物</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E4%BA%94%E5%8D%B1%E9%99%A9%E5%9B%BA%E4%BD%93%E5%BA%9F%E7%89%A9/</link>
      <pubDate>Fri, 17 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E4%BA%94%E5%8D%B1%E9%99%A9%E5%9B%BA%E4%BD%93%E5%BA%9F%E7%89%A9/</guid>
      <description>关键要点  研究显示，部分地区危险固体废物的处理量超过产生量，可能涉及历史积存废物的处理。 证据倾向于显示，工业结构和经济水平可能影响废物处理能力，部分重工业地区处理能力不足。 数据显示，云南处理量远超产生量（约3.19倍），而青海处理量仅为产生量的27%，反映区域差异显著。  数据概览 危险固体废物是指因毒性、易燃性、腐蚀性或反应性可能危害人类健康和环境的废物。提供的表格列出了中国各地区的产生量和处理量（单位：万吨），处理量包括利用和处置。
   地区 产生量 废物利用处置量     北京 24.97 27.01   天津 63.70 63.79   河北 357.46 369.69   山西 213.98 225.36   内蒙古 540.58 495.87   辽宁 137.53 134.81   吉林 197.02 169.95   黑龙江 118.54 126.56   上海 131.91 133.41   江苏 522.05 607.04   浙江 444.79 464.</description>
    </item>
    
    <item>
      <title>有趣的数据_各地区资源排名(四):工业固体废物</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E5%9B%9B%E5%B7%A5%E4%B8%9A%E5%9B%BA%E4%BD%93%E5%BA%9F%E7%89%A9/</link>
      <pubDate>Thu, 16 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E5%9B%9B%E5%B7%A5%E4%B8%9A%E5%9B%BA%E4%BD%93%E5%BA%9F%E7%89%A9/</guid>
      <description>关键要点  工业固体废物管理涉及生产、综合利用、处置和倾倒，数据显示各地区处理方式差异显著。 研究表明，部分地区如天津综合利用率高（约99.5%），而如河北生产量大但部分废物可能未处理。 证据倾向于显示，废物管理与经济发展和政策相关，倾倒量通常较低但不容忽视。  工业固体废物的定义与重要性 工业固体废物是指工业生产过程中产生的固态废弃物，如钢铁生产的炉渣或发电厂的飞灰。妥善管理这些废物至关重要，因为它们可能对环境和公众健康造成威胁，如土壤污染或水源污染。同时，通过综合利用（如在水泥生产中再利用飞灰），可以减少资源浪费，降低环境影响。
数据概览与地区差异 根据提供的数据，各地区工业固体废物的生产和处理量差异明显。例如，山西和内蒙古的生产量分别高达42635和35117万吨，显示出重工业发达地区的废物输出大。而天津的综合利用率接近99.5%，说明其废物再利用效率极高。相比之下，河北生产量34081万吨，但综合利用和处置后仍有3802万吨未明确处理，可能存储或在途，反映管理挑战。
分析与启示 数据表明，经济发达地区如天津和上海往往利用率高，可能得益于技术先进和政策支持。而生产量大的地区如山西和河北，处置量占比较高，可能是由于废物种类复杂，综合利用难度较大。倾倒量总体较低，如新疆为5.00万吨，但仍需关注非法倾倒对环境的影响。未来，政策可聚焦提升利用率，减少存储废物，推广先进技术如废物资源化。
 详细分析：工业固体废物管理现状与地区特征 工业固体废物是工业生产中不可避免的副产品，包括但不限于冶金、化工、电力等行业的固态废弃物，如炉渣、粉煤灰和工业污泥。这些废物的管理不仅关系到资源的高效利用，还直接影响生态环境和公众健康。例如，未妥善处理的废物可能导致土壤重金属污染，威胁地下水安全。因此，科学管理工业固体废物已成为环境治理的重要议题。
数据来源与分类 提供的数据详细列出了各地区的工业固体废物生产量、综合利用量、处置量和倾倒丢弃量，单位为万吨。以下是部分关键数据，完整表如下：
   地区 产生量 综合利用量 处置量 倾倒丢弃量     北京 415 193 223 0.00   天津 1739 1731 6 0.02   河北 34081 18880 11399 0   山西 42635 17150 19546 92.00   内蒙古 35117 12377 13711 1.73   辽宁 25526 11478 7942 10.51   吉林 4676 2407 1512 0.</description>
    </item>
    
    <item>
      <title>有趣的数据_各地区资源排名(三):猪牛羊肉产量</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E4%B8%89%E7%8C%AA%E7%89%9B%E7%BE%8A%E8%82%89%E4%BA%A7%E9%87%8F%E7%9C%81%E4%BB%BD%E6%8E%92%E5%90%8D/</link>
      <pubDate>Tue, 14 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E4%B8%89%E7%8C%AA%E7%89%9B%E7%BE%8A%E8%82%89%E4%BA%A7%E9%87%8F%E7%9C%81%E4%BB%BD%E6%8E%92%E5%90%8D/</guid>
      <description>关键要点  研究表明，内蒙古的人均猪牛羊肉产量最高，为104公斤/人，可能是由于其广阔的草原适合放牧。 证据倾向于认为，上海和北京等城市化省份人均产量低（如上海为3.1公斤/人），主要依赖进口。 地理、经济和文化因素似乎对各省的肉类生产有显著影响，存在区域差异。  引言 中国各省的人均猪牛羊肉产量差异显著，反映了农业、经济和文化等多方面的影响。本文将分析这些差异，探讨高产和低产省份的特点，并讨论其对食品安全和经济发展的意义。
数据分析 以下是全国省级人均猪牛羊肉产量的详细数据，单位为公斤/人：
   地区 猪牛羊肉公斤/人     内蒙古 104   云南 90   吉林 80   黑龙江 79.5   西藏 73.6   湖南 72.6   青海 66.6   辽宁 65.3   四川 62.6   新疆 61.3   湖北 59.3   江西 57.1   广西 52.4   贵州 50.</description>
    </item>
    
    <item>
      <title>有趣的数据_各地区资源排名(二):用水情况</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E4%BA%8C%E7%94%A8%E6%B0%B4%E6%83%85%E5%86%B5/</link>
      <pubDate>Mon, 13 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E4%BA%8C%E7%94%A8%E6%B0%B4%E6%83%85%E5%86%B5/</guid>
      <description>关键要点  研究表明，农业用水在新疆和黑龙江等地区占主导，工业用水在上海和江苏等城市化地区显著。 证据倾向于认为，人口稀疏地区如新疆人均用水量高（2215.6立方米/人·年），而北京（185.4立方米/人·年）较低。 意外细节：北京的生活用水和人工生态用水均达17.2，显示城市绿化需求。  用水总量 中国各地区用水总量差异大，新疆最高（570.4亿立方米），青海最低（24.3亿立方米）。这反映了地理和经济条件的多样性。
各行业分布 农业用水在新疆（496.2亿立方米）和黑龙江（278.4亿立方米）占主导，工业用水在上海（57.9亿立方米）和江苏（236.9亿立方米）显著，城市如北京生活用水和人工生态用水均衡。
人均用水量 人均用水量从北京的185.4立方米/人·年到新疆的2215.6立方米/人·年不等，人口稀疏地区用水量高，城市化地区相对低。
 中国各地区用水详细分析 这篇详细分析探讨了中国31个地区的用水数据，关注总量、各行业分布（农业、工业、生活、人工生态）和人均用水量，总量单位可能为亿立方米，人均单位为立方米/人·年。目标是理解区域差异、原因及其对水管理、可持续性和经济发展的影响。
数据概览与初步观察 表格列出了各地区用水情况，总量从青海的24.3到新疆的570.4不等。构成包括农业、工业、生活和人工生态用水，人均用水量从北京的185.4到新疆的2215.6。例如，北京总量40.6，其中农业3.2，工业3.0，生活17.2，人工生态17.2，人均用水185.4。上海总量97.5，工业（57.9）和生活（23.6）占主导，农业（15.2）和人工生态（0.8）较少，人均用水392.4。
以下是完整表格供参考：
   地区 用水总量 农业用水量 工业用水量 生活用水量 人工生态用水量 人均用水量     北京 40.6 3.2 3.0 17.2 17.2 185.4   天津 27.8 10.3 4.5 6.6 6.4 200.6   河北 182.8 107.7 18.2 27.0 29.9 245.2   山西 72.8 41.0 12.4 14.6 4.8 208.4   内蒙古 194.4 140.</description>
    </item>
    
    <item>
      <title>有趣的数据_各地区资源排名(一):供水情况</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E4%B8%80%E4%BE%9B%E6%B0%B4%E6%83%85%E5%86%B5/</link>
      <pubDate>Sun, 12 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%90%84%E5%9C%B0%E5%8C%BA%E8%B5%84%E6%BA%90%E6%8E%92%E5%90%8D%E4%B8%80%E4%BE%9B%E6%B0%B4%E6%83%85%E5%86%B5/</guid>
      <description>关键要点  研究表明，南方地区如广东和湖南主要依赖地表水，北方地区如河北和河南则更多使用地下水。 证据倾向于认为，水资源分布存在南北差异，南方水量充足，北方需通过南水北调等项目补充。 一些城市如北京和天津的“其他供水”包括再生水和跨区域调水，显示水资源紧张的应对措施。  供水总量分布 中国各地区的供水总量差异显著，江苏和新疆最高，分别为572.0和570.4（单位为亿立方米），而青海和西藏最低，仅24.3和32.2。这反映了地理和经济条件的多样性，南方水系发达，北方则受干旱限制。
供水构成分析 南方省份如广东（地表水390.4/405.1）依赖河流，北方如河北（地下水88.2/182.8）靠地下水。北京和天津的“其他供水”（分别为12.0和5.6）包括南水北调工程，显示城市化地区的水资源压力。
意外细节：其他供水的意义 北京和天津的“其他供水”占比高（北京近30%），这表明在水资源短缺下，跨区域调水和再生水的使用变得至关重要，可能是未来水管理的重要方向。
 调查笔记：中国各地区供水详细分析 这篇详细分析探讨了中国31个地区的供水数据，关注总量、地表水、地下水和其他水源，单位为亿立方米。目标是理解区域差异、原因及其对水管理、可持续性和经济发展的影响。
数据概览与初步观察 表格列出了各地区的供水情况，总量从青海的24.3到江苏的572.0不等。构成包括地表水、地下水和其他水源，反映了不同的水管理策略。例如，北京总量40.6，其中地表水15.1，地下水13.5，其他水源12.0。而上海总量97.5，几乎全为地表水（97.4），地下水为0，其他水源仅0.1。
以下是完整表格供参考（单位是亿立方米）：
   地区 供水总量 地表水供水量 地下水供水量 其他供水量     北京 40.6 15.1 13.5 12.0   天津 27.8 19.2 3.0 5.6   河北 182.8 84.8 88.2 9.8   山西 72.8 39.5 27.7 5.5   内蒙古 194.4 105.7 81.6 7.1   辽宁 129.3 72.9 50.8 5.7   吉林 117.</description>
    </item>
    
    <item>
      <title>有趣的数据_空中力量排行2025</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E7%A9%BA%E4%B8%AD%E5%8A%9B%E9%87%8F%E6%8E%92%E8%A1%8C2025/</link>
      <pubDate>Fri, 10 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E7%A9%BA%E4%B8%AD%E5%8A%9B%E9%87%8F%E6%8E%92%E8%A1%8C2025/</guid>
      <description>有趣的数据：全球空中战斗力排行(2025) 引言 上一期我们发布了桥水基金评估的超级大国指数，收到了一些反馈，有兴趣的参看《{链接url}》。
 这一期我们再来看看WDMMA（World Directory of Modern Military Aircraft）发布的全球空中战斗力排名。 与上期不同的是，WDMMA将美国的几个军事单位（如空军、海军、陆军等）拆分开来计算战斗力，类似于我们在讨论钢铁产量时会将河北单独列出来进行比较。这种拆分方式让我们能够更细致地了解各国空中力量的构成和分布。  全球空中战斗力排行 以下是WDMMA发布的2025年全球空中战斗力排名： 从表中可以看出，美国的空中力量占据了绝对优势，尤其是美国空军以242.9的得分和5,004架飞机的规模稳居榜首。值得注意的是，美国海军、陆军和海军陆战队也分别位列第二、第四和第五，显示出美国在空中力量上的全面领先。
按国家维度的空中战斗力排行表 为了更直观地比较各国的空中战斗力，我们使用AI将WDMMA的数据按国家维度重新整合，例如
 美国：美国空军 (242.9) + 美国海军 (142.4) + 美国陆军 (112.6) + 美国海军陆战队 (85.3) = 582.2 分，飞机架数为 5,004 + 2,504 + 4,333 + 1,211 = 13,052架。 中国：中国空军 (63.8) + 中国海军航空兵 (49.3) + 中国陆军航空兵 (31.3) = 144.4 分，飞机架数为 3,733 + 436 + 1,188 = 5,357架。  得到了以下排名：
   国家 综合得分 综合飞机架数     🇺🇸 美国 582.</description>
    </item>
    
    <item>
      <title>英伟达黄仁勋20250107CES主题演讲（全文）</title>
      <link>/ai/%E8%8B%B1%E4%BC%9F%E8%BE%BE%E9%BB%84%E4%BB%81%E5%8B%8B20250107ces%E4%B8%BB%E9%A2%98%E6%BC%94%E8%AE%B2%E5%85%A8%E6%96%87/</link>
      <pubDate>Tue, 07 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%8B%B1%E4%BC%9F%E8%BE%BE%E9%BB%84%E4%BB%81%E5%8B%8B20250107ces%E4%B8%BB%E9%A2%98%E6%BC%94%E8%AE%B2%E5%85%A8%E6%96%87/</guid>
      <description>北京时间1月7日（周二）上午，英伟达创始人兼CEO黄仁勋在拉斯维加斯举行的CES 2025大会上发表开幕主题演讲。 现场，黄仁勋宣布推出包括售价高达1999美元的RTX 5090显卡在内的一系列全新产品。黄仁勋称，这款新显卡将成为英伟达游戏芯片业务的支柱。 除了RTX50系列显卡外，黄仁勋还发布了多款“王炸”，包括语言基础模型Llama Nemotron和世界基础模型Cosmos。开放式Llama Nemotron大型语言模型和Cosmos视觉语言模型可在任何加速系统上为AI代理提供超级动力。 同时，黄仁勋预告，将于5月推出一款名为Project Digits的个人AI超级计算机，其核心是新的GB10 Grace Blackwell超级芯片，它具有足够的处理能力来运行复杂的AI模型，仅需标准电源插座即可运行。 此外，黄仁勋宣布推出下一代汽车处理器Thor，这是一款革命性的机器人计算机，旨在处理大量传感器数据。他还表示，丰田和英伟达将合作开发下一代自动驾驶汽车。
 欢迎来到CES！ 你们兴奋地来到拉斯维加斯吗？ 你们觉得我的夹克怎么样？ 我想我可以和Shapiro（CES总裁）的风格有所不同。 毕竟我在拉斯维加斯。 如果你们都不喜欢，那就忍着吧。 我真的觉得你们需要消化一下。 因为再过1个小时左右，我会让你们满意。
欢迎来到NVIDIA，我们将带您领略NVIDIA的世界。 女士们、先生们，欢迎来到NVIDIA。
这里的一切都是由人工智能生成的。 这是一个非凡的旅程，也是一个非凡的年份，始于1993年。 通过NV1，我们希望构建能够执行普通计算机无法完成的任务的计算机。 NV1使得在您的个人电脑上拥有成为游戏主机的能力。
我们的编程架构名为UDA，直到不久之后才有了字母C，但UDA，统一设备架构。第一个为UDA开发的应用程序是世嘉的《VR战士》。六年后，在1999年我们发明了可编程GPU。这开启了超过20年的惊人进步，GPU这一神奇的处理器让现代计算机图形成为可能。现在，30年后，世嘉的《VR战士》已经完全电影化。这是即将推出的新《VR战士》项目。我迫不及待地想看到它，绝对令人难以置信。六年之后，UDA诞生了。
在1999年后的六年里，我们发明了CUDA，以便能够向一系列能够从中受益的算法解释或表达我们GPU的可编程性。最初，CUDA很难解释，实际上花费了多年时间，大约六年。无论如何，大约六年后，也就是在2012年，Alex Kershevsky、Ilya Suskovor和Jeff Hinton发现了CUDA，并用它来处理AlexNet（一个卷积神经网络），接下来的发展便成为了历史。从那时起，人工智能以惊人的速度发展。起初是感知AI，现在我们能够理解图像、文字和声音，走向生成式AI。
现在我们谈论的是代理性AI，也就是那些能够感知、推理、计划并采取行动的AI。
接下来是下一阶段，即物理AI，这也是我们今晚将讨论的一部分，它始于2012年。然后在2018年，神奇的一年里，发生了一件令人难以置信的事情。谷歌发布了Transformer模型的BERT，自此AI领域真正迎来了腾飞。正如大家所知，Transformers彻底改变了人工智能的格局。事实上，它甚至彻底改变了整个计算领域的格局。我们深刻认识到，AI不仅仅是一个新应用领域或新的商业机会，更重要的是，由Transformers驱动的机器学习，将从根本上改变方式。
计算在每一个层面上都发生了革命性变化，从手动编码指令以运行在CPU上的软件工具，到我们现在拥有的可以创建和优化神经网络并在GPU上进行处理的机器学习，从而生成人工智能。技术堆栈的每一个层面都已经完全改变，这在过去12年中是一次令人难以置信的变革。现在，我们能够理解几乎任何形式的信息。你们肯定见过文本、图像和声音等内容，但我们不仅能理解这些，我们还能理解氨基酸，理解物理。我们不仅理解它们，还能够翻译和生成它们。
应用的可能性几乎是无穷无尽的。事实上，市场上几乎所有的AI应用，都是通过什么样的输入模态学习而来的？它又把什么样的信息模态转换成了什么，最终生成了什么样的信息模态？如果你问这三个基本问题，几乎每个应用都可以得到推断。因此，当你看到一个又一个以AI驱动、以AI为本质的应用时，这一基本概念必然存在。机器学习改变了每一个应用的构建方式，改变了计算的方式，以及超越这些的可能性。正如GeForce GPU，在很多方面，所有这些与AI相关的成就都是GeForce所建立的。GeForce使得AI能够惠及大众。那么，现在呢？人工智能回归GeForce。 许多事情没有人工智能是无法完成的。现在让我给你展示其中的一部分。
……（宣传片）
没有任何计算机图形研究者或计算机科学家会告诉你这是可能的。
我们可以为每一个像素进行光线追踪。光线追踪是光的模拟。你看到的几何形状数量绝对惊人。如果没有人工智能，这一切都是不可能的。我们做了两件根本性的事情。我们当然使用了可编程着色和光线追踪加速，产生了极其美丽的像素。然后，我们让人工智能根据那个像素进行调节和控制，以生成大量其他像素。它不仅能够在空间上生成其他像素，因为它知道颜色应该是什么，而且它是在 NVIDIA 的超级计算机上进行训练的。因此，运行在GPU上的神经网络能够推断和预测我们没有渲染的像素。不仅如此，这被称为DLSS（超分辨率技术）。
最新一代的DLSS不仅仅是生成帧。它还能预测未来，为每一帧计算生成三个额外的帧。你看到的，如果我们只是说你看到的四帧，因为我们将渲染一帧并生成三帧。如果我说四帧在全高清和4K下，那就是大约3300万个像素。在这3300万个像素中，我们只计算了两个。能够计算出200万个像素，并让人工智能预测其余的3300万个像素，简直是个绝对的奇迹。结果，我们能够以令人难以置信的高性能进行渲染，因为人工智能的计算量大大减少。
当然，这需要大量的训练才能实现，但一旦训练完成，生成过程极其高效。因此，这是人工智能的一项令人难以置信的功能，这也是为什么有如此多惊人的事情正在发生。我们利用GeForce推动人工智能的发展，而如今人工智能又在革命GeForce。
今天我们宣布我们的下一代产品，RTX Blackwell系列。让我们来看看。
这是我们最新基于Blackwell架构的全新GeForce RTX 50系列显卡。这是一款性能野兽，搭载920亿个晶体管，4000 AI TOPS，是以前Ada的3倍。这都是我刚刚展示的画面所必需的硬件。
我们拥有380个光线追踪的太浮点运算能力，以便为我们必须计算的像素提供尽可能美丽的图像。当然，还有125个着色器的太拉浮点运算能力。实际上，除了并行着色器的泰拉浮点运算能力之外，还有一个整数单元，性能相等。因此，有两个双重着色器，一个用于浮点运算，另一个用于整数运算。来自美光的G7内存，速度为每秒1.8TB，是我们上一代产品性能的两倍。
我们现在有能力将AI工作负载与计算图形工作负载混合处理。这一代产品最令人惊讶的地方是，可编程着色器现在也能处理神经网络。因此，这个着色器能够承载这些神经网络，从而我们发明了神经纹理压缩和神经材料着色。由此产生的，是无法仅靠传统技术实现的令人惊叹的美丽图像，因为我们运用了AI技术学习纹理，学习压缩算法，最终获得卓越的结果。
好的，这就是全新的RTX Blackwell 50系列。甚至连机械设计都是一个奇迹。看看这个，它有两个风扇。整个显卡就像是一个巨大的风扇。那么问题来了，显卡在哪里？它真的这么大吗？电压调节器的设计达到了最先进水平。令人难以置信的设计。工程团队做得非常出色。
那它和之前的相比如何呢？这就是RTX 4090，价格是1599美元。这是你能做出的最好的投资之一。花1599美元，你可以把它带回家，搭配你的10000美元的PC娱乐指挥中心。不是吗？别告诉我这不是真的。它是液冷的，四处都有华丽的灯光。如果说这就是现代家庭影院完全说得通。
而现在，花1500美元到1599美元，你就可以对它进行升级，并为它注入强大的动力。现在，随着Blackwell家族的到来，RTX 5070拥有4090的性能，仅售549美元。5090的性能是4090的两倍。我们将于1月开始大规模生产，当然，产品将很快上市。
这是令人难以置信的，但我们成功地将这些巨大的高性能GPU放入了一台笔记本电脑。这是一台5070笔记本电脑。价格为1299美元的5070笔记本具有4090的性能。我想这里有一个。让我给你展示一下。你能想象吗，你拥有这个令人惊叹的显卡，Blackwell，我们将把它缩小并放入笔记本中，不利用我们的人工智能你是做不到的。原因是我们通过我们的核心生成大部分像素。因此，我们只追踪所需的像素，其他像素则由人工智能生成。结果是，能效简直是天文数字。计算机图形的未来是神经渲染，人工智能的融合。
5090显卡将能够融入到一款薄型笔记本电脑中，那款电脑厚度在14.9毫米。我们还有5080、5070 Ti和5070。那么，女士们，先生们，这就是RTX Blackwell系列。
GeForce将人工智能带入了世界，实现了人工智能的普及，而如今人工智能又回过头来彻底改变了GeForce。
让我们谈谈人工智能，接下来我们来到NVIDIA的另一处地方。这确实是NVIDIA的总部。好吧，让我们深入讨论一下人工智能行业。整个行业一直在追逐和竞相扩展人工智能，而扩展法则则是一个强大的模型，它是一个经验法则，已经在多个世代的研究者和行业中被观察和证明。扩展法则表明，你拥有的训练数据越多，模型越大，应用的计算能力越强，因此你的模型将变得更加有效或更强大。因此，扩展法则依然适用。令人惊奇的是，我们现在正在迈向新的阶段。
当然，互联网每年产生的数据量几乎是去年的两倍。我认为在未来几年内，人类将产生的数据量将超过人类自古以来所产生的总数据。因此，我们依旧在产生海量数据，而且这些数据正变得多模态。视频、图像和声音，这些数据都可以用于训练人工智能的基础知识和基本知识。但实际上，现在出现了另外两个扩展法则，这些法则也颇具直观性。第二个扩展法则是后训练扩展法则。后训练扩展法则利用强化学习、人类反馈等技术和方法。基本上，人工智能会生成和产生答案。
基于人类查询，而人类给出反馈。虽然这要复杂得多，但这个强化学习系统通过大量高质量的提示使得人工智能不断提升其技能。它可以在特定领域进行技能微调，能更好地解决数学问题，更好地进行推理，等等。因此，这本质上就像是有一个导师或教练在你学习结束后给予反馈。你会接受测试，得到反馈，不断改善自己。我们还有强化学习的人工智能反馈和合成数据生成。这些技术就类似于自我练习。你知道某一特定问题的答案，并不断尝试直到获得正确答案。</description>
    </item>
    
    <item>
      <title>有趣的数据_城市排名</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%9B%BD%E5%86%85%E5%9F%8E%E5%B8%82%E6%8E%92%E5%90%8D/</link>
      <pubDate>Fri, 03 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E5%9B%BD%E5%86%85%E5%9F%8E%E5%B8%82%E6%8E%92%E5%90%8D/</guid>
      <description>中国城市快递业务收入前50名分析报告 年快递业务收入城市排名前50名    序号 城 收入（亿元）     1 上海 1212.39   2 广州 595.94   3 深圳 476.25   4 杭州 294.28   5 北京 233.26   6 金华 225.67   7 东莞 204.43   8 苏州 176.27   9 揭阳 120.16   10 成都 120.14   11 佛山 115.21   12 武汉 104.2   13 天津 100.</description>
    </item>
    
    <item>
      <title>做数据同步程序过程的一些零散的笔记</title>
      <link>/book/%E5%81%9A%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%A8%8B%E5%BA%8F%E8%BF%87%E7%A8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Mon, 11 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%81%9A%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E7%A8%8B%E5%BA%8F%E8%BF%87%E7%A8%8B%E7%9A%84%E4%B8%80%E4%BA%9B%E8%AE%B0%E5%BD%95/</guid>
      <description> 用python写了一个数据同步工作，过程零散的笔记，记录一下
 不要低估硬件的影响   因为中间硬件资源调整不开，运维同事给了一个sata硬盘凑数，结果慢到另人发指
  数据同步在不同硬盘下的实际运行速度和磁盘io性能：
  sas机械盘 iops：2k, 写入20M/s， io操作占比40%，写入速度1万行/秒
  sata机械盘	iops：0.7k, 写入10M/s， io操作占比100%，写入速度0.4万行/秒
  ssd盘	iops：22k, 写入262M/s， io操作占比70%，写入速度8万行/秒
  一开始就要想到并行  同步一个实例拆 同步一个库，再拆 同步一个表，一开始需要拆到这个级别，等跑起来，还要优化 同步表中的一部分  初始化很重要  表数据初始化的速度和性能非常重要 尤其是程序可能有bug的时候，会反复用到表初始化地步 比起增量同步，全量同步难在如何提速。 线上增量的消费速度，可需要追上就行。这部分单线程就能满足，反而容易 如何将一张上亿的大表，尽可能快的全量抽到另一个实例中，这个非常重要  注意时间和日期格式和中文编码  被oracle 的 -0002-1-1 这样的公元前日期折磨得怀疑人生 不同的数据库类型的日期要求和编码。处理起来会非常麻烦  python的优势  python的胶水特性在做数据同步时非常实用 可以快速的完成试错和让应用跑起来很重要  要有验证逻辑  同步是否成功，数据量对比 是否有延时 数据关键特征是否匹配  要做好规划  如果可能一个实例一个实例的去攻克 如果可能一开始画好程序的主要路径 要充分考虑部分同步出错时的补救方法  </description>
    </item>
    
    <item>
      <title>常用脚本_PostgreSQL常用命令</title>
      <link>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_postgresql%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_postgresql%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</guid>
      <description> 进入PostgreSQL  su - postgres cd /usr/local/pgsql/bin/ ./psql  看当前端口号  SHOW port;  列出所有数据库  \l  切换数据库  \c dbname  列出当前数据库的所有表  \d  查看指定表的所有字段  \d tablename  查看指定表的基本情况  \d+ tablename  确认是否是主库（false为主）  SELECT pg_is_in_recovery();  有几个从库  SELECT count(*) FROM pg_stat_replication;  查看从库状态  SELECT * FROM pg_stat_replication;  建库  CREATE DATABASE cdt_pg_maxkb;  建用户  CREATE USER pgmaxkb_owner WITH PASSWORD &#39;「password」&#39;;  给权限  GRANT ALL PRIVILEGES ON DATABASE cdt_pg_maxkb TO pgmaxkb_owner;  数据库的连接信息  select * from pg_stat_activity </description>
    </item>
    
    <item>
      <title>MySQL多实例部署报错：io_setup failed with EAGAIN</title>
      <link>/mysql/mysql%E5%A4%9A%E5%AE%9E%E4%BE%8B%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99io_setup-failed-with-eagain/</link>
      <pubDate>Wed, 30 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%A4%9A%E5%AE%9E%E4%BE%8B%E9%83%A8%E7%BD%B2%E6%8A%A5%E9%94%99io_setup-failed-with-eagain/</guid>
      <description>多实例部署时遇到io_setup() failed with EAGAIN报错  2024-10-30T14:17:28.568852+08:00 0 [System] [MY-013169] [Server] /usr/local/mysql8/bin/mysqld (mysqld 8.0.22) initializing of server in progress as process 171586 2024-10-30T14:17:28.587654+08:00 1 [System] [MY-013576] [InnoDB] InnoDB initialization has started. 2024-10-30T14:17:28.593140+08:00 1 [Warning] [MY-012582] [InnoDB] io_setup() failed with EAGAIN. Will make 5 attempts before giving up. 2024-10-30T14:17:28.593225+08:00 1 [Warning] [MY-012583] [InnoDB] io_setup() attempt 1. 2024-10-30T14:17:29.094632+08:00 1 [Warning] [MY-012583] [InnoDB] io_setup() attempt 2. 2024-10-30T14:17:29.595951+08:00 1 [Warning] [MY-012583] [InnoDB] io_setup() attempt 3.</description>
    </item>
    
    <item>
      <title>MySQL为了适应大规模bi拉取数据的参数调整</title>
      <link>/mysql/mysql%E4%B8%BA%E4%BA%86%E9%80%82%E5%BA%94%E5%A4%A7%E8%A7%84%E6%A8%A1bi%E6%8B%89%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/</link>
      <pubDate>Sat, 12 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E4%B8%BA%E4%BA%86%E9%80%82%E5%BA%94%E5%A4%A7%E8%A7%84%E6%A8%A1bi%E6%8B%89%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%82%E6%95%B0%E8%B0%83%E6%95%B4/</guid>
      <description>线上bi实例，专门用于bi拉数或开发定位问题用的，需要调整以下参数
  设置连接空闲的超时时间（秒）。可以通过增加这些参数的值来延长连接的存活时间，以便超长查询有充足的时间完成。  SET GLOBAL wait_timeout = 28800; -- 28800秒（8小时） SET GLOBAL interactive_timeout = 28800;  用于控制服务器等待客户端发送数据的时间（秒），对于较大的导出操作，增大这两个参数可以防止数据传输中断。  SET GLOBAL net_read_timeout = 600; -- 增大到10分钟，适合大查询 SET GLOBAL net_write_timeout = 600;  控制单个查询的最大执行时间（毫秒），  SET GLOBAL max_execution_time = 0; -- 设置为0，表示不限制查询时间  控制排序操作时使用的内存大小,默认值较小。对于导出大数据的查询，将其适当增大（如16M或32M）可以减少磁盘排序操作，提升查询效率。  SET GLOBAL sort_buffer_size = 167772160; -- 160M  控制全表扫描时每次读取的数据量，适合大查询时进行适度调高，通常可以设置为2M到16M。  SET GLOBAL read_buffer_size = 16777216;  控制InnoDB引擎在等待行锁的最大时间（秒）。对于长查询，适当增大可以避免锁等待超时错误。  SET GLOBAL innodb_lock_wait_timeout = 300;  控制最大通信数据包大小。对于导出大数据，可以增加此值避免错误。  SET GLOBAL max_allowed_packet = 1073741824; -脚本</description>
    </item>
    
    <item>
      <title>Centos7安装ffmpeg</title>
      <link>/ops/centos7%E5%AE%89%E8%A3%85ffmpeg/</link>
      <pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/ops/centos7%E5%AE%89%E8%A3%85ffmpeg/</guid>
      <description>安装 EPEL yum install epel-release -y 安装Nux Dextop 库 rpm --import http://li.nux.ro/download/nux/RPM-GPG-KEY-nux.ro rpm -vhU http://li.nux.ro/download/nux/dextop/el7/x86_64/nux-dextop-release-0-1.el7.nux.noarch.rpm 安装 FFmpeg yum install ffmpeg ffmpeg-devel -y 安装验证 ffmpeg -version  安装成功  </description>
    </item>
    
    <item>
      <title>Python虚拟环境安装和配置venv和conda</title>
      <link>/ops/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/ops/python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</guid>
      <description>两种比较常用的虚拟python环境 可以避免多版本的包冲突和python版本依赖  conda 1. 安装 Conda  下载 Miniconda 安装脚本  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  运行安装脚本  bash Miniconda3-latest-Linux-x86_64.sh  重新加载 shell 配置文件以激活 Conda：  source ~/.bashrc 2. 创建 Python 3.10 虚拟环境  使用 Conda 创建一个新的虚拟环境，并指定 Python 3.10 作为解释器：  conda create -n py310env python=3.10 #py310env 是环境的名称，你可以根据需要更改。 3. 激活虚拟环境  创建环境后，激活它以开始使用：  conda activate py310env 4. 验证 Python 版本  确保你已经成功切换到 Python 3.10：  python --version 你应该看到输出类似于 Python 3.10.x。</description>
    </item>
    
    <item>
      <title>数据库迁移流程图</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%E6%B5%81%E7%A8%8B%E5%9B%BE/</link>
      <pubDate>Sat, 14 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%E6%B5%81%E7%A8%8B%E5%9B%BE/</guid>
      <description>合集 MySQL: MySQL迁移前 MySQL迁移中 MySQL迁移后 Oracle: Oracle迁移前 Oracle迁移中 Oracle迁移后 Redis: Redis迁移前 Redis迁移中 Redis迁移后 </description>
    </item>
    
    <item>
      <title>当“延迟退休”成为现实，我们该如何面对“老无所依”</title>
      <link>/book/%E4%BB%8E%E5%BB%B6%E8%BF%9F%E9%80%80%E4%BC%91%E5%88%B0%E8%80%81%E6%97%A0%E6%89%80%E4%BE%9D/</link>
      <pubDate>Fri, 13 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E4%BB%8E%E5%BB%B6%E8%BF%9F%E9%80%80%E4%BC%91%E5%88%B0%E8%80%81%E6%97%A0%E6%89%80%E4%BE%9D/</guid>
      <description>人口老龄化问题  值得纪念的一件事 延迟退休的鞋子落地了 男63岁女58岁 很久很久以前 还是高中的时候 我的一位老师就用课下的时间说过 因为一些政策 中国人口老龄化的问题会随着我们这一代人越来越严重 当时完全听懂了他的逻辑 这一批年轻人老去的时候 会面临很严重的社会问题 再后来携程的梁建章反复在公开场合发表观点 包括三十年后的老人要不养老金减半 要不延迟到七八十岁退休 强调中国人口问题 在很多年轻人找不到工作的时候 就提前在呼吁政策松绑 事实上他的建议是对的 这几年的发展也验证了这些观点 未来10-20年 一个社会的持续发展 一定要有更多的年轻人在推动 否则老人越来越多 人越来越少 社会会失去活力 大多数老年人将会面对老无所依的囧境 这个趋势如此明显 数据不会反转 问题肯定会爆发 下面的这张当前的人口年龄分布图 未来的20-30年内 我们将面对一个巨大的人口持续老龄化过程 大约在2060年左右 60岁以上的老人应该在5亿人左右 20-60岁的劳动人口在4.5亿左右 0-20岁人口在2.5亿 这是非常非常不健康的数据 但是趋势正在往这个方向走 且完全没有改变的机会  应对策略 方案1:延迟退休  为什么要延迟退休？ 让还没老得不能动的老人参与工作 缓解未来的劳动人口不足问题  方案2:降低退休金  社保基金无法支撑未来几年的退休人员支出 这还是赶在经济快速增长的这些年积累基础上的 未来会更夸张 现有的科技不变情况下 未来老年人可支配收入的实际购买力会持续下降  方案3:增加年轻人  鼓励生育 大量增加年轻人 平衡人口比例  方案4:引进外来人口  引进其他发展中国家的劳动人口 来从事体力和服务业 降低我们的人口压力  方案5:新的科学技术飞跃  科技在发展 劳动生产率还在提升 ai和自动化将持续缓解&amp;lt;需要人的工作&amp;gt; 机器人正在更多的参与工厂和日常生活中  小结  我深信中国的人口老龄化问题已经显现 而我的同龄人将会面对这一困局 也一直觉得等我老了的时候 一定会处于“老无所依”的局面 而且还是会孤独终老  </description>
    </item>
    
    <item>
      <title>Centos7安装minnconda和常用命令</title>
      <link>/ops/centos7%E5%AE%89%E8%A3%85minnconda%E5%92%8C%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Thu, 12 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/ops/centos7%E5%AE%89%E8%A3%85minnconda%E5%92%8C%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</guid>
      <description>下载  官网：https://docs.anaconda.com/miniconda/ 找到下载地方：  这里演示的是x86服务器  mkdir -p ~/miniconda3 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 rm ~/miniconda3/miniconda.sh 使用 常用命令  验证安装  source ~/miniconda3/bin/activate conda --version  安装包   conda install package_name 例如：conda install numpy
  安装特定版本的包   conda install package_name=versio 例如：conda install numpy=1.18.1
  更新包   conda update package_name例如： conda update numpy
  移除包   conda remove package_name # 例如：conda remove numpy</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(五):SlowLog</title>
      <link>/mysql/mysql%E7%9A%84slowlog%E6%97%A5%E5%BF%97/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84slowlog%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续：
 MySQL的7种日志(一):概况 MySQL的7种日志(二):RedoLog MySQL的7种日志(三):UndoLog MySQL的7种日志(四):BinLog MySQL的7种日志(五):SlowLog  1.什么是Slowlog  数据库执行一个SQL时，如果超过了设定值(比如说500毫秒),数据库将此SQL和相关信息记录到日志中，这个日志就是SLowlog，我们也称为慢日志 slowLog的开启，是为了定位和发现慢SQL用的。这一点跟前几篇文章里的redbolog,undolog,binlog等日志不一样  SlowLog的超时时间long_query_time  这个需要特别注意，并不是我们通常理解的一个SQL从开始执行到执行完用了多长时间 事实上MySQL判断一个sql是否要被记到slowlog中，是这样的逻辑： 假设我们设置了超过500毫秒的SQL是慢SQl要记下来，MySQL会这样处理 实际SQL执行消耗的时间- 锁等待消耗时间 如果这个时间&amp;gt;=500毫秒，则记下SlowLog否则不记 这就相当于说开车起点到终点的时间如果超过30分钟就很慢了 但我们说的30分钟不包括路上堵车和等红绿灯的时间  # @long_query_time ：我们设置了慢日志记录时间 # sqltime ：mysql判断一个sql的执行用时 # cur_utime ：这条sql从开始执行到结束，实际消耗的时间 # utime_after_lock：锁等待消耗时间 sqltime = cur_utim- utime_after_lock if sqltime&amp;gt;=@long_query_time: recordIt() #写入慢日志 else: pass 2.MySQL慢日志的常用操作 开启  修改my.cnf  [mysqld] slow_query_log = 1 slow_query_log_file = /data/mysql3306/mysql-slow.log #指定了慢查询日志的输出文件路径； long_query_time = 1 # 超过多长时间（秒）的SQL 被记录 修改  慢日志的几个项都可以在线联机修改的  set global long_query_time=0.</description>
    </item>
    
    <item>
      <title>微调有必要做吗？微调还是RAG?</title>
      <link>/ai/%E5%BE%AE%E8%B0%83%E6%9C%89%E5%BF%85%E8%A6%81%E5%81%9A%E5%90%97%E5%BE%AE%E8%B0%83%E8%BF%98%E6%98%AFrag/</link>
      <pubDate>Mon, 09 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E5%BE%AE%E8%B0%83%E6%9C%89%E5%BF%85%E8%A6%81%E5%81%9A%E5%90%97%E5%BE%AE%E8%B0%83%E8%BF%98%E6%98%AFrag/</guid>
      <description>我需要对大模型做微调吗？ 想自定义大模型时，选择：微调还是RAG还是ICL？   需要对大模型做微调？  在人工智能的世界里，大型语言模型（LLM）已经成为了我们探索未知、解决问题的得力助手。 但是你想自己定义一个属于自己的大模型，它有自己特色的数据训练和回答方式。自己从头训练一个大模型的成本太高 这时候可能需要考虑在已有的大模型上做：微调 就像一个微整容手术一样，变得更帅     判断因素 是 否     是否需要特定领域的精确性？ 如果你的应用需要处理特定领域的数据，如医疗、法律或金融，并且需要高度的准确性和对专业术语的理解，那么微调可能是必要的。 如果你的应用是通用的，或者不需要深入特定领域的专业知识，那么可能不需要微调。   是否需要定制化模型行为？ 如果你需要模型以特定的风格、语调或格式响应，或者需要它表现出特定的行为特征，微调可以帮助你实现这些定制化需求。 如果模型的通用行为已经满足需求，或者你不需要特定的响应风格，那么微调可能不是必需的。   是否面临边缘案例的挑战？ 如果你发现模型在处理某些边缘或罕见案例时表现不佳，微调可以帮助改进这些特定情况的处理。 如果模型在所有常见和边缘案例中都表现良好，那么微调可能不是必要的。   是否需要提高模型的可靠性？ 如果模型在遵循复杂指令或生成期望输出方面存在失败，微调可以提高其可靠性。 如果模型已经足够可靠，能够满足你的输出要求，那么可能不需要微调。   是否需要降低成本？ 如果你希望通过微调将大型模型的技能转移到更小的模型中，以减少计算资源的使用和成本，那么微调是有益的。 如果成本不是主要考虑因素，或者你不需要优化模型的大小和性能，那么微调可能不是必需的。   是否需要快速部署新任务？ 如果你需要模型快速适应新任务或能力，微调可以帮助你实现这一点。 如果模型目前的任务已经足够，并且没有立即引入新任务的需求，那么微调可能不是必要的。   是否有足够的训练数据？ 如果你拥有足够的、高质量的、与任务相关的训练数据，微调可以显著提高模型的性能。 如果缺乏足够的训练数据，或者数据质量不高，微调可能不会带来预期的效果。   是否对模型的透明度有要求？ 如果你的应用需要模型的决策过程是可解释的，微调可以帮助你更好地理解和控制模型的行为。 如果模型的透明度不是关键考虑因素，那么可能不需要微调。   是否有足够的资源进行微调？ 如果你有足够的计算资源和专业知识来进行微调，那么这是一个可行的选项。 如果资源有限，可能需要考虑其他方法，如上下文学习或使用现成的模型。    1. 定制化风格与格式 你是否需要一个能够模仿特定人物或服务于特定受众的聊天机器人？通过使用定制数据集对LLM进行微调，我们可以使其响应更加贴近受众的具体要求或预期体验。例如，你可能需要将输出结构化为JSON、YAML或Markdown格式。</description>
    </item>
    
    <item>
      <title>Canal常用配置项整理</title>
      <link>/ops/canal%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E9%A1%B9%E6%95%B4%E7%90%86%E5%92%8C%E6%8A%A5%E9%94%99%E5%A4%84%E7%90%86/</link>
      <pubDate>Sat, 07 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/ops/canal%E5%B8%B8%E7%94%A8%E9%85%8D%E7%BD%AE%E9%A1%B9%E6%95%B4%E7%90%86%E5%92%8C%E6%8A%A5%E9%94%99%E5%A4%84%E7%90%86/</guid>
      <description>配置模板 ################################################# # 支持gtid的实例，应该打开了。以前我们默认是false canal.instance.gtidon=true # 源服务器的连接串 canal.instance.master.address=mysql3308.dboop.com:3308 canal.instance.dbUsername=canalreader canal.instance.dbPassword={password} canal.instance.connectionCharset = UTF-8 canal.instance.enableDruid=false # 下面这些项需要留空，有且只有需要丢了数据，重新指定binlog点的时候才配置，别乱写 canal.instance.master.journal.name= canal.instance.master.position= canal.instance.master.timestamp= canal.instance.master.gtid= # 启用或禁用时间序列数据库 (TSDB) 功能，用于存储 Canal 的元数据。 # 这个还挺重要的，强烈建议打开，这个在表结构变更时有用，具体可以看看原理 # 可以不写canal.instance.tsdb.url，默认保存在本地${canal.file.data.dir:../conf}/${canal.instance.destination:}路径下 canal.instance.tsdb.enable=true #canal.instance.tsdb.url=jdbc:mysql://127.0.0.1:3306/canal_tsdb #canal.instance.tsdb.dbUsername=canal #canal.instance.tsdb.dbPassword=canal # 过滤器,perl的正则表达式.用逗号分割，可以写多个 canal.instance.filter.regex=db01\\..*,db02\\..* #canal.instance.filter.black.regex= # 我们往kafka推消息的配置 canal.mq.topic=secCanal3308 canal.mq.partitionsNum=1 #我们用一个区，如果是分区 #canal.mq.partitionsNum=3 #canal.mq.partitionHash=test.table:id^name,.*\\..* # 下面几个如果行里有大json，超过1M有报错时，可以增加maxRequestSize #canal.mq.canalBatchSize = 500 #canal.mq.batchSize = 81920 #canal.mq.partitionsNum=1 #canal.mq.maxRequestSize = 2097152 ################################################# 我们没用到的配置项 # 我们不依赖与canal做这个切换，这里用不着,事实上这几项也确实不好用 # 也有可能是我们没用明白 #canal.instance.standby.address = #canal.instance.standby.journal.name = #canal.instance.standby.position = #canal.instance.standby.timestamp = #canal.</description>
    </item>
    
    <item>
      <title>MySQL创建远程链接服务器LinkServer步骤</title>
      <link>/mysql/mysql%E5%88%9B%E5%BB%BA%E8%BF%9C%E7%A8%8B%E9%93%BE%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8linkserver%E6%AD%A5%E9%AA%A4/</link>
      <pubDate>Thu, 05 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%88%9B%E5%BB%BA%E8%BF%9C%E7%A8%8B%E9%93%BE%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8linkserver%E6%AD%A5%E9%AA%A4/</guid>
      <description>环境准备  执行show engines;   mysql&amp;gt; show engines; +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | Engine | Support | Comment | Transactions | XA | Savepoints | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ | FEDERATED | NO | Federated MySQL storage engine | NULL | NULL | NULL | | MEMORY | YES | Hash based, stored in memory, useful for temporary tables | NO | NO | NO | | InnoDB | DEFAULT | Supports transactions, row-level locking, and foreign keys | YES | YES | YES | | PERFORMANCE_SCHEMA | YES | Performance Schema | NO | NO | NO | | MyISAM | YES | MyISAM storage engine | NO | NO | NO | | MRG_MYISAM | YES | Collection of identical MyISAM tables | NO | NO | NO | | BLACKHOLE | YES | /dev/null storage engine (anything you write to it disappears) | NO | NO | NO | | CSV | YES | CSV storage engine | NO | NO | NO | | ARCHIVE | YES | Archive storage engine | NO | NO | NO | +--------------------+---------+----------------------------------------------------------------+--------------+------+------------+ 9 rows in set (0.</description>
    </item>
    
    <item>
      <title>MySQL9.0在centos7上安装部署DBA版</title>
      <link>/mysql/mysql9.0%E5%9C%A8centos7%E4%B8%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2dba%E7%89%88/</link>
      <pubDate>Thu, 29 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql9.0%E5%9C%A8centos7%E4%B8%8A%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2dba%E7%89%88/</guid>
      <description>方法一：RPM安装方式 下载： wget https://dev.mysql.com/get/Downloads/MySQL-9.0/mysql-9.0.1-1.el7.x86_64.rpm-bundle.tar tar -xvf mysql-9.0.1-1.el7.x86_64.rpm-bundle.tar ll -rw-r--r-- 1 root root 1082173440 Jul 14 03:03 mysql-9.0.1-1.el7.x86_64.rpm-bundle.tar -rw-r--r-- 1 7155 31415 15319752 Jul 14 02:55 mysql-community-client-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 3628356 Jul 14 02:55 mysql-community-client-plugins-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 709720 Jul 14 02:55 mysql-community-common-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 595117664 Jul 14 02:55 mysql-community-debuginfo-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 2023960 Jul 14 02:55 mysql-community-devel-9.0.1-1.el7.x86_64.rpm -rw-r--r-- 1 7155 31415 4219028 Jul 14 02:55 mysql-community-embedded-compat-9.</description>
    </item>
    
    <item>
      <title>DTCC参会者视角：我在2024数据库技术大会的体验</title>
      <link>/dba/dtcc%E5%8F%82%E4%BC%9A%E8%80%85%E8%A7%86%E8%A7%92%E6%88%91%E5%9C%A82024%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8A%80%E6%9C%AF%E5%A4%A7%E4%BC%9A%E7%9A%84%E4%BD%93%E9%AA%8C/</link>
      <pubDate>Mon, 26 Aug 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/dtcc%E5%8F%82%E4%BC%9A%E8%80%85%E8%A7%86%E8%A7%92%E6%88%91%E5%9C%A82024%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8A%80%E6%9C%AF%E5%A4%A7%E4%BC%9A%E7%9A%84%E4%BD%93%E9%AA%8C/</guid>
      <description>前言  2024年数据库技术大会8月24日在北京成功举行 今年第十五届 是参与最深的一届 现场聆听了很多场的技术分享 还受邀做了一场数据安全的分享 感谢itpub的辛苦组织与坚持 给DBA提供了这么好的交流学习平台 这是发自内心的 每年能攒这么多的DBA在一起 非常难得 在会场会能见到很多老朋友 有的朋友可能是一年能见到一次的朋友 也遇到并结识了很多新的朋友 真的可谓收获满满 分享一下此次的个人参会体验 以及介绍一下我自己的分享主题  国产数据库在收缩  相比与前两年的火热 今年的国产数据库面孔在变少 信创的热度依旧很高 但随着信创的推进 部分单位已经完成了信创 而另一些信创可能正在进行中 前两年不断的涌现的新的数据库类型和创业公司 在减少 相比与前几年几乎每个大厂都在推新的数据库 tidb,ob,gaussDB&amp;hellip; 今年行业趋向收缩到几个大的国产db 这也意味着 未来再进入这个门槛的公司应该不多了 已有的这些厂商都拿到了自己的“地盘” 新势力不会再大量涌现了  向量和时序  因为这两种类型的库 我们都有在尝试 但是没有推 使用度几乎是0 所以格外关注这两个类型的库的发展状况 自己也在平衡 是否有必要在合适的时机引进和推广这类数据库 引进成本和推广代价是否值得 流窜了几个向量和时序的专场 也和同行交流 在特定场景下 是非常有价值的 我认可这件事 并打算后续尝试开始正式的试用  务实和务虚  新技术的快速引进和推广 你会在各个会场都听到各种高大上的解决方案 至少80%的专场都在聊新的技术方案 似乎传统的dba工作已经不在大家的视线范围内了 这样确实很高大上 但是相比与这些高大上 每个公司有特定的系统架构和环境 很多方案不一定能适配其他的场景 我更感兴趣的是基本的事 如何做好一个小目标 如何优化好一个小环节 这样的分享需要耐心的找 以及一点点运气 这些dba日常工作中遇到的那点事 怎么去优化和解决 是很有参考价值的 比如多融合库的在dba这边应该怎么选 比如快手上k8s的原因 等等&amp;hellip; 会在一定时间内 成为我们决定问题时的一个参考方向 其中有一场DBdoctor的分享 是我很想去听的分享 可惜因为时间关系 没有在现场 后期只能从ppt里看看了 而说到务实 不得不提一下 DBA之夜里林春老师了 分享dba从业20年经历的时候 林老师超认真的在说他们是如何辛苦的解决去O问题 怎么加班，怎么熬夜 怎么去想办法 得得的说了好久 非常的生动和具体 这真的是投身一线才有的体会  关于我的分享  说了很多现场的感受 其实我也是其中的分享 专场16里 我做为演讲嘉宾做了一场数据安全的分享 题目是《数据泄露了,有DBA什么事？》 这个名字是临时发挥的 其实有两种解读 我说的很显然是后一种 计划了3个章节来说这件事的 1.</description>
    </item>
    
    <item>
      <title>Centos7安装cx_Oracle驱动</title>
      <link>/oracle/centos7%E5%AE%89%E8%A3%85cx_oracle%E9%A9%B1%E5%8A%A8/</link>
      <pubDate>Tue, 30 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>/oracle/centos7%E5%AE%89%E8%A3%85cx_oracle%E9%A9%B1%E5%8A%A8/</guid>
      <description>安装说明  一台cents7的机器上没有安装过oracle python脚本需要调用远程oracle数据库，需要cx_oracle数据库  python3.9 /data/script/datasec/test.py #产生报错 .... File &amp;quot;/data/script/datasec/newSQL.py&amp;quot;, line 3, in &amp;lt;module&amp;gt; import cx_Oracle ModuleNotFoundError: No module named &#39;cx_Oracle&#39;  这个cx_Oracle驱动非常不好装 以下是安装cx_Oracle的标准步骤  安装Oracle客户端  https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html 在这个网站找到合适的版本，我下的是11.2版本，不同的系统下不同的版本 选择好后，需要用户名密码登陆 下载和安装   wget &amp;quot;https://download.oracle.com/otn/linux/instantclient/11204/oracle-instantclient11.2-basic-11.2.0.4.0-1.x86_64.rpm&amp;quot; rpm -ivh oracle-instantclient11.2-basic-11.2.0.4.0-1.x86_64.rpm  安装完后，会有个目录：  ll /usr/lib/oracle/11.2/client64/lib/ total 183252 -rw-r--r-- 1 root root 53865194 Aug 25 2013 libclntsh.so.11.1 -rw-r--r-- 1 root root 7996693 Aug 25 2013 libnnz11.so -rw-r--r-- 1 root root 1973074 Aug 25 2013 libocci.</description>
    </item>
    
    <item>
      <title>用ai技术来帮普通人完成扣篮</title>
      <link>/ai/%E7%94%A8ai%E6%8A%8A%E7%85%A7%E7%89%87%E5%8F%98%E6%88%90%E8%A7%86%E9%A2%91%E6%95%88%E6%9E%9C%E6%83%8A%E4%BA%BA/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E7%94%A8ai%E6%8A%8A%E7%85%A7%E7%89%87%E5%8F%98%E6%88%90%E8%A7%86%E9%A2%91%E6%95%88%E6%9E%9C%E6%83%8A%E4%BA%BA/</guid>
      <description> 上周朋友给我拍了一张打篮球的照片 当时我想表演一下扣篮 奈何弹跳能力有限 只留一下朴素的照片 好在有ai技术 弥补了老年人跳不起来的尴尬  原片（照片） 成片（视频）   </description>
    </item>
    
    <item>
      <title>一些面试题的合集</title>
      <link>/book/%E7%BD%91%E4%B8%8A%E7%9C%8B%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86%E5%90%88/</link>
      <pubDate>Mon, 08 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E7%BD%91%E4%B8%8A%E7%9C%8B%E5%88%B0%E7%9A%84%E4%B8%80%E4%BA%9B%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9B%86%E5%90%88/</guid>
      <description>看到一些面试题，试着看了其他行业的一些题目觉得还挺好玩的。上传备份在这里
 法律面试题库  法律基础知识及能力倾向测试(新).doc 实质性考核标准.doc  审计面试题库  审计试题答案.doc 审计试题.doc  行政面试题库  秘书笔试题.doc 行政助理（含答案）.doc 行政秘书.doc  销售人员面试题库  1-销售面试题.xls 2-业务面试试题.doc 3-销售面试笔试题目.doc 4-销售人员招聘面试题.doc 5-销售人员标准面试话术.doc 6-外贸面试宝典.doc 7-贸易公司业务助理（市场顾问）面试内容分享.doc 8-慧眼识鹰：销售人才的甄选与诊测技术-龙平-62页.ppt  财务类面试题库  会计试题及答案（三）.doc 会计试题及答案（二）.doc 美的集冷集团财务系统招聘考试试题.doc 财务类案例讨论试题.pdf 会计试题及答案（一）.doc  营销面试题库  物流计划招聘测试题（答案）.doc 业务笔试答案.doc 业务笔试.doc 推广案例题.doc 物流计划招聘测试题(题目）.doc  机电类面试题库  性能设计.doc 电控测试题.doc 注塑模具工艺试题.doc 电器试题答案.doc 平面设计试题答案.doc 性能设计人员基本知识考试题.doc 平面设计试题.doc 结构设计专业试题答案.doc 结构设计专业试题2.doc 性能设计试题答案.doc 技术测评试题.doc 电器试题.doc 电控测试题答案.doc 注塑模具工艺试题答案.doc  IT工程师面试题  SQL面试题目汇总.doc 2012java面试题全攻略.doc NET面试题大全,包括微软、华为、中兴等大企业的面试真题.doc php面试题_百度.doc 软件测试经典面试题.doc IT系统分析员考题 v1-answer.</description>
    </item>
    
    <item>
      <title>AI的电力需求以及国内的电力能源情况</title>
      <link>/ai/%E8%83%BD%E6%BA%90%E4%B8%8Eai/</link>
      <pubDate>Wed, 03 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%83%BD%E6%BA%90%E4%B8%8Eai/</guid>
      <description>AI的惊人耗电量  当我们说大模型AI的使用成本很贵的时候 一方面其训练和推理的硬件GPU本身很贵，另一方面其消耗的电力也很夸张 这里我们只说AI的耗电量  AI训练阶段耗电量  OpenAI的GPT-3模型一次训练的耗电量高达1287兆瓦时 谷歌于2022年发布的大语言模型PaLM一次训练需要消耗3436兆瓦时的电量 模型越大，训练所需要消耗的电量越多。现在gpt-4o,文心一言4,通义千问这些新的大模型训练用电量会更夸张  AI推理阶段的耗电量  GPT每生成1000个英文单词大约消耗0.125千瓦时的电量 OpenAI需要3,617台英伟达公司的HGX A100服务器（共有28,936个图形处理单元 (GPU)）来支持 ChatGPT，这意味着每天的能源需求为 564 兆瓦时 谷歌搜索中应用生成式AI技术，谷歌每年的耗电量将高达290亿千瓦时，也就是每天约7900万度电  AI耗电的原因及改进方向 AI耗电的原因：GPU运算   以H800 GPU PCIE 服务器整机为例：
 CPU耗电约 300W*2， 内存16根耗电约 250W 硬盘6块盘约200W 风扇耗电约150W H800GPU卡耗电约700W*8    合计：最大耗电量约为6800W（90%以上的耗电都是GPU引起的）
  这些还不包括机房空调制冷的电力消耗
    这样的一台服务器，对OpenAI这样的公司来说，一个机房里需要放几万台
  AI耗电的改进方向  随着英伟达的GPU工艺不停的迭代发展 每次英伟达的新品发布会，都会发现新一代的显卡比上一代的能耗是几倍的减少 性能越来越高，单位算力的能耗成倍的减少，这是实大实的在减少能耗 可惜AI的算力要求越来越高，减少的能耗目前赶不上需求的增长 未来很长一段时间AI消耗的能源会越来越多。  国内电力能源现状  来自国家能源局发布的《2023年全国电力工业统计数据》 太阳能和风能装机量正在快速增长     类别 装机容量(万千瓦时) 占比 同比增长     火电 139032 47.</description>
    </item>
    
    <item>
      <title>2024年立夏</title>
      <link>/book/2024%E5%B9%B4%E7%AB%8B%E5%A4%8F/</link>
      <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/2024%E5%B9%B4%E7%AB%8B%E5%A4%8F/</guid>
      <description>立夏  2024年6月21日 立夏 今天的北京稀稀拉拉的下着小雨 北方的夏天雨水也不多 这样凉爽的日子是很难得的 夏天的脚步就这样轻轻来了 记忆中的夏天 总带着一丝灵动和轻盈 来得突然又不突兀  喜欢夏天  喜欢夏天 喜欢她的绚烂和热烈 蓝天无垠，阳光灿烂 一草一木都在展现它最完整的生命力 喜欢夏天 喜欢热得滋滋冒汗的畅快 喜欢热风扑面而来的爽快 我有个习惯 在最热的日子里 洗澡之前 关上门窗 不开风扇没有空调 任由汗水往外刷 然后衣服湿透了去冲澡 这种独特的爱好 多少有点变态 但我确实不怕热 喜欢夏天 喜欢她的清凉与宁静 在烈日炙烤的午后 街道上人烟稀少 只有烈日下的空气缓缓流淌 如果在老家，在乡村 还会有蝉鸣不绝于耳 就是那种空灵 我却很喜欢 躲在树荫下 听风从树叶间轻轻穿过 小时候的我 会光脚试探外面被阳光烤过的土地的温度 喜欢夏天 喜欢她的自由与奔放 夏天的白天很长 夏天的傍晚很长 人们有更多的时间可以用 不用顾虑穿几件羽绒服出门 不用顾虑有风有雨 一件短袖 屋内屋外哪都可以去 可以骑车 可以奔跑 可以野泳 可以做想做的很多事  夏天的傍晚  去掉火热的太阳 夏天的夜晚 微风习习 刚洒完水的院子里 一家人围着“竹床” 守着电扇 在爸妈一声声的感叹中 一碗接一碗的喝绿豆粥 直到把肚子撑得鼓鼓的 那些简单的快乐和纯粹的时光 是属于家人最悠闲的记忆 那份久违的安宁 后来也曾经常经历 但都不如孩童年纪的深刻 有一首歌 温柔的晚风 轻轻吹过 城市的灯火 今夜的晚风 你去哪里 请告诉我 &amp;hellip; 以前我会经常在夏天的傍晚 把车开到一个没人的路上 或减河公园的停车场 打开窗户 反复听着这段旋律 在安静的夜幕中 想象着某种过去发生的事 独自一人品味着孤独 也在找回内心的平静与安宁 夏天的晚风 带走了日间的疲惫 而那首歌 则像是一位老朋友 柔声细语的 说着另一个时空的故事 那旋律与晚风 也成为珍贵的记忆 无论时光如何变迁 那些美好和感动 都是夏夜中美好的注脚 一起构成了我喜欢夏天的原由  盛夏  去年的夏天 我曾感叹 在我状态最好的夏天里 没有人陪在身边 我会很快老去 没有多少个夏天 可以一起度过的 希望这个夏天里 有说有笑 一路有伴  </description>
    </item>
    
    <item>
      <title>20240606喜欢的地方喜欢的事和喜欢的人</title>
      <link>/book/20240606%E5%96%9C%E6%AC%A2%E7%9A%84%E4%BA%8B%E5%92%8C%E5%96%9C%E6%AC%A2%E7%9A%84%E4%BA%BA/</link>
      <pubDate>Thu, 06 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/20240606%E5%96%9C%E6%AC%A2%E7%9A%84%E4%BA%8B%E5%92%8C%E5%96%9C%E6%AC%A2%E7%9A%84%E4%BA%BA/</guid>
      <description>6月6 8车8 我有一个习惯 喜欢重叠的日期和数字 觉得非常有意思 比如08年的08月08日 这个日期就很重要 因为那是我给自己定的工作生活地点的选择点 那是给自己选择的Deadline  愿望  在喜欢的地方 和喜欢的人在一起 做着喜欢的事 这是我的三个愿望 有点奢侈 但我一直很坚定 拥有这些不仅仅需要努力 可能还需要一点运气 可能有些人一辈子也找不到自己喜欢的事 或者遇不到自己喜欢的人  喜欢的地方  海南 就是这个答案 中学的地理课上 偌大的中国大地上 最吸引我的就是海南了 会被海南的各种风景照感染 以至于现在 热带的海边 还是不停的撩动我的心 大学毕业后 当我发现南下去海南 没法生活下去（没有合适的工作） 选择了来北京 来中关村先攒点工作经验 定好了08年8月8日北京奥运会开始了 我就收拾行李离开去海南 计划得有模有样的 结果到了080808那天晚上奥运会如期举行 我骑着自行车在鸟巢周边溜达 看着热闹的人群 发现自己暂时没有能力离开 因为没钱,没有能力离开北京 那天晚上我骑着自行车 绕着奥体公园转圈 有些话到了时间点却没有勇气说出来 对着电话只能说一些眼前的事 也许说出来后面就不一样了吧 我还是喜欢这个地方 虽然去的次数不多 作为游客 未曾感受过那里真正的生活 但我想我还是喜欢的 不出意外的话 将来我还是会在海边老去 对，就是年轻时决定的那个地方  喜欢的事  计算机 毫无悬念的答案 大学的时候 也有过纠结未来做什么工作呢 有两个备选项 1.</description>
    </item>
    
    <item>
      <title>用Python操作Milvus向量数据库的简明教程</title>
      <link>/ops/%E7%94%A8python%E6%93%8D%E4%BD%9Cmilvus%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</link>
      <pubDate>Fri, 24 May 2024 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E7%94%A8python%E6%93%8D%E4%BD%9Cmilvus%E5%90%91%E9%87%8F%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</guid>
      <description>本文介绍如何使用 Python 操作 Milvus 向量数据库，包括创建集合、插入数据、创建索引和执行搜索。Milvus 是一款专为向量查询与检索设计的开源向量数据库，特别适用于 AI 和机器学习场景。
 环境准备 安装 Milvus服务端  1.docker 安装  wget https://github.com/milvus-io/milvus/releases/download/v2.2.3/milvus-standalone-docker-compose.yml -O docker-compose.yml docker-compose up -d   脚本 安装    wget https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/standalone_embed.sh bash standalone_embed.sh start 安装 Milvus 客户端 在开始之前，请确保已经在环境中安装了 Milvus 客户端库 pymilvus。你可以使用以下命令进行安装：
pip install pymilvus 连接 Milvus 首先，我们需要连接到本地的 Milvus 服务。假设 Milvus 服务运行在 127.0.0.1，端口为 19530。
from pymilvus import connections # 连接到 Milvus 服务 connections.connect(alias=&amp;#34;default&amp;#34;, host=&amp;#39;127.0.0.1&amp;#39;, port=&amp;#39;19530&amp;#39;) 定义集合 Schema 接下来，我们需要定义 Milvus 集合的 schema。假设我们要存储的集合包含一个 ID 字段和一个向量字段，向量维度为 128。</description>
    </item>
    
    <item>
      <title>利用开源大语言模型打包成自己的大语言模型</title>
      <link>/ai/%E5%88%A9%E7%94%A8%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%89%93%E5%8C%85%E6%88%90%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E5%88%A9%E7%94%A8%E5%BC%80%E6%BA%90%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%89%93%E5%8C%85%E6%88%90%E8%87%AA%E5%B7%B1%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</guid>
      <description>为什么要用开源模型 大语言模型有两种类型：  闭源的模型，如GPT-3.5、GPT-4、Cluade 、文心一言等 开源的模型，如LLaMA、ChatGLM,Qianwen等  开源模型的优势  已知目前最强的gpt4等大模型是商用闭源的，这些模型参数更大，更加智能，为什么我们会关注开源模型呢？ 可以本地部署运行（利用自己的电脑或服务器，运行）数据交互不需要和外网连接，数据安全性提升 不需要购买服务，不用开会员，跑在自己的电脑上，想用多少就用多少   怎么打包自己的模型 本地运行大模型  本地运行，需要至少一台性能很好的机器，不管是服务器，云服务器，或者自己的电脑，最好有张naviad 的显卡 选择大模型，现在目前最好的是llama3 是由Meta公司开源的，另外gemma是Google的，也非常不错，微软和苹果也开源了。中文的阿里开源的千问也不错 选好大模型后去下载到本地（体积看参数多少，在4G&amp;ndash;100G之间） 下载好后，就可以本地运行了，只需要在命令行中输入命令 如果想要个网页端上对话，可以再下载一个网页端，比如open webui （这是我喜欢用的）。看个人风格  打包自己的模型  有个新闻说是国内现在发布了几百个ai大语言模型，很多都是基于这些开源的模型上训练或改的 即使在开源模型上训练和微调也需要很多的显卡资源和算力。也不是个人可以做到的 如果你和我一样没有很大算力的服务器，又想尝试发布自己的大模型 可以考虑重新打包一个大模型，让它变成你的大模型  怎么打包  步骤1.下载开源模型  wget &amp;quot;https://huggingface.co/shenzhi-wang/Llama3-8B-Chinese-Chat-GGUF-8bit/blob/main/Llama3-8B-Chinese-Chat-q8.gguf&amp;quot;&amp;quot; ll -h -rw-r--r-- 1 root root 8.0G Apr 21 10:21 Llama3-8B-Chinese-Chat-q8.gguf -rw-r--r-- 1 root root 662 Apr 21 14:57 Modelfile  步骤2.编辑Modelfile  vim Modelfile #类型如下 FROM &amp;quot;/data/gguf/Llama3-8B-Chinese-Chat-q8.gguf&amp;quot; TEMPLATE &amp;quot;&amp;quot;&amp;quot;{{ if .</description>
    </item>
    
    <item>
      <title>如何把MySQL和Oracle里的表同步到一个加密库中</title>
      <link>/dba/%E5%A6%82%E4%BD%95%E6%8A%8Amysql%E5%92%8Coracle%E9%87%8C%E7%9A%84%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0%E4%B8%80%E4%B8%AA%E5%8A%A0%E5%AF%86%E5%BA%93%E4%B8%AD/</link>
      <pubDate>Wed, 24 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%A6%82%E4%BD%95%E6%8A%8Amysql%E5%92%8Coracle%E9%87%8C%E7%9A%84%E8%A1%A8%E5%90%8C%E6%AD%A5%E5%88%B0%E4%B8%80%E4%B8%AA%E5%8A%A0%E5%AF%86%E5%BA%93%E4%B8%AD/</guid>
      <description>如图 加密配置元信息表 REATE TABLE `encrypt_baseinfo` (`from_linkname` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;来源数据库信息&#39;,`from_linktype` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;mysql&#39; COMMENT &#39;mysql,oracle,mongo&#39;,`from_connstr` varchar(200) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;127.0.0.1;3306;u_test;password;db_test;&#39;,`to_linkname` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;目标数据库信息&#39;,`to_linktype` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;mysql&#39; COMMENT &#39;mysql,oracle,mongo&#39;,`to_connstr` varchar(200) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;127.0.0.1;3306;u_test;password;db_test;&#39;,`kafka_name` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;kafka_name&#39;,`kafka_groupid` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;kakfak_groupid&#39;,`kakfak_offset` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39; COMMENT &#39;kakfak_auto_offset_reset&#39;,PRIMARY KEY (`from_linkname`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;加密字段基础信息表&#39;CREATE TABLE `encrypt_col_info` (`id` bigint unsigned NOT NULL AUTO_INCREMENT COMMENT &#39;ID、主键&#39;,`service_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;服务信息&#39;,`db_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;数据库信息&#39;,`tab_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;表名&#39;,`col_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;列名&#39;,`sub_col_name` varchar(200) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;json_key信息&#39;,`col_type` varchar(20) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;字段类型&#39;,`col_desc` varchar(5000) COLLATE utf8mb4_unicode_ci DEFAULT NULL COMMENT &#39;列名描述信息&#39;,`create_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#39;创建时间&#39;,`update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#39;最后更新时间&#39;,`linkname` varchar(50) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT &#39;&#39;,`sec_type` tinyint NOT NULL DEFAULT &#39;0&#39; COMMENT &#39;加密类型：0，1：卡号，2：身份证，3：手机，4：地址&#39;,PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=&#39;加密字段信息表&#39;判断配置表里的数据类型是否正确的快速方法  select concat(&#39;mysqlw -h &#39;,replace(linkname,&#39;my&#39;,&#39;mysql&#39;),&#39;-w.</description>
    </item>
    
    <item>
      <title>国内大语言模型现状之法律篇</title>
      <link>/ai/%E5%9B%BD%E5%86%85%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%E4%B9%8B%E6%B3%95%E5%BE%8B%E7%AF%87/</link>
      <pubDate>Tue, 23 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E5%9B%BD%E5%86%85%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%8E%B0%E7%8A%B6%E4%B9%8B%E6%B3%95%E5%BE%8B%E7%AF%87/</guid>
      <description>发展时间线  2022年11月30日,国外OPenAI公司的ChatGPT发布，大语言模型开始普及 2023年2月，国内ChatGPT开始进入国内视野大量媒体报道 2023年5月开始，基于开源ai大语言模型上用国内的法律条文和案例训练的国产法律大模型开始发布（以高校为主） 2023年底。随着llama模型升级，国产法律大模型更加智能，同时商业化落地产品开始出现 2024年由案例检索，合同审查为典型应用场景的国内法律AI产品开始推广应用  国内法律大语言模型现状    国内法律行业的大语言模型对比    大模型名称 日期 发布人 基础模型 推荐指数     ChatLaw 2023-06-28 北京大学深圳信息工程学院 Anima-33B 4星   LaWGPT 2023-04-12 南京大学 LLaMA 4星   LexiLaw 2023-05-16 清华大学 ChatGlM 6B 2星   獬豸(LawGPT_zh) 2023-04-09 上海交通大学 ChatGlM 6B 3星   Lawyer LLaMA 2023-04-13 北京大学 LLaMA 2星   韩非(HanFei) 2023-05-30 香港中文大学 LLaMA 1星   lychee_law-律知 2023-07-13 德国萨尔大学,中国南京大学 GLM-10B 1星   智海-录问 2023-08-08 浙江大学,ali达摩院,华院计算 Baichuan-7B 1星   DISC-LawLLM 2023-09-26 复旦大学 Baichuan-13B-Base 1星   夫子•明察 2023-08-31 山东大学,浪潮云,中国政法大学 ChatGLM 1星    国内法律大模型介绍 ChatLaw-法律大模型  地址：https://github.</description>
    </item>
    
    <item>
      <title>斯坦福大学发布了《2024人工智能指数报告》</title>
      <link>/ai/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E5%8F%91%E5%B8%83%E4%BA%862024%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8C%87%E6%95%B0%E6%8A%A5%E5%91%8A/</link>
      <pubDate>Tue, 16 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E6%96%AF%E5%9D%A6%E7%A6%8F%E5%A4%A7%E5%AD%A6%E5%8F%91%E5%B8%83%E4%BA%862024%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%8C%87%E6%95%B0%E6%8A%A5%E5%91%8A/</guid>
      <description>今天斯坦福大学发布了《2024人工智能指数报告》  原文在这里，一共有500多页pdf https://aiindex.stanford.edu/wp-content/uploads/2024/04/HAI_AI-Index-Report-2024.pdf 我将一些感兴趣的点总结出来  AI的发展状况 AI在2023年的恐怖的发展速度  2023年，有GPT4,Cluade2,DALL-E3，Gemini等非常有突破性的产品发布 以下是15个重要的大模型的发布时间点，这些大模型一定程度上代表了当前AI能达到的最佳水平   AI大模型的发布时间  横轴是发布时间（2012-2024） 纵轴是发布时的计算量（Training compute (petaFLOP - log scale)）   高昂的训练成本  谷歌的 Gemini Ultra 的训练计算成本估计为 1.91 亿美元 OpenAI 的 GPT-4 的训练成本估计为 7800 万美元。 高昂的训练成本使普通用户（甚至包括学术机构和政府）难以参与AI的训练。   不同地区分展的速度不同 大语言AI模型发布数量  美国在2023 年总共开发了 61 个模型。 中国在2023 年总共开发了 8 个模型。   AI领域的投资  2023年，美国投资额为672亿美元 是第二高国家中国投资额的8.7倍 是英国投资额的17.8倍。 缩小范围来看，这一阵容看起来是一样的：自 2013 年以来，美国的累计投资额为 3,352 亿美元，其次是中国，为 1,037 亿美元，英国为 223 亿美元。   </description>
    </item>
    
    <item>
      <title>Ollama的安装和配置</title>
      <link>/ai/ollama%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</link>
      <pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>/ai/ollama%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E9%85%8D%E7%BD%AE/</guid>
      <description>为什么要用Ollama  几乎是最方便的本地部署Ai大模型的方式 支持在 Mac、Windows、Linux 上运行 支持CPU,GPU 不用考虑复杂的本地环境 简直是大模型里的docker   Ollam官网的介绍是：Get up and running with large language models, locally.
 安装Ollla Mac  一行命令brew install ollama  Linux  一行命令  curl -fsSL https://ollama.com/install.sh | sh  Windows  下载ollama的安装包，下载地址 https://ollama.com/download/OllamaSetup.exe 安装上就即可  启动  ollama serve 启动服务 ollama list 查看本地的模型 ollama run 启动模型  配置Ollama 设置服务  vim /etc/systemd/system/ollama.service  [Unit] Description=Ollama Service After=network-online.target [Service] Environment=&amp;quot;OLLAMA_HOST=0.0.0.0:11434&amp;quot; ExecStart=/usr/local/bin/ollama serve #User=ollama #Group=ollama User=root Group=root Restart=always RestartSec=3 Environment=&amp;quot;PATH=/root/anaconda3/bin:.</description>
    </item>
    
    <item>
      <title>MySQL和Oracle数据库的一些审计要求</title>
      <link>/dba/mysql%E5%92%8Coracle%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%A1%E8%AE%A1%E8%A6%81%E6%B1%82/</link>
      <pubDate>Mon, 11 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/mysql%E5%92%8Coracle%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%A1%E8%AE%A1%E8%A6%81%E6%B1%82/</guid>
      <description>MySQL的审计要求  密码是否符合复杂度要求，需要包含大小写字母、数字、字符，密码长度是否达到8位以上  show variables like &#39;%validate%&#39;; INSTALL PLUGIN validate_password SONAME &#39;validate_password.so&#39;; set global validate_password_length=12 set global ... show global variables like &#39;%validate%&#39;;  密码是否定期更换，如，密码有效期设置为90天。  set global default_password_lifetime=360;  是否具有登陆失败锁定策略，例如，登陆失败5次锁定10分钟。   INSTALL PLUGIN CONNECTION_CONTROL SONAME &#39;connection_control.so&#39;; INSTALL PLUGIN CONNECTION_CONTROL_FAILED_LOGIN_ATTEMPTS SONAME &#39;connection_control.so&#39;; show global variables like &#39;connection_%&#39; ;  是否设置超时登出功能，例如，30分钟未操作自动登出  show variables like &#39;wait%timeout%&#39; ; Oracle的审计要求 </description>
    </item>
    
    <item>
      <title>Sora来袭：OpenAI公司是否又开启文本生成视频新篇章？</title>
      <link>/ai/sora%E6%9D%A5%E8%A2%ADopenai%E5%85%AC%E5%8F%B8%E6%98%AF%E5%90%A6%E5%8F%88%E5%BC%80%E5%90%AF%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%91%E6%96%B0%E7%AF%87%E7%AB%A0/</link>
      <pubDate>Tue, 20 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>/ai/sora%E6%9D%A5%E8%A2%ADopenai%E5%85%AC%E5%8F%B8%E6%98%AF%E5%90%A6%E5%8F%88%E5%BC%80%E5%90%AF%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E8%A7%86%E9%A2%91%E6%96%B0%E7%AF%87%E7%AB%A0/</guid>
      <description>Sora来袭  2024年2月15日OpenAI公司在推上连续发了几个Sora文生图视频，引起轰动 OpenAI并未单纯将其视为视频模型，而是作为“世界模拟器” Sora继承了DALL-E 3的画质和遵循指令能力 可以根据用户的文本提示创建逼真的视频 可以深度模拟真实物理世界，能生成具有多个角色、包含特定运动的复杂场景 能理解用户在提示中提出的要求，还了解这些物体在物理世界中的存在方式。  Sora来袭的反应  360集团创始人、董事长 周鸿祎   Sora将缩短AGI（通用人工智能）实现时间，从10年缩短到1年。OpenAI训练该模型应该会以视频和摄像头捕捉的画面为主，人工智能通过观看大量视频将对世界有更深入的理解，这离AGI实现不远
  电影导演兼视觉效果专家 迈克尔·格雷西   很快，像Sora这样的人工智能工具将允许电影制作者仔细控制他们的输出，从头开始创建各种视频，当技术剥夺了其他人的创造力、工作、想法和执行力，却没有给予他们应有的荣誉和经济报酬时，不是一件好事情。
  英伟达科学家 DrJimFan   Sora是一个数据驱动的物理引擎，它是对许多世界的模拟，无论是真实的还是幻想的，模拟器通过一些去噪和梯度数学来学习复杂的渲染、“直观”物理、长期推理和语义基础。
 Sora的团队  Sora核心团队有15人 Sora团队的Leader是Aditya Ramesh：他也是DALLE、DALLE2、DALLE3的主要作者 Sora的核心作者是Bill Peebles和Tim brooks Bill Peebles 在伯克利人工智能研究所完成了博士学位，导师是Alyosha Efros。在此之前，他在麻省理工学院攻读本科，指导老师是Antonio Torralba。他曾在FAIR、Adobe研究院和NVIDIA实习。 Tim brooks 在伯克利人工智能研究所获得了博士学位，导师是Alyosha Efros，他是InstructPix2Pix的作者。在此之前他曾在谷歌工作，参与Pixel手机相机的研发，在NVIDIA从事视频生成模型的研究。   Bill Peebles的说法是“每天基本不睡觉，高强度工作了一年。  如何试用Sora  答案是：现在还不行 到目前为止(北京时间 2023-02-20 18:20) Sora并没有对外开放 目前只有OpenAI内部员工,一批受邀请的视觉艺术家、设计师和电影制作人获得了Sora访问权限，他们也已开始在社交平台不断晒出使用Sora生成的新作品 未来开放时间不确定，但是首批可使用的用户数量不会太多，且肯定不会对中国大陆开放使用 国内用户要新手体验sora的功能，可能还是还很远  Sora的竞争对手    AI 公司 AI视频产品 发布日期      英伟达 PYoCo 2023.</description>
    </item>
    
    <item>
      <title>2023年终总结:岁末年终又一年</title>
      <link>/book/2023%E5%B9%B4%E6%80%BB%E7%BB%93%E5%B2%81%E6%9C%AB%E5%B9%B4%E7%BB%88%E5%8F%88%E4%B8%80%E5%B9%B4/</link>
      <pubDate>Fri, 09 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/2023%E5%B9%B4%E6%80%BB%E7%BB%93%E5%B2%81%E6%9C%AB%E5%B9%B4%E7%BB%88%E5%8F%88%E4%B8%80%E5%B9%B4/</guid>
      <description>又是一年岁末时  2024年2月9日 农历腊月三十 晚上22:51 刚看完沈腾的小品 演的什么玩意 一点也不好笑 今晚到现在看到的节目 就觉得一个唱歌的还不错 主要是造型颜值好看 其他的节目没啥印象了 今年的年尾有点忙乱 拖到现在开始写一年总结了 每年都会在春节前找时间 写点总结 今年拖到现在着实晚了些  生活 一顶帽子  春天的时候头发养得很长 为了罩着这些乱跑的头发 物色了很多帽子 最终找到了一顶JOYRICH的帽子 超爱这顶帽子 到哪都戴着 当个宝贝一样护着 后来实在是脏得不行了 费了老大劲买了个备用的 整个春天里 上上班爬爬山 也没发生什么事 能想起来的就是这个酷酷的帽子 以及帽子下爆炸的头发 还有爆炸的脾气 哪哪都觉得不满意  可怕的新冠  夏天的时候 感染了新冠 可怕的病毒 以前同事们相继感染的时候 觉得自己身体强健 没感染上 有点小得意着 今年终究还是没跑过 连续3天发烧40度 烧得晕头转向 各种并发症 睡下的时候会担心会醒不过来 一个星期时间症状消失 后面跟着的是 长达4个月的肌无力 说不出的那种无力感 这是成年以后 最虚弱的一段经历 在这段时间里 没有户外活动 下班就回家躺着 每天像个怨妇一样 回忆着过去的点滴 想着过去的对与错 也尝试让生活节奏慢下来 体会怎么和自己相处 觉得自己过去在与人相处过程中 太苛责了 把自己的想法强加给身边的人 造成TA们很大的压力 在这个夏天里 把自己关在屋里 想着怎么改变自己 想改变自己不要给别人很多的压力 不去这么强势的要求别人 不要吝啬 对别人好一些 对别人的指责少一些  精心策划的旅行  秋天的时候 计划了一系列的出游计划 其中有五岳爬山 还有四水计划 去了很多地方 整个9，10，11月 三个月里 周末基本都不在北京 一到周末就出门 我用强制的计划性 紧逻密鼓的安排着行程 其中最用心的是五岳计划 穿着同样的短裤和鞋 然后不同颜色的上衣 帽沿朝不同的方向 完美的完成了一次精心策划的行程 非常好玩 对强迫症来说是件很爽的事 各种安排 其实非常复杂 背后也藏着很多故事 到了年终总结的时候 我憋了一个更大的蓝图 “两仪生四象， 四象生八卦， 八卦运五行。” 结果小马问两仪呢？ 我想了又想 觉得勉强来说 如果两仪也能勉强凑上 今年我还去过 唐山和洛水 这能凑两仪 所以这个蓝图还是完美的 两仪，四象，八卦，五行 都齐了     再度重相逢  冬天的时候 做为运动计划的一圈 跑了北京的8个公园 相当不容易的一件事 但是这个冬天里 更重要的事 是和小马的再度重相逢 有那么一段时间 脑海里总是反复出现一段旋律 “简单爱，心所爱，世界也大了起来” 反复出现，反复出现 不知道为什么 好像有心电感应似的 总是相信她会回来的 当我反复听这首歌的时候 她就回来了 用了很长时间去想 我想和谁共度余生 这个曾经不是问题的问题 这两年成了问题 时间会给出答案 走着走着 就这么清晰了起来 我比以前的任何一段关系里 都更要认真 更加包容 虽然还不完美 但愿意为了别人改变自己 拼命的改变 希望她能感受得到 我有多认真 亲爱的 这一次 希望我们能守住这份感情 希望你是那个值得被爱的人 希望你爱着的那个人也是值得被爱的  生活总是很快  一口气写了四段生活的片断 其实也是按着春夏秋冬来的 这一年过得飞快 好像啥也没做 这一年就过去了 尤其是后半年 似乎过了生日后 时间就飞一般的没了 到了现在 这一年的生活 很艰难 因为不太理智的脾气 因为可怕的病毒 这一年也很充实 我不断的反思和改变 对比一年前的自己 没错 我比一年前更加宽容 更加能理解别人 会温和的表达自己的想法 会接受别人的“不聪明” 希望越来越温和 不要伤害到身边的人  工作  这一年的工作 大环境依旧不理想 热火朝天的互联网热浪再也见不到了 市场在冷却 行业还在经历着危机 技术的重要性还在一步步的降低 但是今年是AI元年 随着2月份ChatGPT在国内的大火 各种大模型嗖嗖的往外冒 这么热闹的事 当然少不了热心的我参与  数据库  说说本职工作 首先 很遗憾的是 今年团队继续在缩小规模 从年初的4个人 再缩到年底的2个人 好处是我们还没崩 我们还在努力把工作自动化，精简化 事实上只要把线上环境做标准化了 运维的工作还可以更省力 但是这一年 我们也确实在守成 没有开辟新的大项目 没有做大的改变 这不符合我的工作习惯 一年的时间里 我都没发起“大项目” 一方面是团队缩员造成的工作交接 另一方面是我自己动力不足 不管是主观还是客观上 今年在公司的时间比往年更少 可能是最少的一年 锐气不足   AI  比起在数据库方面的保守策略 在AI方面 我奋起搞事 二月份花了40分钟做了个ChatGPT转接页 火到服务器抗不住 不得不关停了 三月份做了微信的ChatGPT大聪明 陆续加了很多奇奇怪怪的功能 乐此不疲 刚好有很长一段时间 因为新冠恢复期 在家休息 有了大把时间折腾 把AI做视频，音频，图片，私有话训练 都做成了一个个的产品 虽然都不成功 但是确实在不停的尝试 有时候会隔几天就能捣鼓出来一个创意出来 在这一年里 在AI领域 投入了无数的精力和金钱 收获了在AI领域的很多有意思的事 同时也认识了一些有趣的网友 当然到现在我也没明白 怎么把AI产品做商业化  其他   23：52</description>
    </item>
    
    <item>
      <title>环北京五公里八公园跑步行动</title>
      <link>/book/%E5%8C%97%E4%BA%AC5%E5%85%AC%E9%87%8C8%E5%85%AC%E5%9B%AD%E8%B7%91%E6%AD%A5%E8%A1%8C%E5%8A%A8/</link>
      <pubDate>Mon, 05 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%8C%97%E4%BA%AC5%E5%85%AC%E9%87%8C8%E5%85%AC%E5%9B%AD%E8%B7%91%E6%AD%A5%E8%A1%8C%E5%8A%A8/</guid>
      <description>起因  今年秋天爬完了五岳 冬天我绕着北京城 用了8天 跑了8个大公园 每个公园跑5公里 完成了《五公里八公园》 合计用时224分钟 跑了39017步 平均配速:5分32秒 平均步频:173.75步/分钟  行程    公园 日期 跑步距离 用时 平均配速 步数 步频     温榆河公园 2023-11-18 5.04公里 28分42秒 5分41秒 4976步 173步/分钟   东郊湿地公园 2023-11-26 5.05公里 28分58秒 5分43秒 4905步 169步/分钟   城市绿心公园 2023-12-02 5.03公里 26分41秒 5分17秒 4595步 172步/分钟   台湖公园 2023-12-09 5.02公里 28分14秒 5分36秒 4878步 172步/分钟   南海子公园 2024-01-06 5.03公里 27分45秒 5分30秒 4850步 174步/分钟   清源公园 2024-01-14 5.</description>
    </item>
    
    <item>
      <title>MySQL8.0即时在线加字段instant-add-column</title>
      <link>/mysql/mysql8.0%E5%8D%B3%E6%97%B6%E5%9C%A8%E7%BA%BF%E5%8A%A0%E5%AD%97%E6%AE%B5instant-add-column/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql8.0%E5%8D%B3%E6%97%B6%E5%9C%A8%E7%BA%BF%E5%8A%A0%E5%AD%97%E6%AE%B5instant-add-column/</guid>
      <description>原文地址：https://mysqlserverteam.com/mysql-8-0-innodb-now-supports-instant-add-column/
 在MySQL 8.0中迁移到新的事务数据字典使我们的这项工作变得容易得多。在MySQL 8.0之前，元数据（数据字典）存储在称为.frm文件的平面文件中， .frm文件是一种不可思议的格式，已近过时很久了。
该即时加列补丁是由腾讯游戏数据库管理员团队提供的，我们要感谢并感谢腾讯游戏所做的重要而及时的贡献。
以前有什么问题  MySQL 5.6之前，执行DDL的唯一方法是逐行复制行 (copy) MySQL 5.6是第一个支持INPLACE DDL的版本。(inplace)  INPLACE DDL主要由InnoDB处理，而逐行COPY在服务器层处理    copy和inplace的存在的问题  对于大型表，可能要花费很长时间，尤其是在复制环境中。 磁盘空间需求将增加一倍以上，大小与现有表大致相同。 DDL操作占用资源，并且对CPU，内存和IO提出了很高的要求，这从用户事务中争夺资源。 如果涉及复制，slave要一直要等待到DDL的完成，才能开始同步。  新的instant加字段方式 出现的时间点  MySQL 8.0.12 由腾讯游戏数据库管理员团队提供的instant-add-column被官方集成认可 新的加字段语法,通过指定ALGORITHM=INSTANT来代替原来的加字段方式，SQL如下：  ALTER TABLE table_name [alter_specification], ALGORITHM=INSTANT;  MySQL 8.0.29开始，扩展了对ALTER TABLE … ALGORITHM=INSTANT的支持：用户可以在表的任何位置即时添加列、即时删除列、添加列时评估行大小限制。 MySQL 8.0.29开始，添加列时会检查行大小限制。如果超出限制，则会报错。  优势  INSTANT算法的优势在于，仅在数据字典中进行元数据更改。 更改期间无需获取元数据锁定，也不会修改表中的数据。 速度极快，秒速完成，对业务几乎没有影响 不会产生大量的binlog 不会影响主从同步 不会影响性能  原理  简单的说：只修改了表定义元数据，并没有修改真正的数据 翻译官方的原理是：  我们面临的问题是，在立即添加列后元数据发生更改后，如何解析页面上的物理记录？ 请注意，此处的物理记录是指存储在聚集索引的叶页中的记录。聚簇索引的现有二级索引甚至非叶页（B树的内部节点）都不会受到影响。 InnoDB有两种主要的行格式，即冗余行和紧凑行格式。行格式动态是compact的一个较小变体。压缩及其派生的行格式从冗余行格式中删除了一些元数据，以节省空间。 由于这种“节省空间”的更改，当我们必须对页面上物理行中的数据进行反序列化时，我们总是需要从内部元数据结构中查找元数据。 为了使即时添加列起作用，我们需要为页面上的DYNAMIC和COMPACT行格式的物理记录添加一些元数据。 REDUNDANT行格式不需要此附加元数据，因为列数已存储在物理记录中。 额外的信息与数据字典中的一些元数据一起保留在物理记录中。 这与基于相同腾讯补丁的一些下游黑客的做法非常不同，后者在表空间的模糊和未使用的部分存储类似的元数据。 我们认为，将元数据存储在适当的数据字典表中并使其在事务上保持一致将使其更健壮且更自然。此新的元数据存储在物理记录中。 这个新的元数据包括一个存储在info_bits中的标志。 info_bits中的此新信息用于跟踪是否在第一个即时ADD COLUMN之后创建记录。 我们还使用info_bits跟踪物理记录中的字段/列数。当表经历第一个即时ADD COLUMN时的列数以及新添加的列的所有默认值都存储在数据字典中。 这两条信息存储在数据字典表的se_private_data列中。 有了这些额外的信息，现在可以立即执行ADD COLUMN操作，而无需修改表中的任何行。如果没有即时的ADD COLUMN，则表中的所有行将采用与以前相同的格式。 即时发出ADD COLUMN后，对该表的任何更新都将以新格式写入行。从数据字典中查找默认值（如果有）。 在每个即时ADD COLUMN中，都会分别跟踪新添加的列的默认值。这些列的默认值可以随时更改。因此，在重建或截断表之后，可以丢弃即时列数和默认值，此外，可以像以前一样将表中的行更改为旧格式。 如果该表是分区表，则不同的分区可能具有不同数量的即时列，并且需要不同数量的默认值。 如果某些分区被重建，截断或重新创建，则分区中的行也可以像以前一样更改为旧格式。 使用限制  在了解原理之后，我们来看看 “立刻加列” 的使用限制，就很容易能理解其中的前两项： “instant加列” 的加列位置只能在表的最后，而不能加在其他列之间（MySQL8.</description>
    </item>
    
    <item>
      <title>redis的内存报警OOM command not allowed when used memory&gt;maxmemory</title>
      <link>/dba/redis%E7%9A%84%E5%86%85%E5%AD%98%E6%8A%A5%E8%AD%A6oomcommandnotallowedwhenusedmemorymaxmemory/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/redis%E7%9A%84%E5%86%85%E5%AD%98%E6%8A%A5%E8%AD%A6oomcommandnotallowedwhenusedmemorymaxmemory/</guid>
      <description> 非核心业务的一次小故障，未造成用户感知到的业务影响，记录如下
 参与者  DEV1,DEV2 DBA1,DBA2 3主3从的RedisCluster集群：1.10,1.11,1.12,1.20,1.21,1.22  故障起因  DEV1想排查线上Redis是否有对指定的key有访问 11:45 DEV1找到DBA1协助排查 11:50 DBA1在1.11实例上开启monitor进程，监控Redis写入 11:55 monitor进程启动5分钟后，1.11实例的内存占用从2G涨到10G 触发该节点的内存占满，引发故障(该节点的新写入报错，其他节点正常读写) 12:05 DBA1在1.11实例上停止monitor进程，1.11实例的内存占用从10G回退到2G 12:05 Redis集群自动恢复正常   故障发现和处理  12:20 DEV2收到报警 12:23 DEV2找到DBA2反馈程序报错Caused by: io.lettuce.core.RedisCommandExecutionException: OOM command not allowed when used memory &amp;gt; &#39;maxmemory&#39; 12:25 DBA2上线检查问题，在节点1.10上查看内存使用率是2G/10G 正常 12:28 DBA2检查该集群的1.10，1.11,1.12三个节点内存都是2G/10G 没发现异常。 12:30 查不到问题，修改该集群的所有节点最大内存从10G 改到12G 12:30 DEV2重启应用，发现恢复。 12:40 DBA2检查Redis应用，发现set,get的命令从每秒的6000次/秒降到500次/秒，认为业务没有恢复，建议继续排查 12:45 DBA1，DEV1参与排查，DEV2发现有个status任务没有重启成功 12:46 DEV2重启status任务，1分钟后，Redis监控指标恢复正常，故障完成处理 13:12 回溯整个过程，确认是11:50的Monitor进程引起的内存占用异常，原因定位 13:25 沟通确认Monitor进和不可以长期开启的规范。故障完成处理和总结  总结  DBA协助研发排查问题时，开启Monitor进程时间过长，引起一个节点的内存占满，继而引起研发的进程挂掉 非核心业务，没有影响到用户和交易，处理过程中现象比较明显，处理难度低，监控还是不够周全 补充：考虑换LRU策略  </description>
    </item>
    
    <item>
      <title>利用OracleGoldenGate(ogg) 从Oracle同步数据到MySQL</title>
      <link>/oracle/%E5%88%A9%E7%94%A8oraclegoldengateogg-%E4%BB%8Eoracle%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E5%88%B0mysql/</link>
      <pubDate>Thu, 18 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/oracle/%E5%88%A9%E7%94%A8oraclegoldengateogg-%E4%BB%8Eoracle%E5%90%8C%E6%AD%A5%E6%95%B0%E6%8D%AE%E5%88%B0mysql/</guid>
      <description>案例A：增加增量同步 1.增加extract进程 dblogin userid GOLDENGATE@{{ Oracle server url }},password {{ Oracle Password }} register extract E02 database add extract E02,integrated tranlog, begin now add EXTTRAIL /data/ogg19oracle/dirdat/eb, extract E02,MEGABYTES 100 add schematrandata {{ oracle.Schema }} 编辑ogg文件  vim /data/ogg19oracle/dirprm/e02.prm  extract e02 setenv (NLS_LANG=AMERICAN_AMERICA.ZHS16GBK) userid GOLDENGATE@{{ Oracle server url }},password {{ Oracle Password }} exttrail /data/ogg19oracle/dirdat/eb GETUPDATEAFTERS GETUPDATEBEFORES NOCOMPRESSDELETES NOCOMPRESSUPDATES table {{ SCOTT.AAAA }}; table {{ SCOTT.BBBB }}; 2.增加pump进程  add extract p02 exttrailsource /data/ogg19oracle/dirdat/eb add rmttrail /data/ogg19mysql/dirdat/eb, extract p02 3.</description>
    </item>
    
    <item>
      <title>OracleGoldenGate运维常用命令</title>
      <link>/oracle/oraclegoldengate%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4ogg/</link>
      <pubDate>Mon, 08 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/oracle/oraclegoldengate%E8%BF%90%E7%BB%B4%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4ogg/</guid>
      <description>常用命令 1. 管理(MGR)进程命令 INFO MANAGER 返回有关管理器端口和进程id的信息。 START MANAGER 开启管理进程 STATUS MANAGER 返回管理进程状态 STOP MANAGER 停止管理进程 2. 采集(EXTRACT)进程命令 ADD EXTRACT 添加一个采集组 ALTER EXTRACT 更改采集组的属性 CLEANUP EXTRACT 删除采集组的运行历史记录 DELETE EXTRACT 删除采集组。 INFO EXTRACT 返回有关采集组的信息。 KILL EXTRACT 强制终止采集组。 LAG EXTRACT 返回有关采集延迟的信息。 REGISTER EXTRACT 向Oracle数据库注册采集组START EXTRACT 启动采集组 STATS EXTRACT 返回处理采集的统计信息。 STATUS EXTRACT 返回采集组的状态 STOP EXTRACT 停止采集组。 FORCEAPPEND 允许data pump在现有的初始加载文件上添加新的跟踪文件 UNREGISTER EXTRACT 从Oracle数据库注销采集组。 3. 回放(Replicat)进程命令 ADD REPLICAT 添加一个复制组 ALTER REPLICAT 更改复制组的属性。 CLEANUP REPLICAT 删除复制组的运行历史。 DELETE REPLICAT 删除一个复制组。 INFO REPLICAT 返回关于复制组的信息。 KILL REPLICAT 强制终止一个复制组。 LAG REPLICAT 返回关于复制延迟的信息。 REGISTER REPLICAT 向Oracle数据库注册一个复制组。 START REPLICAT 启动一个复制组。 STATS REPLICAT 返回处理一个复制组的统计信息。 STATUS REPLICAT 返回一个复制组的状态。 STOP REPLICAT 停止复制组。 4.</description>
    </item>
    
    <item>
      <title>拜占庭将军的问题TheByzantineGeneralsProblem</title>
      <link>/book/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Fri, 05 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%8B%9C%E5%8D%A0%E5%BA%AD%E5%B0%86%E5%86%9B%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>什么是拜占庭将军问题？（The Byzantine Generals Problem） 拜占庭  拜占庭位于如今的土耳其的伊斯坦布尔,是当时东罗马帝国的首都 拜占庭将军问题它是由2013年图灵奖获得者计算机大神兰伯特在1982年发表的论The Byzantine Generals Problem提出。 拜占庭将军问题不是一个真实存在的问题，而是一个虚拟问题。 拜占庭将军问题本质是，在存在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。   由于当时拜占庭罗马帝国国土辽阔,为了防御目的，因此每个军队都分隔很远，将军与将军之间只能靠信差传消息。在战争的时候，拜占庭军队内所有将军和副官必需达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营。但是，在军队内有可能存有叛徒和敌军的间谍，左右将军们的决定又扰乱整体军队的秩序。在进行共识时，结果并不代表大多数人的意见。这时候，在已知有成员谋反的情况下，其余忠诚的将军在不受叛徒的影响下如何达成一致的协议，拜占庭问题就此形成
 拜占庭将军问题  拜占庭罗马帝国国土辽阔，为了达到防御目的，每个军队都分隔很远，将军与将军之间只能靠信差传消息。在战争的时候，拜占庭军队内所有将军和副官必须达成一致的共识，决定是否有赢的机会才去攻打敌人的阵营。 一组拜占庭将军分别各率领一支军队共同围困一座城市。为了简化问题，将各支军队的行动策略限定为进攻或撤离两种。因为部分军队进攻，部分军队撤离可能会造成灾难性后果，因此各位将军必须通过投票来达成一致策略，即所有军队一起进攻或所有军队一起撤离。 因为各位将军分处城市不同方向，他们只能通过信使互相联系。 在投票过程中每位将军都将自己投票给进攻还是撤退的信息通过信使分别通知其他所有将军，这样一来每位将军根据自己的投票和其他所有将军送来的信息就可以知道共同的投票结果而决定行动策略。 系统的问题在于，可能将军中出现叛徒，他们不仅可能向较为糟糕的策略投票，还可能选择性地发送投票信息。由于将军之间需要通过信使通讯，叛变将军可能通过伪造信件来以其他将军的身份发送假投票。 而即使在保证所有将军忠诚的情况下，也不能排除信使被敌人截杀，甚至被敌人间谍替换等情况。 因此很难通过保证人员可靠性及通讯可靠性来解决问题。这时候，在已知有成员谋反的情况下，其余忠诚的将军在不受叛徒的影响下如何达成一致的协议，就由此形成了历史中鼎鼎有名的拜占庭将军问题。  解决方案 方案一：口头协议  设总人数为n, 叛徒数为m,只要满足 n＞3m, 那这个问题就是可以解决的。 我们可以将拜占庭将军问题简化为将军或司令和副官模型。将军是第一个提出建议的人，副官可以执行或不执行将军的命令。 这里需要注意的是：Lamport提出的容错的两个条件：  IC1：即所有的忠诚的副官要遵守同一个命令，即达成一致； IC2：假如将军是忠诚的，那么每一个忠诚的副官都应该按照将军的意思行事。（如果将军是叛徒，所以只要满足IC1条件即可。）    情景:m=1,n=3（出错了）  将军G(General), 副官1和副官2 当叛徒数m=1,总人数=3时 假设将军是忠诚的，副官1是叛徒。那么就会出现一下情况：将军发起进攻命令，1和2副官收到的都是进攻命令。1是叛徒，1收到进攻，同步给2后退，此时2收到1进攻1后退,2不知道怎么办。  情景:m=1,n=4（可行）  将军仍然记为G,副官分别为1,2,3。  假设将军G是忠诚的，副官2是叛徒  将军G对1,2,3号副官发起进攻命令，那1会从2那里得到撤退命令，从3那里得到进攻命令，那1得到的命令集合就是2个进攻1个撤退命令；同样3，得到的命令集合也是2个进攻1个撤退。  假设将军G是叛徒，副官1,2,3都是忠诚的。  将军对1,3号副官发起进攻命令，对2副官发起撤退命令，那1会从2那里得到撤退命令，会从3那里得到进攻命令，1得到的命令集合就是2攻击1撤退，同理2,3都是2攻击1撤退。 1,2,3会同时进攻，满足IC1  方案二：书面协议  之所以会出现在口头传达中的那些错误是因为一些叛徒可以说谎，这里通过签名就是为了防止说谎。在签名算法中加了两个条件： （a）忠诚将军的签名是不能伪造的，内容修改可检测。（即 即使是叛徒也要原封不动的签了名将消息转发出去） （b）任何人都可以识别将军的签名，叛徒可以伪造叛徒司令的签名。 而且这里规定：每条消息只可以复制，然后加上自己的姓名再发出去。 下面是具体的算法：   初始化中的 Vi 类似于一个集合，表示的是第i个将军收到的命令，比如 Vi= {Attack} 之所以说是个集合是因为Vi里面不会有重复的命令出现。这在算法步骤（2）的（B) 部分描述的很清楚。   在算法步骤（1）中将军将签了自己姓名的消息广播发给所有副官。注意这里发的格式是 V:0，V是命令，0代表自己的身份。  算法步骤（2）（A）中，每个副官将收到的消息 V:0 把命令V放入自己的命令集合Vi（因为初始的时候他们的命令集合都是空的，所以不存在重复问题） 然后他们将命令拷贝，然后加上自己的签名 ，得到消息： V:0:i 然后再发给其他的副官。   在算法步骤（2）（B）中因为副官i 也会收到别的副官发来的消息v:0:1:&amp;hellip;:jk.</description>
    </item>
    
    <item>
      <title>8种常用于数据库的数据结构</title>
      <link>/dba/8%E7%A7%8D%E5%B8%B8%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</link>
      <pubDate>Tue, 02 Jan 2024 00:00:00 +0000</pubDate>
      
      <guid>/dba/8%E7%A7%8D%E5%B8%B8%E7%94%A8%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/</guid>
      <description>1.跳跃表(skipList) 什么是skiplist  跳跃表（skiplist）是一种随机化的数据， 由 William Pugh 在论文《Skip lists: a probabilistic alternative to balanced trees》中提出， 跳跃表以有序的方式在层次化的链表中保存元素， 效率和平衡树媲美 —— 查找、删除、添加等操作都可以在对数期望时间下完成， 并且比起平衡树来说， 跳跃表的实现要简单直观得多。  图示    用途：  Redis  2.哈希索引（Hash Index） 什么是hash Index  基于哈希表实现，只有精确匹配索引所有列的查询才有效。对于每一行数据，存储引擎都会对所有的索引列计算一个哈希码（hash code），哈希码是一个较小的值，并且不同键值的行计算出来的哈希码也不一样。哈希索引将所有的哈希码存储在索引中，同时在哈希表中保存指向每个数据行的指针。 哈希索引可细分为静态哈希和动态哈希这两大类，  静态哈希  基于散列技术的文件组织使我们能够避免访问索引结构，同时也提供了一种构造索引的方法。在对散列的描述中，使用桶(bucket)来表示能存储一条或多条记录的一个存储单位。通常一个桶就是一个磁盘块，但也可能大于或者小于一个磁盘块。 散列索引将散列函数作用于搜索码以确定对应的桶， 然后将此搜索码以及对应的指针存入此桶(或溢出桶)中。 静态散列最大的缺点在于必须在实现系统时选择确定的散列函数。此后若被索引的文件变大或缩小，要想再改变散列函数就不容易了。因为散列函数 h 将搜索码值映射到桶地址的固定集合 B 上： 根据当前文件大小选择散列函数，这样的选择会使得性能随着数据库的增大而下降。换言之，初始时集合 B 太小，一个桶就会包含许多不同的搜索码值的记录，从而可能发生桶溢出。当文件变大时，性能就会受到影响。 根据将来某个时刻文件的预计大小选择散列函数。 尽管这样可以避免性能下降，但是初始时会造成相当大的空间浪费。  动态哈希  针对静态散列技术出现的问题，动态散列（dynamic hashing）技术允许散列函数动态改变，以适应数据库增大或缩小的需要 当数据库增大或缩小时，可扩充散列可以通过桶的分裂或合并来适应数据库大小的变化，这样可以保持空间的使用效率。此外，由于重组每次仅作用于一个桶，因此所带来的性能开销较低。  图示    3.ssTable 什么是ssTable  SSTable文件是memtable 数据到一定阈值写入文件形成的，由于内存容量总是有限的，将一定量数据写入磁盘可以存放更多数据，所以leveldb相比redis能存放更多数据。既然数据持久化到磁盘，那么还有必然涉及到从磁盘中查询数据，从磁盘中查询数据与从内存中查询数据的效率是不一样的，所以SSTable 数据组织方式必然与众不同，因为必须要提高查询效率，不能给一个key就去遍历所有SSTable。因此本文的另一个目的就是学习SSTable 文件如何组织key-value，提高查询效率。为了提高内存中数据查询效率 我们学习了各种数据结构如红黑树，散列表，那么SSTable是学习如何提高文件查询数据效率的一个很好例子。  图示    4.</description>
    </item>
    
    <item>
      <title>Oracle处理归档日志archivelog空间报警</title>
      <link>/oracle/oracle%E5%A4%84%E7%90%86%E5%BD%92%E6%A1%A3%E6%97%A5%E5%BF%97archivelog%E7%A9%BA%E9%97%B4%E6%8A%A5%E8%AD%A6/</link>
      <pubDate>Fri, 29 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/oracle/oracle%E5%A4%84%E7%90%86%E5%BD%92%E6%A1%A3%E6%97%A5%E5%BF%97archivelog%E7%A9%BA%E9%97%B4%E6%8A%A5%E8%AD%A6/</guid>
      <description>查看归档日志空间占用率  select * from v$flash_recovery_area_usage; --查看空间占用率 select * from v$recovery_file_dest; --查看归档日志的存放地址; show parameter recovery; --查看归档空间大小 增加表空间大小（磁盘空间足够的话） alter system set db_recovery_file_dest_size=250G; --请归档空间增大到250G 删除过期文件 export ORACLE_SID=****** rman target / crosscheck archivelog all; //查看可以所有的归档文件 delete expired archivelog all; //清空过期的归档文件 delete force noprompt ARCHIVELOG ALL COMPLETED BEFORE &#39;trunc(SYSDATE)-90&#39;; //清除90天前的归档文件 quit </description>
    </item>
    
    <item>
      <title>ApacheDoris在Centos7环境下的安装部署</title>
      <link>/dba/apachedoris%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Mon, 25 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/apachedoris%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/</guid>
      <description>环境准备 linux版本 Centos7:  cat /etc/redhat-release  系统最大打开文件句柄数​ cat etc/security/limits.conf * soft nofile 65536* hard nofile 65536关闭交换分区（swap）​ swapoff -ased -i &#39;/ swap / s/^\(.*\)$/#\1/g&#39; /etc/fstab检查文件系统  df -hT |grep &amp;quot;ext4&amp;quot;GCC版本(&amp;gt;=4.8.2) gcc -v# gcc version 4.8.5 2015062JAVA版本(&amp;gt;=1.8) java -version # java version &amp;quot;1.8.0_202&amp;quot;确认cpu是否支持avx2 cat /proc/cpuinfo | grep avx2安装 下载 cd /data/software/wget https://apache-doris-releases.oss-accelerate.aliyuncs.com/apache-doris-2.0.3-bin-x64.tar.gztar zxvf apache-doris-2.0.3-bin-x64.tar.gzcd cd apache-doris-2.0.3-bin-x64llmkdir /data/doris5306mv * /data/doris5306/安装fe 修改fe.</description>
    </item>
    
    <item>
      <title>2023年冬至</title>
      <link>/book/2023%E5%B9%B4%E5%86%AC%E8%87%B3/</link>
      <pubDate>Mon, 18 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/2023%E5%B9%B4%E5%86%AC%E8%87%B3/</guid>
      <description> 周末回了趟老家“做冬至” 冬至 在北方习俗是“吃饺子” 在江淮地区的习俗是习烧纸祭祖 现在也有文雅的说法叫「送冬衣」 表示冬天要到来了 担心已过世的亲人在彼岸受冻 要给他们汇去置办过冬衣物和钱财 用周末时间匆匆的回了趟老家  行程  说来也巧 自从主动接受了延迟和晚点这些设定了以后 经常会赶上高铁晚点 这次回合肥高铁晚点90分钟 合肥回北京高铁又晚点20分钟 还好心态平和 不急不燥的 甚至想安慰接站的人不要急 当然我姐因为老是喜欢省停车费 不愿意把车停到停车场 守在高铁站外面 约好在落客平台碰面 快到站的那会 叮当打电话问到哪了 整得很匆忙 主观上如果不着急 那么客观上产生的意外 只会delay的时间表 不会delay到我们的情绪  天气  冷 非常冷 赶上寒潮了 回去的第一天还有太阳 温度还是0度往上一点的 到了第二天 阴天，零下3度 南方的湿冷天 那种冷是侵入式的 直接给我冻得缩在屋里不愿出门 当然在北京 这大冬天的我也不愿意户外活动 太笨重了，放不开手脚 冬天还是宅一点舒服 本来计划周日的上午 去组织一下附近的老同学聚个餐的 后来一看时间有点挤 索性窝在家里 陪老张聊聊天 跟小外甥一起去外面砸冰玩水 节奏缓慢而悠闲 正好适应这寒冷的冬天 可惜没有太阳 要不然我能倚在院里角落里晒太阳  家人  小外甥又长大了一点 可以很流畅的表达想要什么 见面花了半个小时熟悉后 就会很亲近 拉着我陪他玩 晚上睡觉时 要跟我一屋睡 最后被姐夫拖走了 不是很顽皮也不爱哭 但是很倔 想玩的和想吃的 会很执着 怎么分散注意力 也会记着这事 可能明年就要上幼儿园了 我姐的关注力都在小外甥身上 把小孩照顾得很好 这次回去 也给我姐买了盒护手霜 然后我姐非常开心 因为经常回去 但每次都是买酒和玩具 这是头一次专门给她买了礼物 然后她据此推断这次的礼物是经人提点的 她对了一半 创意是别人给的 但是订单和选品都是我来的 给老张和大伯分别买了烟酒 老张说他需要几双结实的袜子 提前在浙江的袜子厂 批发了70双我经常穿的的棉袜 满满的一个大包裹 老张说他能穿好几年了 老张的身体状态似乎好了点 但是还是不太乐观 也没什么好办法 我没多少时间可以陪他 他适应不了在城市的生活 上次老张努力尝试在北京生活的时候 赶上了疫情爆发让他崩溃了 再后来他连来北京待几天的兴趣都没了 这次我再提天气暖和的时候来北京住一段时间 他一开始说不来 后来我说跟我姐，姐夫一家人一起 还有大伯 组队过来看看天安门啥的 才来了兴致 大伯更是高兴 因为他竟然没来过北京 虽然每次回家都会给他捎一些北京的特产 但是忘了给大伯安排一次来北京看看的行程 这个想起来有点不应该 现在大伯已经80岁了 小时候我觉得有大家长感觉的长辈 不知不觉间就老了 跟我说视力很差了 这种很亲近的长辈正变得沧老 会让人产生很强烈的不舍和无奈 我的父辈们正在老去 他们年轻时候的成熟和担当 我都没有学会 虽然年轻的心态是件好事 但是成熟和责任心 积极的行事风格 也要有稳定的情绪表达 希望在下一辈小朋友们的心里 我不止是一个愿意陪他们玩陪他们疯的长辈 也是个有责任心有担当的长辈 一个能温和的愿意接纳不同的观点和意见的长者  </description>
    </item>
    
    <item>
      <title>AIGC在哔哩哔哩内部的使用场景探索</title>
      <link>/ai/aigc%E5%9C%A8%E5%93%94%E5%93%A9%E5%93%94%E5%93%A9%E5%86%85%E9%83%A8%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%8E%A2%E7%B4%A2/</link>
      <pubDate>Thu, 07 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/aigc%E5%9C%A8%E5%93%94%E5%93%A9%E5%93%94%E5%93%A9%E5%86%85%E9%83%A8%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%8E%A2%E7%B4%A2/</guid>
      <description> 2023年11月24日 系统架构师大会在上海举行 其中有一场b站OTT&amp;amp;漫画技术部邬晶的分享 《大模型与AIGC 在B站OTT、漫画业务中的应用探索》 分享中提到AIGC在哔哩哔哩内部的使用场景探索 包括以下几个场景的尝试  客服助手  编码助手 漫画辅助创作 辅助内容处理 合同格式化 辅助本地化      我感兴趣的有：客服助手，合同格式化  AI客服助手 背景：  客服侧同学需要同时处理多个业务线，每条业务线也一直处 于高速迭代状态，业务细节多，用户问题杂，为了帮助客服 同学提高问题处理效率，我们基于ChatGPT3.5输出了智能客服系统  解决方案 挑战 收益 拦截率:88.5% -&amp;gt; 92% 节省人力:13%
AI合同格式化 背景： 解决方案 挑战 收益  目前还在开发进行中 目标: 将合同电子化、平台化。各种权利项平滑入库，提升可维护性。降低商务成本。  小结  已完成的AI客服助手方案 是比较现实的可以落地的解决方案 也就是利用向量数据库来做关联匹配 实际运行中遇到的挑战 也是做此类产口需要关注的 这个分享说得非常具体了 合同格式化的是件收益不错的事B站也在开发中  </description>
    </item>
    
    <item>
      <title>快速完成一个异构数据同步异常检查功能</title>
      <link>/dba/%E5%BF%AB%E9%80%9F%E5%AE%8C%E6%88%90%E4%B8%80%E4%B8%AA%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%BC%82%E5%B8%B8%E6%A3%80%E6%9F%A5%E5%8A%9F%E8%83%BD/</link>
      <pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%BF%AB%E9%80%9F%E5%AE%8C%E6%88%90%E4%B8%80%E4%B8%AA%E5%BC%82%E6%9E%84%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%BC%82%E5%B8%B8%E6%A3%80%E6%9F%A5%E5%8A%9F%E8%83%BD/</guid>
      <description>起因  上周我们做的一次Oracle到MySQL迁移，在迁移完成后需要将MySQL数据反向同步到Oracle中，以便于没有迁移干净的原Oracle业务还能提供只读访问 反向同步用的是OGG，我们在OGG同步层面加了异常报警（但是被DBA给人为禁用了 除掉同步软件层面的报警外，我们需要一个偏业务层面的，针对表数据的报警和监控  需求  支持各种数据源的同步对比 支持表的行数对比，最大id对比，表最后更新时间对比 扩展性好，方便配置 异常报警 监控同步状态的页面  解决 拆解思路  需要一个建两张表：表1:存放任务配置信息，表2:存放采集到的数据 需要一个任务：定时（5分钟）去源库和目标库，运行一个SQL,取当前状态并存入到刚才建的history表中 需要一个页面：展示采集结果，可以方便的查看同步状态和延时 需要一个报警任务：异常数据时，发送报警  任务1:建表  CREATE TABLE `msync_config` (`sync_name` varchar(50) NOT NULL,`source_linkconnstr` varchar(100) NOT NULL,`target_linkconnstr` varchar(100) NOT NULL,`source_sqlstr` varchar(2000) NOT NULL,`target_sqlstr` varchar(2000) NOT NULL,`alert_count` int NOT NULL DEFAULT &#39;5&#39;,`alert_maxid` int NOT NULL DEFAULT &#39;5&#39;,`alert_delaysecond` int NOT NULL DEFAULT &#39;60&#39;,`alert_userlist` varchar(100) NOT NULL DEFAULT &#39;&#39;,`add_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP,`update_time` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,`alert_type` varchar(50) NOT NULL DEFAULT &#39;count+time&#39; COMMENT &#39;count,id,time三种组合,+代表and,-代表or&#39;) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3 CREATE TABLE `msync_history` (`addtime` datetime NOT NULL,`sync_name` varchar(50) NOT NULL,`source_count` bigint NOT NULL DEFAULT &#39;0&#39;,`target_count` bigint NOT NULL DEFAULT &#39;0&#39;,`source_maxid` bigint NOT NULL DEFAULT &#39;0&#39;,`target_maxid` bigint NOT NULL DEFAULT &#39;0&#39;,`source_maxtime` datetime NOT NULL DEFAULT &#39;0000-00-00 00:00:00&#39;,`target_maxtime` datetime NOT NULL DEFAULT &#39;0000-00-00 00:00:00&#39;,PRIMARY KEY (`addtime`,`sync_name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3任务2:配置采集任务 &amp;lt;action result=&amp;quot;allcount,okcount,errcount&amp;quot; type=&amp;quot;sql_loop&amp;quot; &amp;gt;&amp;lt;connstr&amp;gt;link:dboop_db&amp;lt;/connstr&amp;gt;&amp;lt;sqlstr&amp;gt;select sync_name,source_linkconnstr,target_linkconnstr,source_sqlstr,target_sqlstr from msync_config&amp;lt;/sqlstr&amp;gt;&amp;lt;action result=&amp;quot;sync_name,source_linkconnstr,target_linkconnstr,source_sqlstr,target_sqlstr&amp;quot; type=&amp;quot;sql_select&amp;quot; &amp;gt;&amp;lt;connstr&amp;gt;link:dboop_db&amp;lt;/connstr&amp;gt;&amp;lt;sqlstr&amp;gt;select sync_name,source_linkconnstr,target_linkconnstr,source_sqlstr,target_sqlstr from msync_config where sync_name=%s&amp;lt;/sqlstr&amp;gt;&amp;lt;sqlpara&amp;gt;{__0}&amp;lt;/sqlpara&amp;gt;&amp;lt;/action&amp;gt;&amp;lt;!</description>
    </item>
    
    <item>
      <title>MySQL内置函数</title>
      <link>/mysql/mysql%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/</link>
      <pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0/</guid>
      <description>MySQL字符串函数    函数 说明     ASCII(s) 返回字符串 s 的第一个字符的 ASCII 码。   CHAR_LENGTH(s) 返回字符串 s 的字符数   CHARACTER_LENGTH(s) 返回字符串 s 的字符数   CONCAT(s1,s2&amp;hellip;sn) 字符串 s1,s2 等多个字符串合并为一个字符串   CONCAT_WS(x, s1,s2&amp;hellip;sn) 同 CONCAT(s1,s2,&amp;hellip;) 函数，但是每个字符串直接要加上 x，x 可以是分隔符   FIELD(s,s1,s2&amp;hellip;) 返回第一个字符串 s 在字符串列表(s1,s2&amp;hellip;)中的位置   FIND_IN_SET(s1,s2) 返回在字符串s2中与s1匹配的字符串的位置   FORMAT(x,n) 函数可以将数字 x 进行格式化 &amp;ldquo;#,###.##&amp;rdquo;, 将 x 保留到小数点后 n 位，最后一位四舍五入。   INSERT(s1,x,len,s2) 字符串 s2 替换 s1 的 x 位置开始长度为 len 的字符串   LOCATE(s1,s) 从字符串 s 中获取 s1 的开始位置   LCASE(s) 将字符串 s 的所有字母变成小写字母   LEFT(s,n) 返回字符串 s 的前 n 个字符   LEFT(s,n) 返回字符串 s 的前 n 个字符   LENGTH(str) 返回字符的长度，一个汉字算三个字符，一个数字或字母算一个字符。   LOCATE(s1,s) 从字符串 s 中获取 s1 的开始位置   LOWER(s) 将字符串 s 的所有字母变成小写字母。   LPAD(s1,len,s2) 在字符串 s1 的开始处填充字符串 s2，使字符串长度达到 len   LTRIM(s) 去掉字符串 s 开始处的空格   MID(s,n,len) 从字符串 s 的 start 位置截取长度为 length 的子字符串，同 SUBSTRING(s,n,len)   POSITION(s1 IN s) 从字符串 s 中获取 s1 的开始位置   REPEAT(s,n) 将字符串 s 重复 n 次   REPLACE(str,from_str,to_str) 将字符串 str 中出现的字符串 from_str 替代成字符串 to_str，并返回替换后新字符串，该方法对大小写敏感。   REVERSE(s) 将字符串s的顺序反过来   RIGHT(s,n) 返回字符串 s 的后 n 个字符   RPAD(s1,len,s2) 在字符串 s1 的结尾处添加字符串 s2，使字符串的长度达到 len   RTRIM(s) 去掉字符串 s 结尾处的空格   SPACE(n) 返回 n 个空格   STRCMP(s1,s2) 比较字符串 s1 和 s2，如果 s1 与 s2 相等返回 0 ，如果 s1&amp;gt;s2 返回 1，如果 s1&amp;lt;s2 返回 -1   SUBSTR(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串   SUBSTRING(s, start, length) 从字符串 s 的 start 位置截取长度为 length 的子字符串   SUBSTRING_INDEX(s, delimiter, number) 返回从字符串 s 的第 number 个出现的分隔符 delimiter 之后的子串。如果 number 是正数，返回第 number 个字符左边的字符串。如果 number 是负数，返回第(number 的绝对值(从右边数))个字符右边的字符串。   如果 number 是负数，返回第(number 的绝对值(从右边数))个字符右边的字符串。    TRIM(str) 去掉字符串 str 开始和结尾处的指定的符号，不指定默认为空格。   UCASE(s) 将字符串转换为大写   UPPER(s) 将字符串转换为大写    MySQL数字函数    函数 说明     ABS(x) 返回 x 的绝对值　   ACOS(x) 求 x 的反余弦值(参数是弧度)   ASIN(x) 求反正弦值(参数是弧度)   ATAN(x) 求反正切值(参数是弧度)   ATAN2(n, m) 求反正切值(参数是弧度)   AVG(expression) 返回一个表达式的平均值，expression 是一个字段   CEIL(x) 返回大于或等于 x 的最小整数　   CEILING(x) 返回大于或等于 x 的最小整数　   COS(x) 求余弦值(参数是弧度)   COT(x) 求余切值(参数是弧度)   COUNT(expression) 返回查询的记录总数，expression 参数是一个字段或者 * 号   DEGREES(x) 将弧度转换为角度　   n DIV m 整除，n 为被除数，m 为除数   EXP(x) 返回 e 的 x 次方　   FLOOR(x) 返回小于或等于 x 的最大整数　   GREATEST(expr1, expr2, expr3, &amp;hellip;) 返回列表中的最大值   LEAST(expr1, expr2, expr3, &amp;hellip;) 返回列表中的最小值   LN 返回数字的自然对数   LOG(x) 返回自然对数(以 e 为底的对数)　   LOG10(x) 返回以 10 为底的对数　   LOG2(x) 返回以 2 为底的对数   MAX(expression) 返回字段 expression 中的最大值   MIN(expression) 返回字段 expression 中的最小值   MOD(x,y) 返回 x 除以 y 以后的余数　   PI() 返回圆周率　   POW(x,y) 返回 x 的 y 次方　   POWER(x,y) 返回 x 的 y 次方　   RADIANS(x) 将角度转换为弧度　   RAND() 返回 0 到 1 的随机数　   ROUND(x) 返回离 x 最近的整数   SIGN(x) 返回 x 的符号，x 是负数、0、正数分别返回 -1、0 和 1　   SIN(x) 求正弦值(参数是弧度)　   SQRT(x) 返回x的平方根　   SUM(expression) 返回指定字段的总和   TAN(x) 求正切值(参数是弧度)   TRUNCATE(x,y) 返回数值 x 保留到小数点后 y 位的值（与 ROUND 最大的区别是不会进行四舍五入）    MySQL日期时间函数    函数 说明     ADDDATE(d,n) 计算起始日期 d 加上 n 天的日期   ADDTIME(t,n) 时间 t 加上 n 秒的时间   CURDATE() 返回当前日期   CURRENT_DATE() 返回当前日期   CURRENT_TIME 返回当前时间   CURRENT_TIMESTAMP() 返回当前日期和时间   CURTIME() 返回当前时间   DATE() 从日期或日期时间表达式中提取日期值   DATEDIFF(expr1, expr2) 计算日期 expr1 表达式与日期 expr2 之间相隔的天数。   DATE_ADD(d，INTERVAL expr type) 计算起始日期 d 加上一个时间段后的日期   DATE_FORMAT(d,f) 按表达式 f的要求显示日期 d   DATE_SUB(date, INTERVAL expr unit) 从指定日期减去指定的日期时间间隔。   DAY(d) 返回日期值 d 的日期部分   DAYNAME(d) 返回日期 d 是星期几，如 Monday,Tuesday   DAYOFMONTH(d) 计算日期 d 是本月的第几天   DAYOFWEEK(d) 日期 d 今天是星期几，1 星期日，2 星期一，以此类推   DAYOFYEAR(d) 计算日期 d 是本年的第几天   EXTRACT(type FROM d) 从日期 d 中获取指定的值，type 指定返回的值。type可取值为：,MICROSECOND,SECOND,MINUTE,HOUR,DAY,WEEK,MONTH,QUARTER,YEAR,SECOND_MICROSECOND,MINUTE_MICROSECOND,MINUTE_SECOND,HOUR_MICROSECOND,HOUR_SECOND,HOUR_MINUTE,DAY_MICROSECOND,DAY_SECOND,DAY_MINUTE,DAY_HOUR,YEAR_MONTH   FROM_DAYS(n) 计算从 0000 年 1 月 1 日开始 n 天后的日期   HOUR(t) 返回 t 中的小时值   LAST_DAY(d) 返回给给定日期的那一月份的最后一天   LOCALTIME() 返回当前日期和时间   LOCALTIMESTAMP() 返回当前日期和时间   MAKEDATE(year, day-of-year) 基于给定参数年份 year 和所在年中的天数序号 day-of-year 返回一个日期   MAKETIME(hour, minute, second) 组合时间，参数分别为小时、分钟、秒   MICROSECOND(date) 返回日期参数所对应的毫秒数   MINUTE(t) 返回 t 中的分钟值   MONTHNAME(d) 返回日期当中的月份名称，如 Janyary   MONTH(d) 返回日期d中的月份值，1 到 12   NOW() 返回当前日期和时间。   PERIOD_ADD(period, number) 为 年-月 组合日期添加一个时段   PERIOD_DIFF(period1, period2) 返回两个时段之间的月份差值   QUARTER(d) 返回日期d是第几季节，返回 1 到 4   SECOND(t) 返回 t 中的秒钟值   SEC_TO_TIME(s) 将以秒为单位的时间 s 转换为时分秒的格式   STR_TO_DATE(string, format_mask) 将字符串转变为日期   SUBDATE(d,n) 日期 d 减去 n 天后的日期   SUBTIME(t,n) 时间 t 减去 n 秒的时间   SYSDATE() 返回当前日期和时间   TIME(expression) 提取传入表达式的时间部分   TIME_FORMAT(t,f) 按表达式 f 的要求显示时间 t   TIME_TO_SEC(t) 将时间 t 转换为秒   TIMEDIFF(time1, time2) 计算时间差值   TIMESTAMP(expression, interval) 单个参数时，函数返回日期或日期时间表达式；有2个参数时，将参数加和   TO_DAYS(d) 计算日期 d 距离 0000 年 1 月 1 日的天数   UNIX_TIMESTAMP([date]) 获取对应的时间戳，无参时，返回当前的时间戳。   WEEK(d) 计算日期 d 是本年的第几个星期，范围是 0 到 53   WEEKDAY(d) 日期 d 是星期几，0 表示星期一，1 表示星期二   WEEKOFYEAR(d) 计算日期 d 是本年的第几个星期，范围是 0 到 53   YEAR(d) 返回年份   YEARWEEK(date, mode) 返回年份及第几周（0到53），mode 中 0 表示周天，1表示周一，以此类推    MySQL聚合函数    函数 说明     AVG() 返回指定参数的平均值。   BIT_AND() 返回表达式的所有位按位 AND 结果。   BIT_OR() 返回表达式的所有位按位 OR 结果。   BIT_XOR() 返回表达式的所有位按位 XOR 结果。如果参数为 NULL，返回值也将为 NULL。   COUNT() 返回条数。   COUNT(DISTINCT) 返回指定不同值的条数。   GROUP_CONCAT() 将 GROUP BY 产生的同一个分组中的值连接起来，返回一个字符串结果。   MAX() 返回最大值。   MIN() 返回最小值。   STD() 返回总体标准差。   STDDEV() 返回总体标准差。   STDDEV_POP() 返回总体标准差。   STDDEV_SAMP() 返回样本标准差。   SUM() 返回运算和的结果。   VAR_POP() 返回总体方差。   VAR_SAMP() 返回样本方差。   VARIANCE() 返回总体方差。    MySQL其他函数    函数 说明     BIN(x) 返回 x 的二进制编码   BINARY(s) 将字符串 s 转换为二进制字符串   case when else end 分支选择   CAST(expr AS type [ARRAY]) 转换数据类型   COALESCE(expr1, expr2, &amp;hellip;.</description>
    </item>
    
    <item>
      <title>MySQL8.0尝试用json索引替换全文索引</title>
      <link>/mysql/mysql8.0%E5%B0%9D%E8%AF%95%E7%94%A8json%E7%B4%A2%E5%BC%95%E6%9B%BF%E6%8D%A2%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/</link>
      <pubDate>Mon, 27 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql8.0%E5%B0%9D%E8%AF%95%E7%94%A8json%E7%B4%A2%E5%BC%95%E6%9B%BF%E6%8D%A2%E5%85%A8%E6%96%87%E7%B4%A2%E5%BC%95/</guid>
      <description>原因  MySQL8.0.22版本 线上有一张task表的users字段因历史原因 存放了以逗号分隔的用户id列表 程序中会匹配用户id进行查询 用到的SQL如下：  # 查询 select users from task limit 2; | users | |300511164303031, 310406164883350, 151134164673502, 330203164377115, 310633164035316, 310408164888300, 170515164003106, 150636164603618, 310510164335822, 151336164653174, 310508164331806, 301115164423156 | |310406164883350,181138164432020,1000130,330312164322768,170515164003106,300608164825431,331015164472774,150304164442136,331108164613233,1000164,301113164430265,171016164003026,300333164732303,151134164673502,1000143,331034164487883,181033164253337,310633164035316,150304164442101,1000136,330312164636073,310508164331806,330302164334267,181017164275220,301115164423156,330203164377336,310303164733465,330312164322726,330203164377115,310408164888300,311116164231848,1000123,310214164825778,301317164618388,300333164732155,151013164628330,300511164303031,1000138,1000185,150636164603618,300415164783624,310237164871433,310510164335822,151336164653174,330210164387154 | -- 数据和表名，列名已做掩码转换。非真实数据 # 示例 select * from task where MATCH(users) AGAINST(&#39;19323422341234&#39; );	 表的数据量不多40多万条记录 但是频繁的出现慢查询(超过500毫秒)  优化思路  定位到全文索引慢的时候 第一反应是拆了这个全文索引查询 业务方将逗号字段拆表的改动量大暂时不考虑 折中办法是将这个字段换成json类型 然后用json的索引来替换全文索引 我在想这个方案的时候 给忠哥的预估是性能会提升3-10倍 当时没做测试 靠的是经验和信口开河 一通怂恿说服了研发同事 开始拉群开整  验证和测试 # 加json字段 alter table task add users_list json ; # 填值 update task set users_list = concat(&#39;[&#39; ,TRIM(BOTH &#39;,&#39; FROM users),&#39;]&#39;) where users is not null and users !</description>
    </item>
    
    <item>
      <title>python报错ModuleNotFoundError_No_module_named_lzma</title>
      <link>/ops/python%E6%8A%A5%E9%94%99modulenotfound_no_module_named_lzma/</link>
      <pubDate>Thu, 23 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/python%E6%8A%A5%E9%94%99modulenotfound_no_module_named_lzma/</guid>
      <description>执行Python脚本时报错  File &amp;quot;/usr/local/python3.9/lib/python3.9/site-packages/pooch/processors.py&amp;quot;, line 14, in &amp;lt;module&amp;gt; import lzma File &amp;quot;/usr/local/python3.9/lib/python3.9/lzma.py&amp;quot;, line 27, in &amp;lt;module&amp;gt; from _lzma import * ModuleNotFoundError: No module named &#39;_lzma&#39; 修复方法一：backports  安装 backports.lzma  yum install xz-devel -y yum install python-backports-lzma -y pip3.9 install backports.lzma  修改 lzma.py vim /usr/local/python3.9/lib/python3.9/lzma.py  #修改前 from _lzma import * from _lzma import _encode_filter_properties, _decode_filter_properties #修改后 try: from _lzma import * from _lzma import _encode_filter_properties, _decode_filter_properties except ImportError: from backports.</description>
    </item>
    
    <item>
      <title>安装百度飞浆PaddleSpeech</title>
      <link>/ai/%E5%AE%89%E8%A3%85%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%B5%86paddlespeech/</link>
      <pubDate>Wed, 22 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E5%AE%89%E8%A3%85%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%B5%86paddlespeech/</guid>
      <description>安装  pip3.9 install paddlepaddle -i https://mirror.baidu.com/pypi/simple pip3.9 install pytest-runner -i https://mirror.baidu.com/pypi/simple pip3.9 install paddlespeech -i https://mirror.baidu.com/pypi/simple 报错1:ImportError: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.22&#39; not found  解决：   find / -name &amp;quot;libstdc++.so.6*&amp;quot; #找到一个路径 export LD_LIBRARY_PATH=/root/anaconda3/pkgs/libstdcxx-ng-8.2.0-hdf63c60_1/lib/:$LD_LIBRARY_PATH 报错2: </description>
    </item>
    
    <item>
      <title>Centos7安装php8.1和composer</title>
      <link>/ops/centos7%E5%AE%89%E8%A3%85php8.1%E5%92%8Ccomposer/</link>
      <pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/centos7%E5%AE%89%E8%A3%85php8.1%E5%92%8Ccomposer/</guid>
      <description>设置yum源  yum -y install https://mirrors.tuna.tsinghua.edu.cn/remi/enterprise/remi-release-7.rpm yum -y install yum-utils 安装php8.1  yum -y install php81-php-cli php81-php-common php81-php-devel php81-php-embedded php81-php-fpm php81-php-gd php81-php-mbstring php81-php-mysqlnd php81-php-pdo php81-php-opcache php81-php-xml php81-php-soap php81-php-posix 查看是否安装成功及php配置文件以及对应目录 rpm -qa |grep php81 pm -ql php81-php-fpm 系统配置  ln -sf /opt/remi/php81/root/usr/bin/php* /usr/bin 查看PHP版本 php -v 安装：composer php -r &amp;quot;copy(&#39;https://install.phpcomposer.com/installer&#39;, &#39;composer-setup.php&#39;);&amp;quot; php composer-setup.php cp composer.phar /usr/bin/composer cd /www/wwwroot/www.top580.com/php composer install composer update </description>
    </item>
    
    <item>
      <title>MySQL组复制GroupReplication参数</title>
      <link>/mysql/mysql%E7%BB%84%E5%A4%8D%E5%88%B6groupreplication%E5%8F%82%E6%95%B0/</link>
      <pubDate>Tue, 14 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%BB%84%E5%A4%8D%E5%88%B6groupreplication%E5%8F%82%E6%95%B0/</guid>
      <description>参数 group_replication_allow_local_disjoint_gtids_join （已弃用）  布尔型，默认值为OFF，MySQL 5.7.17版本引入，5.7.21版本弃用，8.0.4版本中删除。 这是MySQL5.7时代搭MGR集群时经常会用到的参数，设置为开启后，新加节点变得更容易，但是慎用该参数，不正确的使用可能会导致复制组中的数据出现不一致。MySQL8版本中已弃用。 即使该组中缺失一些事务（joiner节点比组中的事务还要多），也允许joiner节点加入该组。   group_replication_allow_local_lower_version_join  布尔型，默认为OFF，MySQL 5.7.17版本引入。 也是新节点加入时使用到的变量，一般不会开启，开启后允许低版本的MySQL节点加入集群 MySQL 8.0.17及其之后的版本在比对版本号时，会考虑次要版本号（例如：MySQL 8.0.17，会将次要版本号17一起进行比较），在MySQL 8.0.16及其之前的版本在比对版本号时，只考虑主要版本号（例如：MySQL 5.7.22，只比对主要版本号5.7）。 将该系统变量设置为ON并不会使低版本的Server与组兼容，但能允许低版本Server加入组，不过没有任何措施来防止低版本Server与组中现有成员的不兼容行为，因此，为了确保低版本Server的正确操作，必须人为确保如下两项措施，如果不能确保这两项措施，则运行低版本的Server可能会碰到错误而导致加入组失败。 * 运行较低版本的Server加入组之前，必须先停止该Server上的任何写操作 * 从运行较低版本的Server加入组的位置开始，停止对组中所有成员的任何写操作   group_replication_auto_increment_increment  整型类型，默认值为7（如果你的组中有更多或更少的组成员，则，可以在组复制启动之前调整好该系统变量的值，以对应你的组中的成员数量），取值范围：1~65535。MySQL 5.7.17版本引入。 注意：该系统变量的值在所有组成员上必须相同 注意：一旦设置将代替系统变量auto_increment_inncrement。且将auto_increment_offset设置为Server id值 自动设置复制组中的每个成员的自增列的步长值，以确保在多主模式的组中，每个组成员的自增列值有序且不重叠。 当成员停止组复制时，普通系统变量auto_increment_inncrement和auto_increment_offset的值将会恢复原状（启动组复制之前的值） 只有当系统变量auto_increment_increment和auto_increment_offset保持默认值时，组复制启动时才会自动做与组复制的适配调整和恢复，如果这两个系统变量的值被设置了非默认值，则组复制不会做自动调整（从MySQL 8.0开始，当组处于单主模式下时，这两个系统变量也不会做自动调整）。所以，对于这两个系统变量的值，要么在组复制下不对其进行手工指定（让其使用默认值），要么就一定要设置正确，否则，在多主模式的主中，很容易造成主键冲突。 系统变量group_replication_auto_increment_increment在组复制运行时无法修改，需要先停止组复制，修改该系统变量的值，然后再启动组复制   group_replication_bootstrap_group  布尔型，默认为OFF，MySQL 5.7 17版本引入。 只在新建集群时第一个启动节点时用，用完就得关，别轻易设置，容易脑裂 指定使用哪个Server来引导组（这里指的是将此系统变量设置为ON的Server）。该系统变量只能在一个Server上设置，并且只能在首次引导组或重新引导整个组时在其中一个Server上设置。当复制组引导成功之后需要及时将该系统变量为OFF来动态关闭（在组所有可能涉及的Server的配置文件中建议统一将此系统变量设置为OFF）。如果在某Server上使用该变量引导复制组之后再在另外一个Server中使用该变量引导复制组，则如果两个Server使用了相同的组名称时，可能会产生人为的脑裂。   group_replication_consistency  注意：这是重要参数,控制事务一致性等级 从MySQL 8.0.14引进 对于绝大多数场景，使用默认的 EVENTUAL 等级就足够 一般我们建议用默认值或Before  1 EVENTUAL  在这个等级下，RO和RW事务执行前，都不会要求等待积压事务先行应用完成。 这是默认等级，也是在引入该选项前的行为。这意味着以下几点： RW事务无需等待，而可能先于其他节点进行外部化（将事务广播到其他节点）。 RO事务可能读取到旧数据。 在Primary节点切换时，新产生的RW事务有可能会因为冲突而回滚。  2 BEFORE_ON_PRIMARY_FAILOVER  当发生Primary节点切换时，在新的Primary上需要先等待把所有来自旧Primary节点的积压事务应用完毕，之后才能正式完成切换，转成ONLINE状态，成为新的Primary节点，继续响应新的事务请求。 这么做可以保证在发生故障转移时，客户端不会查到旧数据，保证了数据一致性，不过客户端上也可能会产生延迟等待。  3 BEFORE  RW事务在应用之前，RO事务在执行之前，都要先等待前面堆积的事务完成。 这可以保证RO事务总能读取到最新事务，但对于RW事务而言，只是等待堆积事务应用完成，但并不要求其他节点上也完成该事务。  4 AFTER  它比BEFORE更近一步，要求RW事务在其他节点上也要等待应用完毕。这样一来，后续的事务在任何节点上就都能获取最新事务数据。 事实上，要慎用该级别及更高以上级别，可能会引发其他问题，可参考这个文章：技术分享 | 为什么MGR一致性模式不推荐AFTER  5 BEFORE_AND_AFTER  一致性级别要求最高，对RO和RW事务都要求同步事务数据。也就是说，RW事务在应用之前，要先等待前面堆积的事务完成，并且还需要等待它的事务变更在其他所有节点上也都应用；RO事务在执行之前，也要先等待前面堆积的事务完成。  group_replication_member_weight  整型类型，默认值为50，取值范围为：0~100。MySQL 5.</description>
    </item>
    
    <item>
      <title>信创和国产数据库</title>
      <link>/dba/%E4%BF%A1%E5%88%9B%E5%92%8C%E5%9B%BD%E4%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E4%BF%A1%E5%88%9B%E5%92%8C%E5%9B%BD%E4%BA%A7%E6%95%B0%E6%8D%AE%E5%BA%93/</guid>
      <description>什么是“信创”  “信创”的全称是“信息技术应用创新产业”，旨在实现信息技术领域的自主可控，保障国家信息安全。信创产业的主体包括基础硬件、基础软件、应用软件、信息安全、系统集成等部分。 数字化基础设施的安全可控关系到经济发展，直接决定着供应链安全、产业链安全和信息化安全，进而决定国家安全。近年来，国际局势波诡云谲，在频繁的制裁断供事件下，科技自立自主自强变得更加迫在眉睫。“十四五”规划纲要已明确要将科技自立自强作为国家发展的战略支撑，2023年两会上发布的《政府工作报告》强调要建设现代化产业体系，推进科技自立自强。《数字中国建设整体布局规划》则提出要构筑自立自强的数字技术创新体系，筑牢可信可控的数字安全屏障。2022年我国中央及各地政府更是相继发布了上百条信创相关政策，构建具有完全自主知识产权的创新技术体系。 通俗的说是用国产硬件(芯片,存储,) 划重点:自主可控 划范围:基础硬件、基础软件、应用软件、信息安全、系统集成 在实际执行中核心的是：芯片、操作系统、数据库、中间件、整机  信创的主要厂商  芯片CPU：飞腾、鲲鹏、海光、龙芯、兆芯、申威 操作系OS：普华软件、中标麒麟、银河麒麟、统信UOS、红旗、中科方德、中兴新支点 数据库DB：武汉达梦、人大金仓、神州通用、南大通用、万里开源、华为GaussDB、阿里Oceanbase 中间件：东方通、金蝶、宝兰德、华宇软件、普元信息 办公软件：金山软件、福昕软件、万兴科技 安全保密：三六零、奇安信、中孚信息、万里红、格尔软件  信创的市场 行业  信创体系覆盖2+8+N个领域 2:即党、政 8:金融、电力、电信、石油、交通、教育、医疗、航空航天8个关于国计民生的重要行业 N:N个消费市场。N个行业中的办公OA、编辑类的国产软件。  市场规模  预计2023年中国信创产业规模将达20961.9亿元 2027年有望达到37011.3亿元  方向  信创的国有自主可控软硬件替代国外的商业软件一般采用两种方式进行 方向一：上云，通过将服务迁移或合并至阿里云，电信云，华为云，腾讯云&amp;hellip;由云厂商提供信创服务 方向二：自主替换，可以由单位自身的研发团队或第三方国产厂商支持完成软硬件服务的替换  信创的进展  进展非常快，趋势明确 除部分领域（芯片,操作系统）进展不顺利外 存储,数据库，整机，中件间等领域进展得非常顺利 大量的国产硬件,数据库已经完成了对国外商业软件的替换 部分单位和关键行业也完成私有云或公有云的迁移。 进展非常顺利的原因： 1.国家政策要求 2.国外商业软硬件有巨大的利润空间，这部分利润空间可以节省出来 最核心的信创是：芯片，存储，操作系统，数据库，通用软件 下面从我了解的角度来展开聊聊国产数据库的那点事 有哪些是真国产，哪些是假国产 哪些是真的自主研发，哪些是披着皮的洋鬼子  国产数据库 国产数据库的发展  国产数据库在信创政策出来之前 就已经在茁壮成长了 原因一：国外的商业数据库太贵了 一套oracle集群收费每年可能要到几十万块钱 正版的太贵，只能商业谈判走折扣 有的甚至直接用盗版 原因二：国外的开源数据库技术发展 主要是mysql,Pg的发展 给国内的厂商和技术团队提供了方便的二次开发定制的机会 等到信创的政策出来后 巨大的利润空间和强大的需求 引起国产数据库全面开花 这些年国产数据库的创业团队如雨后春笋一样 爆炸增长 其中有传统的老厂商 也有踩着互联网浪潮过来的创业新厂 这些国产数据库厂商不管有多少家 但总是逃不过以下三个大类 1.</description>
    </item>
    
    <item>
      <title>当程序出现了个bug</title>
      <link>/ai/%E5%BD%93%E4%BD%A0%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%87%BA%E7%8E%B0%E4%BA%86%E4%B8%AAbug/</link>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E5%BD%93%E4%BD%A0%E7%9A%84%E7%A8%8B%E5%BA%8F%E5%87%BA%E7%8E%B0%E4%BA%86%E4%B8%AAbug/</guid>
      <description>今天下午 研发有个数据误更新 需要回滚数据 在利用DBA提供的数据找回功能时 刚好我在旁边看着他在处理 眼瞅着生成的加滚SQL文件没有换行 一个大文本没有分行 所以有文字都在一行上 很难阅读 研发得自已处理下这个文件才能看 我让他换个文本编辑器试一下 结果还是不分行 我问另一个经常用这个功能的同事 以下是对话 - “你以前用的时候也是这样吗？” - 答：oracle就是不会分行,MySQL没问题 - “那怎么没跟我提起过呢“ - 答：以为就只能这样，不分行了处理下也能用 - .... -  我说这是一个bug 得处理一下 所以bug第一步  0.发现问题  一个平台或功能得有个反馈的机制 经常有同事或用户 在使用时发现了问题 不一定会主动反馈 需要有一定的鼓励措施 我在做大多数功能的时候 会强调如果使用过程中有任何问题 都可以找我沟通 每次有人找我反馈问题时 都会积极响应 （有时候是用户操作有问题引起的）  1.确认和复现问题  收到反馈后 需要自己确认和复现这个问题 要有再次触发的条件 某些极端的情况下 可能会很难触发和复现问题 此时需要自己阅读代码 找到可疑的模块 再在脑海里重构流程 从而发现问题 我知道这个能力不是所有人都具备的 但个人觉得这是一个好的程序员 应该具备的能力 在此过程中 有一定的概率会发现 这个问题可能是个bug 可能不是个bug 如果不是bug也需要找到问题所在（通常都是流程的条件不具备）  2.定位bug代码  通过现象 找到问题代码 这个定位过程有长有短 有时候甚至需要很久很久 反复的阅读代码 增加日志 才能定位到问题代码 如果是很长时间以前的代码 那简直是个灾难 也因为这个原因 我不喜欢把项目的架构设计得太复杂 因为时间长了 我可能已经记不得当时是怎么设计的了 就很头疼 在我主导的项目里 都力求简单 类设计通常都是按小模块独立 有时候我也需要阅读和调试别人的代码 讲真的 有的时候真的非常费劲 这可能就是不同人的不同表达差异 我也跟正经的研发同学 聊过这类问题 其中有非常善于review别人代码的 经常发现和定位问题的团队管理人员 他们有一种能力 就是架空与代码层去思考bug 以及某种直觉 猜到可能是哪个逻辑没有处理好 这个真的很厉害 需要长时间的一线代码管理 以及不断的定位和处理bug 才能产生的熟练程度  3.</description>
    </item>
    
    <item>
      <title>五岳登山旅行系列之总结篇</title>
      <link>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E6%80%BB%E7%BB%93%E7%AF%87/</link>
      <pubDate>Wed, 08 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E6%80%BB%E7%BB%93%E7%AF%87/</guid>
      <description>导航  1.恒山篇。 2.泰山篇。 3.华山篇,嵩山篇。 4.衡山篇。 5.总结篇。  五岳计划  2023年4月感染了新冠病毒 身体受到了很大的伤害 在长达2个月的恢复期里 宅在家里没有力气外出活动 恢复期里计划此次行程 目标是爬完祖国的5大名山 9月份开始执行 期间遇到华山,嵩山,衡山连续下雨未能成行 从2023年9月3日到11月6日 共用了4个周末时间 完成了五岳爬山计划 旅行路上共用时:150小时50分钟 花费:8663.5元  行程单    目的地 日期 出行方式 用时 花费 评价 推荐     恒山 2023-09-03 自驾 38小时32分 1155.4元 可圈可点 2颗星   泰山 2023-09-08 高铁 13小时10分 1078.3元 无与伦比 4颗星   华山 2023-10-11 高铁 22小时15分 1204.6元 人间仙境 5颗星   嵩山 2023-10-12 高铁 25小时35分 972.</description>
    </item>
    
    <item>
      <title>五岳登山旅行系列之衡山篇</title>
      <link>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E8%A1%A1%E5%B1%B1%E7%AF%87/</link>
      <pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E8%A1%A1%E5%B1%B1%E7%AF%87/</guid>
      <description>目录  1.恒山篇。 2.泰山篇。 3.华山篇,嵩山篇。 4.衡山篇。 5.总结篇。  行程  11-02 19:00 因衡山周末下雨，改道杭州(备选方案:成都,厦门,福州,昆明) 11-02 19:25 预订到11月4日北京到杭州机票(674元) 11-03 14:20 预订11月4日杭州东到湖州高铁票G7650(58元) 11-03 14:30 预订11月5日杭州南到株洲Z247软卧(324元) 11-03 14:32 预订11月5日株洲到衡山k457硬座(14.5元) 11-03 14:35 预订11月6日长沙到北京的返程机票(574元) 11-03 14:40 预订11月4日湖州全季酒店(375元) 11-04 05:40 出租车去首都机场T3,行程正式开始(40元) 11-04 07:30 CA170北京到杭州飞机起飞 11-04 09:20 到达杭州萧山机场T4 11-04 09:35 到达地铁19号线 11-04 10:20 杭州东到湖州高铁票G7650改签至G1672 11-04 10:32 到达杭州东6A检票口，顺利坐上高铁 11-04 10:37 G1672从杭州出发 11-04 10:58 到达湖州站 11-04 11:05 湖州站口租车(430元) 11-04 11:30 完成租车手续，开车前往湖州衣裳街 11-04 12:25 衣裳街鸭头店卤肉消费(36元) 11-04 12:32 衣裳街网红店:丁莲芳吃饭(37.5元) 11-04 12:52 周生记吃了一碗混沌(16元) 11-04 13:04 徐忠良定胜糕消费(8元) 11-04 13:19 盘古烤猪蹄消费(37元) 11-04 13:21 奶茶店消费(26元) 11-04 13:59 离开衣裳街，停车费(10元) 11-04 16:55 月亮湾广场，冰琪淋(20元) 11-04 18:30 湖鲜晚餐(656元) 11-04 20:40 离开马桶盖广场,开车回酒店,停车费(10元) 11-04 21:10 到达酒店,开始洗漱 11-04 22:25 出酒店，夜逛湖州 11-04 22:42 水果超市买水果(102元) 11-05 10:21 出发,开车去南浔古镇 11-05 10:56 下高速,高速费(14元) 11-05 11:12 到达南浔古镇停车场，满了,绕着古镇 11-05 11:30 到达李白酒楼后的停车场,停车开始逛南浔古镇 11-05 12:19 南浔古镇吃:羊排羊肉面(93元) 11-05 12:41 吃完饭预订湖州回杭州东的G7439车票(32元) 11-05 12:58 南浔文创店买扇子(25元) 11-05 13:31 万福大糕买了一块藕糕(26.</description>
    </item>
    
    <item>
      <title>数据库工具选型</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B7%A5%E5%85%B7%E9%80%89%E5%9E%8B/</link>
      <pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B7%A5%E5%85%B7%E9%80%89%E5%9E%8B/</guid>
      <description> 一个采访
 我的回答 1、对于您而言，选择使用数据库工具的原因是什么？为了解决哪些需求和痛点？  提高人效,减少DBA重复执行某些固定操作的时间成本 避免人为误操作,避免误操作等潜在风险 保证操作一致性,确保任务和流程按一致的方式执行,避免不同的DBA有不同的检查和执行动作造成的潜在问题  2、在进行数据库工具选型时，可以采取哪些方法和策略（可以从需求分析、候选工具筛选、实验和评估、考虑业务场景、参考案例、最终选型、持续优化和调整等方面展开）？  第一步:确认需求,明确我们需要工具来解决什么问题,基本需求是什么,最好可以实现什么样的效果 第二步:是否可以从现有的平台工具上扩展或衍生出来,如果可以改造和研发的成本是多少 第三步:同行交流及内部沟通,看看其他公司是如何解决此类问题,是否有类似的解决方案 第四步:从收集到的数据库工具中,筛选:功能是否满足需求,是否开源，开发语言是什么,是否适合二次开发,稳定性如何, 第五步:实验，在筛选的数据库工具中，在测试环境进行试用,如果需要改造的,投入小部分资源验证改造可行性 第六步:集合到现有的DBA平台中,让所有的工具在一个平台下使用  3、在选型过程中，需要综合考虑数据库工具的哪些关键因素？  功能性:能帮我们解决什么样的问题 可靠性:是否会带来其他的问题，尤其是涉及安全问题 行业认可度,尽可能的避免小众的解决方案和工具 可扩展性:是否开源及开发语言是什么,会优先选择团队习惯的开发语言的开源工具  4、数据库工具既有免费的，也有付费的，亦有国外和国内的软件之分，您更趋于选择哪一类？为什么？  趋向于选择开源免费的,付费的数据库工具很少会使用,一方面数据库软件的收费通常都比较贵,使用成本不低,另一方面我们选择数据库工具时更多的还是需要集成到现有的dba平台中,开源的改造和调试难度更低，一些自定义的需求和功能，需要二次开发，这时候开源的优势就非常明显 国内和国外的软件没有倾向性,如果是复杂的工具，偏向国内的一点,中文文档还是阅读性上还是比翻译过来的文档更容易理解  5、您使用过哪些数据库工具，它们的适用范围，优缺点分别是什么？在使用过程中，可能会遇到哪些问题？您有什么运维建议吗？  inception和goinception:数据库审核工具,适用于SQL审核上线,几乎完美的解决了SQL上线审核的难点和痛点问题，非常方便集成到现有系统中 gh-ost:数据库上线工具，一般联合上面的inception工具一起使用完成SQL上线，需要注意的是在一些特定情况下有数据丢失的风险 datax和cannal:优秀的数据同步工具，可以应付大多数的数据同步任务。 redis-shake和mongo-shake:在redis和mongo迁移时用到的工具,非常方便  6、在您看来，优秀的数据库工具应该具备哪些功能？请您推荐几款上手快、安装简单、方便易用、支持多种数据库的数据库管理工具？  优秀的数据库工具功能：稳定可靠,解决问题,方便集成和二次开发 推荐goinception,gh-ost以及集成了众多数据库工具的Archery  7、作为用户，您认为目前主流的数据库工具，还存在哪些需要改进的地方？  已经做得很好了，可能在数据防火墙等数据库安全领域还缺少一些通用的解决方法和工具  AI的回答 1、对于您而言，选择使用数据库工具的原因是什么？为了解决哪些需求和痛点？  提高数据管理效率：通过使用数据库工具，可以实现对数据的快速存储、查询和分析，从而提高工作效率。 支持复杂查询和分析：数据库工具通常具有强大的查询和分析功能，可以满足各种复杂的业务需求。 数据安全性和可靠性：数据库工具可以提供数据备份、恢复和高可用性等功能，确保数据的安全性和可靠性。 成本效益：选择合适的数据库工具可以降低硬件、软件和维护成本，提高投资回报率。  2、在进行数据库工具选型时，可以采取哪些方法和策略（可以从需求分析、候选工具筛选、实验和评估、考虑业务场景、参考案例、最终选型、持续优化和调整等方面展开）？  需求分析：明确业务需求和应用场景，了解所需的性能、功能和安全性要求。 候选工具筛选：收集耿耿于意的数据库工具信息，如开源与商业产品、国内外厂商等，并进行初步筛选。 实验和评估：针对初选的工具进行实际测试，包括性能测试、功能测试和安全测试等，以评估其是否符合需求。 考虑业务场景：根据实际业务场景和需求，选择适合的工具，例如分布式数据库适用于高并发场景，关系型数据库适用于事务馊处理等。 参考案例：查阅相关案例和评价，了解其他用户的使用经验和效果，为选型提供参考。 最终选型：综合以上因素，确定最适合的库工具。 持续优化和调整：在实际使用过程中，根据实际情况对工具进行优化和调整，以提高性能和满足新需求。  3、在选型过程中，需要综合考虑数据库工具的哪些关键因素？  剰性能：根据业务需求，选择具有足够性能的数据库工具。 功能：选择具备所需功能的工具，如数据存储、查询、分析和备份等。 安全性：选择具有强大安全特性的工具，如数据加密、访问控制和审计等。 可扩展性：选择可以随着业务增长而轻松扩展的工具。 成本：考虑工具的购买、维护和支持成本，以及可能带来的经济效益。 兼容性：选择能与现有系统和应用兼容的工具。  4、数据库工具既有免费的，也有付费的，亦有国外和国内的软件之分，您更趋于选择哪一类？为什么？  免费或开源的产品：这些产品通常具有较高的性价比，可以降低初始投入成本。 成熟稳定的产品：选择经过市场验证且具有良好口碑的产品，以确保稳定的运行和使用体验。 国内产品：考虑到文化背景和技术支持等因素，选择国内厂商的产品可能会更加合适。但具体情况还需根据项目需求和个人经验来判断。  5、您使用过哪些数据库工具，它们的适用范围，优缺点分别是什么？在使用过程中，可能会遇到哪些问题？您有什么运维建议吗？    Microsoft SQL Server：适用于大型企业级应用，如金融、零售等行业。优点是功能强大，性能稳定，易于管理和维护。缺点是价格较高，对于小型企业来说可能过于昂贵。在使用过程中，可能会遇到性能瓶颈、数据安全问题等。运维建议是定期进行性能调优、备份和恢复演练，以确保数据的可靠性。    MySQL：适用于中小型企业级应用，如博客、论坛等。优点是开源免费，性能良好，易于学习和使用。缺点是功能相对较弱，对于大型复杂应用可能不够灵活。在使用过程中，可能会遇到性能瓶颈、数据安全问题等。运维建议是定期进行性能调优、备份和恢复演练，以确保数据的可靠性。    Oracle Database：适用于大型企业级应用，如金融、电信等行业。优点是功能强大，性能稳定，适用于高并发和高可用性场景。缺点是价格较高，对于小型企业来说可能过于昂贵。在使用过程中，可能会遇到性能瓶颈、数据安全问题等。运维建议是定期进行性能调优、备份和恢复演练，以确保数据的可靠性。    PostgreSQL：适用于中小型企业级应用，如数据分析、物联网等。优点是功能强大，性能稳定，支持复杂的查询语句。缺点是非商业化，对于大型企业来说可能不够可靠。在使用过程中，可能会遇到性能瓶颈、数据安全问题等。运维建议是定期进行性能调优、备份和恢复演练，以确保数据的可靠性。    6、在您看来，优秀的数据库工具应该具备哪些功能？请您推荐几款上手快、安装简单、方便易用、支持多种数据库的数据库管理工具？  优秀的数据库工具应具备以下功能：   强大的数据管理功能，包括表结构设计、数据导入导出、数据备份与恢复等。    高效的数据查询和分析功能，支持复杂的查询语句和统计分析。    良好的扩展性和性能，能够应对大规模数据和高并发访问。    丰富的安全特性，确保数据的安全性和完整性。    友好的用户界面和便捷的操作方式，降低学习成本和使用难度。    7、作为用户，您认为目前主流的数据库工具，还存在哪些需要改进的地方？  推荐的几款上手快、安装简单、方便易用、支持多种数据库的数据库管理工具 understatement, pgAdmin, Sequel Pro 和 DBeaver。这些工具都具有良好的用户体验和功能，支持多种数据库，可以根据您的需求选择合适的工具。  总结  AI语言组织上更流畅，在讲功能作用画大饼时候比真人组织得有条理 但真实的工具，核心要解决的问题，离我还早。  </description>
    </item>
    
    <item>
      <title>《菊次郎的夏天》观后感</title>
      <link>/book/%E8%8F%8A%E6%AC%A1%E9%83%8E%E7%9A%84%E5%A4%8F%E5%A4%A9/</link>
      <pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%8F%8A%E6%AC%A1%E9%83%8E%E7%9A%84%E5%A4%8F%E5%A4%A9/</guid>
      <description> 《菊次郎的夏天》 公路片,喜剧片,温情片 说不清是什么类型的电影 大多数人带入的视角 应该都是正男 当然后来才知道主角有可能是菊次郎 一开始不喜欢这脾气炸裂的中年人 无理咋呼 这一点就很不喜欢 但是剧情推进又觉得 他们的认知水平里 觉得这样是对的 尝试代入他的世界看问题时 觉得生活还真的就这样 人和人之间莫名的和谐 菊次郎和他母亲的那段 那种难以表达的复杂情感 被电影镜头用一种很自然的方式展现出来 那种看着上一辈子的父母 似乎能理解他们的想法 又不能融入 不能参与太多的 莫名的无助和伤感 而可怕的是 自己也不再年轻 不再像正男一样 有无数的可能 人生就像一次旅程 沿途会遇到各种风景 但最后我们都要回归 而菊次郎逗逼的性格 跟现实中的我 其实非常像 我至今没有遇到过 像我一样不正经的同龄人 这样性格的人 有人喜欢有人厌 逗逼的性格觉得是我的优势 至少不会得抑郁症 而我需要做的是控制情绪稳定 有个说法是每个擅长喜剧的导演 都是暴燥的控制狂 不想当导演 只是个观众 看到一部好电影 自然的融入情节中 仿佛置身于电影里的情节 和角色们一起喜怒哀乐 喜欢菊次郎的夏天这个电影 感受到真实感 那种细微的 不好表达的人类情感 被电影表达出来了 就很神奇 然后再看片名 夏天 我对季节轮换很敏感 因为我对时间有变态的强迫症 会在不同的季节 安排不同的事 曾经每年的夏天 都是阳光灿烂的记忆 就像今年的秋天一样 而现在 秋天已经过去了 明天就开始入冬了 时间太瘦 指缝太宽 再回首 记忆越发模糊了 今年冬天的第一场雪落下的时候 希望脑海里不会再有那个声音 你看 终究我们会失去所有 菊次郎的夏天 和我的秋天 一样精彩 愿这个冬天都健康温暖 一切安好  </description>
    </item>
    
    <item>
      <title>爱狗人和恨狗人</title>
      <link>/book/%E7%88%B1%E7%8B%97%E4%BA%BA%E5%92%8C%E6%81%A8%E7%8B%97%E4%BA%BA/</link>
      <pubDate>Sun, 22 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E7%88%B1%E7%8B%97%E4%BA%BA%E5%92%8C%E6%81%A8%E7%8B%97%E4%BA%BA/</guid>
      <description> 2023年10月16日8时 成都一只没拴绳的罗威纳犬撕咬小孩 看了完整的视频 很难理解作为人类不会因同类的伤害感到愤慨 对这种爱狗养狗人士无知给社会带来的危害而选择视而不见 后来的连锁反应是 一些地方的民警和城管开始抓流浪狗 再然后民间的爱狗人士又开始出来说流浪狗有多可怜 再有杨迪和一些明星出来说流浪狗可怜 不管是张馨予的长微博还是杨迪的短视频 看起来都挺有理的 他们在自己的角度说他们这样的养狗人是怎么怎么和狗和谐共处 以及他们和狗狗们的心心有印 明星有条件可以做到“文明”养狗 普通人呢 普通人有条件对狗做各种训练吗 普通人有独栋别墅来不打扰邻居吗 我非常反感养狗人士 这跟爱心无关 主要是“不打扰别人”  遛狗不牵绳  傍晚的时候在小区里公园里 至少有一半的遛狗的都不牵绳子 “我的狗狗不咬人，牵绳子干吗” 问就是说他的宝贝不咬人 这种神仙逻辑在于 1.没咬人之前你当然以为你家狗不咬人 2.不咬人它还是会打扰到别人 比如扑过去闻的，吓唬别人的&amp;hellip; 以及有些人天生对狗就不喜欢 比如说我 我从来不觉得这种带毛的动物有啥可爱之处的 狗在我旁边经过 不管是大只的还是一脚就能踩死的小狗 我都不喜欢 都会感到紧张 当一只狗热情的过来凑热闹 我是真实的觉得被恶心和冒犯到的 每个人有权利保持自己的爱心和爱好 但是都应该想到自己的爱好 不能建立在对其他人的正常生活或权利产生侵害上面 你也不能把你家狗不咬人这种设定 强行要求别人去接受和理解 更不能选择性的觉得别人也喜欢你家狗去接近 当侵犯别人私人空间的事情 发生了或者潜在发生了 请意识到让别人为你的爱好买单是种错误的行为  侵犯别人的空间  在公共场合的言行 跟在家或私人关系不同 保持互相尊重 不侵犯陌生人的空间 应该成为常识 我会经常思考和反思 自己的行为是否打扰到别人 这些年我也会经常看到有人开始讨论 在公共场所吸烟 在安静封闭的公众场合大声喧哗 广场舞 插队 甚至随地大小便 &amp;hellip; 我很欣慰越来越多的人开始意识到 公共场合的“文明” 就是保持人和人之间的距离 不同与熟人,爱人,亲人 在公共场合 人和人之间是要保持距离的 所以爱狗人士要明白 一旦你的狗离开了家门 它的行为就可能对邻居 对一起坐电梯的人 对路上的行人 对小区里公园里正常遛弯的行人 造成了“麻烦” 这个“麻烦”不是你自以为是的小事 极端情况下 可能像成都那个恶狗一样 对一个无辜的一家人造成不可挽回的损失 就这还有脸说狗不咬人不拴绳的  控制力  大多数养狗的人 会对自己狗如此有信心 是因为一部分场景下 验证了狗的听话 对自己的控制力过分自信了 国内现状下养狗人 可能只有20%的狗 是办过狗证的 大多数的狗可能都没有经过训练和筛选 都是凭着狗主人的主观感受 来断定狗是否听话和忠诚 然后流行的是 一些胳膊细得跟胡萝卜一样 抓一只鸡都抓不牢的女人 喜欢养一些大型犬禁养犬 这些狗根本就不是她们能拉得住的 牵根绳子也就是装个样子 经常会被狗牵着走 &amp;hellip; 这种身处危险而不自知的愚蠢 让人无奈同时 而这种愚蠢还给周围的其他人带来了危害 所以一有这种恶犬伤人事件出来 各平台的异烟肼的搜索量就上来 这世界上有人喜欢刺激和冒险 也有很多普通人 不希望把自己放在危险下 是件很正常的事  关于陪伴  这世上还是有很多喜欢狗 或者说喜欢宠物的人 尊重他们 好像只有养狗这个群体 会妨碍到其他人 所以我对养其他宠物的人 都很理解和支持 人嘛 有点自己的喜好 本身就是件好事 (前题是别影响其他人) 有情感有喜好 对自己喜欢的人或物投入感情 所以才是有血有肉的人 我也喜欢动物 比如动物园里的动物 cctv9，10里的动物 比如朋友家养的猫 我妈以前也养过好久的猫 我自己养过乌龟和鱼 养鱼是很多很多年前的事 那会我刚从学校走入社会 和当时的女朋友分手的时候 她送过给我一盆鱼 努力的喂鱼,定时换水 养了很长的时间 不知道为啥 那会我还是比较闲的 换作现在不一定有这么耐心了 再然后大约是16年前后 试着养乌龟 因为听说这玩意特别简单 买了个专用的水景鱼缸 部了景 去花鸟市场买了最皮实的巴西龟 还有乌龟食物 结局一样 一只接一只的挂了 后来就放弃了养动物的心了 我养了两次宠物 但都不是真正的宠物 因为它们其实不会陪伴 不会互动 人和宠物的关系 在不同的影视作品和文字中都有感受过 动物或者说宠物对人的陪伴 心理上的作用 我也能感受到 但我却没有这样的经历或投入 我更喜欢人 觉得动物再灵性 也没有人可爱和机灵 真的 宠物不如宠人  </description>
    </item>
    
    <item>
      <title>一段电视讲话看巴以冲突的美国人态度</title>
      <link>/book/%E4%B8%80%E6%AE%B5%E7%94%B5%E8%A7%86%E8%AE%B2%E8%AF%9D%E7%9C%8B%E5%B7%B4%E4%BB%A5%E5%86%B2%E7%AA%81%E7%9A%84%E7%BE%8E%E5%9B%BD%E4%BA%BA%E6%80%81%E5%BA%A6/</link>
      <pubDate>Fri, 20 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E4%B8%80%E6%AE%B5%E7%94%B5%E8%A7%86%E8%AE%B2%E8%AF%9D%E7%9C%8B%E5%B7%B4%E4%BB%A5%E5%86%B2%E7%AA%81%E7%9A%84%E7%BE%8E%E5%9B%BD%E4%BA%BA%E6%80%81%E5%BA%A6/</guid>
      <description>10月19晚上美国总统拜登跟以色列总理见面后发表电视讲话 美国和以色列明显的在中东在欺负人 加沙那个地方都被圈成那样了 加沙那个地方都被圈成那样了 但是在美国政客眼里 依旧能理直气壮的说出来 “以色列被欺负了 美国要坚定的帮助以色列” 如果是大毛和二毛打架 帮乌克兰还能说得通 但是以色列都把巴勒斯坦的加沙欺负成那样了 眼瞅着加沙就要被推平了 这也能站在真理的一方 说得头头是道 哪有什么公平正义 对政客来说 屁股早就歪了 我的英文水平还是有点费劲 得借助翻译软件才能看懂 还是羡慕那些英语好的人 想学却学不会的技能 只能羡慕了 术业有专攻 技能也有方向 当我在羡慕别人的语言能力时 说不定也有人会羡慕我的代码表达能力 各自加油.  以下是全文
Good evening, my fellow Americans.
We’re facing an inflection point in history. One of those moments where the decisions we make today are going to determine the future for decades to come. That’s what I’d like to talk with you about tonight.</description>
    </item>
    
    <item>
      <title>测试环境治理数据库相关部分</title>
      <link>/dba/%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%B2%BB%E7%90%86/</link>
      <pubDate>Thu, 19 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%B2%BB%E7%90%86/</guid>
      <description> 下午邀请了去哪儿网的测试同学来公司分享 介绍了去哪儿网的测试环境3.0版本 1.0是固定环境(实体) 2.0是noah平台(测试云平台) 3.0是软路由平台 这里面的代际变更和功能点这里不多做介绍 我想总结和对比的是测试环境治理时的数据库部分  测试环境的数据库的不同阶段需求  1.业务量小的时候，一套测试环境就可以了(&amp;lt;10人研发:test) 2.继续发展,需要把测试环境和开发环境分离出来(10-30人研发:test,dev) 3.业务继续壮大,研发人员开始出现金个分组,需要多套测试环境(30-100人研发团队:test1,test2,dev) 4.业务继续壮大,研发人员开始出现金个分组,需要多套测试环境(100-500人研发团队:test1,test2,test3&amp;hellip;,dev,preprod) 5.业务继续壮大,研发人员分事业部，需要云环境(500+研发团队:测试云环境)  多套测试环境的数据结构同步  这里要分两种情况来区别处理 如果是测试云环境，需要有个基准环境出来，过程中有大量的自动化运维参与 如果是独立部署的测试环境,需要手动同步  测试云环境  以生产库为最终标准 定义一个基准环境，基准环境的数据库结构会实时跟线上同步 生产环境的上线将会自动应用到基准环境 其他环境的数据结构会和基准环境对齐 在一个固定的时间点，自动或手动的强制和基准环境对齐 允许非基准环境下的数据库结构和基准环境不一致 非基准环境造的数据和变更会随着环境一起销毁 非基准环境可以随时销毁，重新从基准环境拉一套出来 基准环境的数据维护是件很重要的事  独立部署的测试环境  在非云状态的固定的独立部署的一套或多套测试环境 基准环境就是生产环境 数据库表结构会和生产环境对齐 每个固定的时间点所有测试环境都会和生产环境对比表结构和其他对象 发送出来异常报告 是否修复需要测试或研发人员自行决定是否对齐 SQL上线时可以选择多环境上线，一段脚本在多个环境同时执行 参见：数据库多环境SQL上线。  造数据问题  测试环境的数据谁来造？ 字典类的基础数据从线上同步（允许定时同步） 客户,订单类数据尽可能的由测试人员随机生成 部分数据允许按一定规则从线上同步回来（需要脱敏） 测试环境的数据也会有完整的防误删和备份策略  </description>
    </item>
    
    <item>
      <title>二进制和太极,两仪,四象,八卦,五行</title>
      <link>/book/%E5%A4%AA%E6%9E%81%E7%94%9F%E4%B8%A4%E4%BB%AA%E4%B8%A4%E4%BB%AA%E7%94%9F%E5%9B%9B%E8%B1%A1%E4%B9%8B%E5%85%AB%E5%8D%A6%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6/</link>
      <pubDate>Wed, 18 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%A4%AA%E6%9E%81%E7%94%9F%E4%B8%A4%E4%BB%AA%E4%B8%A4%E4%BB%AA%E7%94%9F%E5%9B%9B%E8%B1%A1%E4%B9%8B%E5%85%AB%E5%8D%A6%E5%92%8C%E4%BA%8C%E8%BF%9B%E5%88%B6/</guid>
      <description>从莱布尼茨说起  莱布尼茨（Leibniz，1646年7月1日－1716年11月14日） 德国哲学家,数学家,律师 他在1679年发明了二进制 他和牛顿先后独立发现了微积分，大学里的微积分的那些奇怪的数学符号大部分是这个人画的  1701年法国汉学大师若阿基姆·布韦向莱布尼茨介绍了《周易》和八卦的系统。 莱布尼茨表示：“阴”与“阳”基本上就是他的二进制的中国版。   二进制  二进制是计算机世界的主宰 所有计算机(电脑/网络/手机&amp;hellip;)相关的设备里的存储和通信都是用二进制的0和1来表示 无论是电脑里的几百G的游戏或手机里的高清视频 还是程序员写的一段代码 或是我此刻输入的这段文字 都会被转换成0,1这两个简单的数字来存储，来传输  二进制与八卦  无论是三百多年前的莱布尼茨，还是现在的程序员 都发现八卦和二进制的奇怪契合 八卦可以很容易直观的用二进制表示出来 都是0-阴,1-阳来表示 都可以通过增加位数来表达更充实的数据 还都可以用二进制进行运算  两仪生四象  两仪生四象 两仪是指(阴:0，阳:1) 四象是指(太阴,少阳,少阴,太阳) 用阴(0)生成出来的:00太阴,01少阳 用阳(1)生出来的:10少阴,11太阳  四象也可以用季节表示(春:01,夏:11,秋:10,冬:00) 这里面冬天(00)阴气到达极点，阳气开始升发，阴气减少。 春的二进制表示是:01 夏阳气到达极点:11 秋阳气开始退去:10 冬又变回了:00 于是我们有了：两仪生四象 及四象的运转规律   四象生八卦  当用两根线表示的时候，阴阳生成了四象 那用3根线表示的时候，就生成了八卦  坤：黑黑黑，卦符阴阴阴，二进制数为000 艮：黑黑白，卦符阴阴阳，二进制数为001 坎：黑白黑，卦符阴阳阴，二进制数为010 巽：黑黑白，卦符阴阳阳，二进制数为011 震：白黑黑，卦符阳阴阴，二进制数为100 离：白黑白，卦符阳阴阳，二进制数为101 兑：白白黑，卦符阳阳阴，二进制数为110 乾：白白白，卦符阳阳阳，二进制数为111    而四象生八卦的方式是：         六十四卦  继续沿生下去：八八重卦，得六十四卦。 也就是说用三枚铜钱的正反面来算8卦的时候 需要6枚铜钱来算64卦  只是这六十四卦很多生僻字 我也难得去细细看每一卦是啥意思了 看这个方圆图就觉得挺事事的   五行  古人认为世间的万物都是由金、木、水、火、土五种元素 因为我的名字跟五行有关 所以金木水火土这五个字比较在意 八卦和五行是互相转换的 八卦的字太难念了，很多字我都不认识，更别说古人了。 于是我们可以换成通俗一点的表示 乾代表天 坤代表地 巽代表风 震代表雷 坎代表水 离代表火 艮代表山 兑代表泽 八卦对应的五行分别是： 金:天,泽； 木:雷,风； 土:地,山； 水:水； 火:火 除掉天,泽对应金不好理解外，其他基本上都挺好理解的 关于金:天,泽，可以用京东来辅助记忆下  五行及其应用  说到五行 这时候神神叨叨的中医又出场了 做为一个中医黑 肯定是不信中医这套奇怪理论的 但是不妨看看他们的五行之说  以及五行的其他表达形式     五行 木 火 土 金 水     五气 柔 息 充 成 坚   季节 春 夏 长夏 秋 冬   方位 东 南 中央 西 北   气候 风 热 湿 燥 寒   五天 苍天 丹天 黄天 素天 玄天   五气色 青 红 黄 白 黑   五宫 东宫 南宫 中宫 西宫 北宫   五味 醯 酒 饴蜜 姜 盐   五形 直 尖 方 薄 圆   方位 左 上 中 右 下   五菜 韭 薤 葵 葱 藿   五果 李 杏 枣 桃 栗   五化 生 长 化 收 藏   五谷 麻麦 黍 稷 穀稻 豆   五畜 鷄 羊 牛 马 彘   五律 角 征 宫 商 羽   五虫 毛虫 羽虫 倮虫 介虫 麟虫   五方 东 南 中 西 北   五岳 泰山 衡山 嵩山 华山 恒山   五地 山林 丘陵 原湿 坟衍 川泽   五彩 青 赤 黄 白 黑   五帝 太昊 炎帝 黄帝 少昊 颛顼   五神 句芒 祝融 后土 蓐收 玄冥   五器 规 衡 绳 矩 权   五正 后稷 司马 司徒 司寇 司空   五官 春官 夏官 中官 秋官 冬官   五教 父义 母慈 子笑 兄友 弟恭   五常 仁 礼 信 义 智   五事 视 言 思 听 貌   五德 明 从 睿 聪 恭   五瑞玉 青圭 赤璋 黄琮 白琥 玄璜   五脏 肝 心 脾 肺 肾   五官 目 舌 口 鼻 耳   五体 筋膜 血脉 肌肉 皮毛 骨髓   五声 呼 笑 歌 哭 呻   五情 怒 喜 忧思 悲 惊恐   病位 头项 胸脇 脊 肩背 腰股   五华 爪 面 唇 皮毛 发   五音 嘘 呵 呼 哭 吹   五劳 步 视 坐 卧 立   五嗅 臊 焦 香 腥 腐   五神 魂 神 意 魄 志   五津 泣 汗 湿 燥 寒   五味 酸 苦 甜 辛 咸   五欲 色 味 安佚 臭 声   五恶 肝 心 脾 肺 肾   志伤 怒 喜 思 悲 恐   志胜 悲 恐 怒 喜 思   五象 少阴 老阳 中宫 少阳 老阴   五星 木星 火星 土星 金星 水星   五灵 青龙 朱雀 麒麟 白虎 玄武   五季 春 夏 长夏 秋 冬   六气 风 热、火 湿 燥 寒   五促 生 长 化 收 藏   五时 日旦 日中 日西 日入 午夜   八卦 震巽 离 坤艮 乾兑 坎   天干 甲乙 丙丁 戊己 庚辛 壬癸   地支 寅卯 巳午 辰戌 申酉 子亥   五色 青 赤 黄 白 黑   五臭 臊 焦 香 腥 腐   五味 酸 苦 甘 辛 咸   五畜 鷄 羊 牛 鷄 豚   五谷 麦 黍 稷 稻 豆   五体 筋 血(脉) 肉 皮 骨   五藏 血 脉 营 气 精   五情 怒、惊 喜 思 悲、忧 恐   五液 泣 汗 涎 涕 唾   五官 眼(目) 舌 唇(口) 鼻 耳   五音 角 征 宫 商 羽   五智 魂 神 意 魄 志   五腑 胆 小肠、三焦 胃 大肠 膀胱   五指 食指 中指 大拇指 无名指 小指   五性 少刚(雅) 阳刚(急) 申和(直) 少柔(刚) 阴柔(隐)    </description>
    </item>
    
    <item>
      <title>2023年五岳登山旅行系列之华山嵩山篇</title>
      <link>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%8E%E5%B1%B1%E5%B5%A9%E5%B1%B1%E7%AF%87/</link>
      <pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E5%8D%8E%E5%B1%B1%E5%B5%A9%E5%B1%B1%E7%AF%87/</guid>
      <description>目录  1.恒山篇。 2.泰山篇。 3.华山篇,嵩山篇。 4.衡山篇。 5.总结篇。  行程  10-11 22:10 查看华山和嵩山周末的天气情况，决定出行 10-11 22:20 购买10月13日k179次列车北京到郑州软卧(263元) 10-12 20:45 购买10月14日郑州到华山北G2001高铁(174.5元) 10-13 18:27 预订华山门票_北上西下路线(400元) 10-13 19:45 地铁望京东站出发,行程开始 10-13 21:08 地铁北京西站到达(6元) 10-13 21:30 候车厅买了一瓶水(7元) 10-13 22:04 k179次列车正点出发 10-14 06:20 k179次列车到达郑州，提前10分钟到达 10-14 06:35 郑州站4号候车厅的洗手池边刷牙洗脸 10-14 06:44	郑州站候车厅德克士吃早饭(51元) 10-14 07:48 G2001次列车正点出发 10-14 09:34 G2001次列车华山北站到达 10-14 09:41 出站口买了一双登山手套(5元) 10-14 09:46 华山北游客服务中心摆渡大巴(3元) 10-14 10:07 到达华山游客中心坐北峰转运中巴车 10-14 10:35 到达北峰索道入口 10-14 10:48 出北峰索道开始登北峰 10-14 11:00 到达北峰山顶,海拔1614米 10-14 11:08 北峰顶的华山论剑合影,免排队(50元) 10-14 11:10 开始正式爬山,从北峰到中峰一路往上爬 10-14 11:54 途中补水，一瓶水一块卷饼(25元) 10-14 12:27 到达中峰峰顶,海拔2037.</description>
    </item>
    
    <item>
      <title>透明数据加密测试</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%AF%86%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Sun, 08 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%8A%A0%E5%AF%86%E6%B5%8B%E8%AF%95/</guid>
      <description>加密环境  MySQL主从实例&amp;ndash;&amp;gt; 主:127.0.2.30:3401,从:127.0.2.30:3402 MySQL空白实例：127.0.2.27:3405 启用127.0.2.30:3401 主实例的闪电加密  加密工具  **透明数据库透明加密  加密规则  ogg_pump 表：pumpid,tablename 两列做列加密 wc_host_arch 表：cpu,network_in 两列做列加密  加密前后对比  访问数据库得到的数据如下   mysqlw -h 127.0.2.30 -P 3401 dbsec -e &amp;quot;select pumpid,tablename from ogg_pump order by pumpid limit 10&amp;quot; +--------+-------------------+ | pumpid | tablename | +--------+-------------------+ | 1 | SYS_DICTIONARY | | 2 | SYS_USR | | 3 | OWN_HI_TASK_FINAL | | 4 | OWN_HI_PROCESS | | 5 | RC_CST_CAR_INFO | | 6 | RC_CST_BSC_INFO | | 7 | RC_XJNP_APPLY | | 8 | HSJ_BASIC | | 9 | HSJ_ENTINV_ITEM | | 10 | HSJ_FR_POSITION | +--------+-------------------+ mysqlw -h 127.</description>
    </item>
    
    <item>
      <title>DBA操作规范-2023</title>
      <link>/dba/dba%E6%93%8D%E4%BD%9C%E8%A7%84%E8%8C%83-2023/</link>
      <pubDate>Sat, 07 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/dba%E6%93%8D%E4%BD%9C%E8%A7%84%E8%8C%83-2023/</guid>
      <description>基本规范意识  所有操作应该遵循下述原则: 故障优先:当发生线上故障时,需立即暂停原有的工作安排,部分流程规范要求可以适度简化,优先故障恢复 事前通知:影响业务的操作，必须事先和DBA团队负责人确认影响范围。在钉钉里发《运维变更通知》 业务低峰:有潜在业务影响的操作，需和业务方沟通业务低峰执行 备份先行:所有操作，必须事前做好备份,给快速回滚创造条件。包括但不限于：配置文件变更，数据变更,文件删除  服务器操作  连接数据库服务器统一走jumpserver平台 在生产数据库服务器上安装任何软件需经过DBA负责人确认 拷贝大于50G的文件，注意和网络部门确认 生产环境的数据库服务器删除大于500G的文件，需留意IO影响  新建数据库选型  优先MySQL数据库，原则上Oracle,SQLServer,PG,不再支持新建实例和数据库 新建数据库实例需要明确版本,用一键安装脚本完成安装，同时做好实例的备份计划 新数据库选择端口，应考虑：同业务优先，测试和生产同端口  数据库操作  参见《MySQL开发和操作规范》,《Oracle_SQL标准规范》  流程与工单  流程优先:已经做完自动化流程的日常工作，必须走流程，禁止绕开自动化流程的手动操作 工单补充:一些没有自动化流程的工作或特殊操作,须提交DBA工单，不认可：口头沟通,微信,钉钉 与其他部门的协作与沟通也应遵守相应的流程规范要求，例运维部门要求的服务器和网络变更流程规范  权限操作  root,dba账号作为特殊账号不可以配置在业务中，不可以提供给非DBA人员使用或测试 业务新申请账号原则上统一走DBA流程，禁止手动创建账号和密码 用于运维的用户账号，例如：备份账号backupuser,主从同步账号repluser可以手动创建 任何新建账号注意弱密码问题：要求12个字符串的随机字符串 禁止在生产测试申请开通个人帐号，业务和研发查询数据库只能通过DBA平台的页面 原则上不提供给研发人员明文数据库账号和密码，只提供加密后的密码串，统一配置nacos  敏感数据保护  敏感数据包括但不限于：真实姓名、手机号码、邮箱地址、家庭住址、身份证号、银行卡号、交易记录、账号密码等； 严禁私自查询与工作无关的用户或业务数据，包括但不限于：客户信息,员工信息,人力信息,oa信息,财务信息； 禁止在内部群发邮件、群聊、Github、技术分享时泄露敏感数据，比如密码等； 从生产环境同步数据到测试环境，需要和DBA负责人确认：数据量级，是否脱敏 原则上不支持帮业务和研发人员手动导数据的需求，此类需求走DBA平台的在线查询，离线查询 以及大数据库部门的导出  值班制度  值班参考《dba值班表》的排班，每天的值班时间是9:00~次日8:59 当天值班在非工作时间，要求外出带上可随时联网的笔记本。发生故障时15分钟以内可以联网处理问题 当天值班DBA优先处理流程与工单,非工作时间可以用钉钉的DBA模块完成快速审批  </description>
    </item>
    
    <item>
      <title>旅行的意义是什么</title>
      <link>/book/%E6%97%85%E8%A1%8C%E7%9A%84%E6%84%8F%E4%B9%89%E6%98%AF%E4%BB%80%E4%B9%88/</link>
      <pubDate>Thu, 05 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%97%85%E8%A1%8C%E7%9A%84%E6%84%8F%E4%B9%89%E6%98%AF%E4%BB%80%E4%B9%88/</guid>
      <description> 旅行的意义是什么 昨天晚上回京的路上 想到国庆这8天里 旅行的人们 在他们眼中 旅行的意义是什么呢  “读万卷书，不如行万里路”  这是古人留下来的一对旅行的思考 那时候信息闭塞 能传播的媒价有限 不像现在高清图片，视频，多媒体满天飞 甚至有vr这种变态的拟真环境 已经可以在家就能多角度的来体会：自然风光，风土人情  品异地美食  同样的道理 现在有哪种好吃的或特色菜 在北京吃不到的呢 可能有 但应该非常非常小众 小众到这种美食很难满足大多数的口味要求 再加上有些人对美食兴趣不大（比如我） 到哪都喜欢吃点简单的对付完事  放松心情  放松心情的方式太多了 多到我如果想放松的话 有几百种方式可以更简单高效的完成放松 远比出门旅行来得方便直接   那么对我来说，旅行的意义是什么呢？
 放大世界  如果一个人每天都是工作生活两点一线 时间久了 他的世输送就会被限制在那 工作和生活中的一点小事 可能会让他觉得很大 旅行 哪怕只是到陌生的公园里走走 去郊区的小路上溜达一下 让生活这趟列车走个支线 遇到不同的人和事 暂时从重复的工作生活中剥离开来 打开心扉，转换心情 旅行是对自己的一种奖励和调节  眼见为实  有些风景 多好的记录片 也代替不了眼见为实 那干净到成了色带的白沙滩 那连绵几十公里的高山草原 那一览无遗的红灯区 有些风景和风俗 不管别人的描述有多生动 亲眼见到的体会到的 还是会深深的被震憾到  和谁同行  有时候旅行的意义可能不在景 可能更多的在于陪伴你旅行的人 更让人回味的片段 可能是三亚海边抢到躺椅后的得意洋洋 可能是天黑赶到大梅沙被人嘲笑的轻松无奈 可能是龙虾太贵不知道英语怎么说时有人帮我翻译的踏实 可能是樱花节决定不去看樱花时的默契 可能是一个无人问津的野山里找栗子的无厘头 可能是空荡的服务站里有人陪我守着充电站时的安静 可能是太湖边倚在帐篷里看风景时的满足 可能是动物园里学乌鸦叫时的搞怪 那些当时觉得很寻常的经历 那些曾经或现在一起旅行的人 共同构成了记忆 美好的记忆 有的时候旅行也不是为了自己 可能是为了老人 可能是为了孩子 相比于我们自己 他们的行动不如我们自由 带他们去看风景 安排和陪伴他们的行程 看着家人在旅途中开心地笑着 或者说一句 这地不错 这个地方还好玩 亲情需要陪伴 一起旅行是个很好的方式 不过老人和小孩对旅行的观点完全不同 老人会反馈：玩得太累 小孩会嚷嚷：还没玩够  留下印迹  我是最喜欢在旅行打卡拍照的那群人 比起在景点凹造型的大妈们 不遑多让 我对过去的记忆很差 喜欢用有记录可查的方式 记录下当时的人和事 就像我热衷于写工作日记一样 我喜欢旅行途中打卡拍照 喜欢留下印迹 如果不是道德制约 我可能会拿个小刀在每个到过的景点 都刻上：到期一游的记号 人生不过短短的一趟旅程 我想记下这些片段 因为这些迟早会被遗忘 就像我 用不了多久 我会忘掉这些经历 也许有天我老年痴呆了 失忆了 翻到过去的照片 会不会很欣喜  小结  2023年10月5日 关于旅行，我去过的地方不多 关于旅行，我在意风景，不在意吃喝玩乐 更在意的是和谁同行 旅行对我的意义 最重要的是：陪伴  </description>
    </item>
    
    <item>
      <title>数据库故障演练纪实</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%95%85%E9%9A%9C%E6%BC%94%E7%BB%83%E7%BA%AA%E5%AE%9E/</link>
      <pubDate>Tue, 26 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E6%95%85%E9%9A%9C%E6%BC%94%E7%BB%83%E7%BA%AA%E5%AE%9E/</guid>
      <description> 敢不敢随机挑选几台数据库机器，拔掉网线？
 演练时间  2023-09-26 19:30  演练目的  模拟线上服务器异常时 1.业务产生的影响 2.数据库高可用的生效时间 3.业务影响程度  前期准备  DBA将负责的三条数据库线的机器列表提供给运维 由运维同学采用抽奖程序，随机每条业务线抽到一台服务器 DBA检查抽中的服务器上的数据库及影响的业务范围 拉上受影响业务线的研发负责人和相关人士 注：各种leader 开会沟通时间点和风险点 确认时间点2023-09-26 19:30 开始  实际演练过程 影响范围  部分业务线在切换时产生短暂报错（计划中，影响可忽略不计） 数据库这边高可用方案可正常发挥作用，流量切换和高可用都是按预期的进行 总体演练结果，非常顺利。  2023-09-27 更新  Redis的演练导致大数据部门的一个故障 表现为flink的任务卡住了。 排查时还跑错了key和任务，用时较长，故障影响较大 Redis_Cluster集群 ，从节点的断网，也会影响业务短暂异常，如果程序处理不好，会造成很大的问题  </description>
    </item>
    
    <item>
      <title>安装Baichuan2百川智能大模型</title>
      <link>/ops/%E5%AE%89%E8%A3%85baichuan2%E7%99%BE%E5%B7%9D%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Mon, 25 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%AE%89%E8%A3%85baichuan2%E7%99%BE%E5%B7%9D%E6%99%BA%E8%83%BD%E5%A4%A7%E6%A8%A1%E5%9E%8B/</guid>
      <description>下载模型和预安装 git clone https://github.com/baichuan-inc/Baichuan2 python3.9 -V /usr/local/python3.9/bin/python3.9 -m pip install --upgrade pip python3.9 -m venv baichuan2 source baichuan2/bin/activate python -V python3.9 -m pip install --upgrade pip pip install -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/ vim cli_demo.py 运行和报错处理  执行python3.9 cli_demo.py 
 报错:urllib3 v2.0 only supports OpenSSL 1.1.1+ ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the &#39;ssl&#39; module is compiled with &#39;OpenSSL 1.0.2k-fips 26 Jan 2017&#39;. See: https://github.com/urllib3/urllib3/issues/2168 排查：  进python环境  python3.</description>
    </item>
    
    <item>
      <title>一个简单有趣的程序</title>
      <link>/ops/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E6%9C%89%E8%B6%A3%E7%9A%84%E7%A8%8B%E5%BA%8F/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E6%9C%89%E8%B6%A3%E7%9A%84%E7%A8%8B%E5%BA%8F/</guid>
      <description>会编程实在是一件很好玩的事 手机里选了300张照片 想着单纯的洗照片有点无聊 得在每张照片上加个单词 这时候 写一段python 简单而高效  代码如下： from PIL import Image, ImageDraw, ImageFont import os # def writeWord(fontname=&#39;沐瑶软笔手写体.ttf&#39;,words=&amp;quot;Python&amp;quot;): # img = Image.new(&#39;RGB&#39;, (800, 400), color = &#39;white&#39;) # draw = ImageDraw.Draw(img) # font = ImageFont.truetype(f&#39;fonts/{fontname}&#39;, 100) # draw.text((100, 150), words, font = font, fill = &#39;black&#39;) # img.save(&#39;PythonArt.png&#39;) def watermark_Image(img_path,output_path, text): img = Image.open(img_path) drawing = ImageDraw.Draw(img) black = (254, 223, 225, 179) font = ImageFont.truetype(&#39;fonts/opposansb.ttf&#39;, 350) drawing.</description>
    </item>
    
    <item>
      <title>五岳登山旅行系列之泰山篇</title>
      <link>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E6%B3%B0%E5%B1%B1%E7%AF%87/</link>
      <pubDate>Sun, 10 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E6%B3%B0%E5%B1%B1%E7%AF%87/</guid>
      <description>目录  1.恒山篇。 2.泰山篇。 3.华山篇,嵩山篇。 4.衡山篇。 5.总结篇。  行程  09-08 22:00 预订G119北京到泰安高铁票(242元) 09-09 07:50 出发,自驾前往北京南站,行程开始 09-09 08:47 到达北京南站 09-09 09:00 北京南站吃早饭(38元) 09-09 09:24 G119次列车北京南站正点出发 09-09 11:32 G119次列车到达泰安站 09-09 11:35 滴滴网约车从高铁站到泰山天景区(25.30元) 09-09 12:10 到达泰山天外村游客中心,购票入园(150元) 09-09 12:15 乘坐景区大巴摆渡车 09-09 12:50 摆渡大巴到达中天门 09-09 12:56 中天门吃饭(35元) 09-09 13:10 买了根登山棍，开始登山(5元) 09-09 14:06 中途休息，买了瓶水(15元) 09-09 14:20 到达南天门 09-09 14:42 到达观鲁台 09-09 15:00 到达五岳独尊打卡点 09-09 15:05 到达最高峰玉皇顶,海拔1545米 09-09 15:15 开始往回走 09-09 15:28 到达下山索道入口,开始排队(100元) 09-09 16:05 到达中天门，换乘景区摆渡车去往天外村游客中心 09-09 16:42 从天外村游客中心出泰山景区 09-09 16:44 打车去泰安高铁站(25元) 09-09 16:52 购买G138高铁票(223元) 09-09 17:05 到达泰安高铁站 09-09 17:24 G138次列车从泰安正点出发 09-09 19:35 G138次列车到达北京南站 09-09 19:45 驾车离开北京南站(停车费220元) 09-09 21:00 到达，行程结束 行程共用时13小时10分钟  费用  交通:242+25.</description>
    </item>
    
    <item>
      <title>【大聪明】微信机器人关机下线</title>
      <link>/ai/%E5%A4%A7%E8%81%AA%E6%98%8E%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%85%B3%E6%9C%BA%E4%B8%8B%E7%BA%BF/</link>
      <pubDate>Fri, 08 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E5%A4%A7%E8%81%AA%E6%98%8E%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%85%B3%E6%9C%BA%E4%B8%8B%E7%BA%BF/</guid>
      <description>   大聪明下线了  大聪明微信机器人不再提供AI回复 生于2023-02-24，卒于2023-09-08  一些时间点  2023-02-20 计划将ChatGPT搬到微信上 2023-02-24 第一版微信机器人上线，基于ChatGPT3.5 2023-02-25 建群:《大聪明的应答测试》 2023-03-18 支持:百度文心一言 2023-03-20 第一个微信群满员 2023-03-24 支持:GPT4.0和bing Chat 2023-03-29 支持:私聊 2023-04-02 建第二群:《大聪明的2群》 2023-04-02 支持:生成图片 2023-04-04 支持:本地训练的机器人（不需要和外网交互） 2023-04-21 支持:生成语音 2023-04-24 支持:图片识别,可以识别图片内容。 2023-04-25 支持:生成视频 2023-05-12 开始做网页版ChatGPT工作平台(www.top580.com) 2023-09-08 下线,不再提供服务  下线原因  ChatGPT热度已经下降 随着热度下降 更新这个功能的热情也不高 大聪明的最后一次代码更新:2023-05-12 更多的精力我在做网页版的AI应用 主要是AI的私有化训练方面 而ai问答这些基础应用 当前各类ChatGPT应用已经随处可见 体验和实用性比微信大聪明要好很多  一些数据  半年的时间里 大聪明共回复了:9201 条消息 其中:1535条私聊消息,7666 群聊消息 共有557人和大聪明在微信上沟通 互动最多的是:李明浩/dba:747次对话,luluci:305次,顾杰~学生:271次 ChatGPT让彼此陌生的人挤在一个群里 共度这一段探索的旅程 AI还在继续进步和发展 感谢大家半年的陪伴 陪我做了一件有趣的事 青山不改绿水长流 有缘江湖再见  </description>
    </item>
    
    <item>
      <title>五岳登山旅行系列之恒山篇</title>
      <link>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E6%81%92%E5%B1%B1%E7%AF%87/</link>
      <pubDate>Sun, 03 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/2023%E4%BA%94%E5%B2%B3%E7%99%BB%E5%B1%B1%E6%97%85%E8%A1%8C%E7%B3%BB%E5%88%97%E4%B9%8B%E6%81%92%E5%B1%B1%E7%AF%87/</guid>
      <description>目录  1.恒山篇。 2.泰山篇。 3.华山篇,嵩山篇。 4.衡山篇。 5.总结篇。  行程  09-01 20:48 买水果零食(167.4元) 09-02 06:43 出发,行程开始 09-02 07:32 接上同伴一起出发，路上有点堵 09-02 08:46 到达京藏高速官厅服务区,吃水果零食(充电费27.05元) 09-02 09:38 从官厅服务站出发 09-02 11:33 到达山西广灵服务区,吃水果零食,充电(充电费30.93元) 09-02 12:58 从广灵服务区出发 09-02 13:50 到达北岳恒山风景区停车场,(高速费145.26元) 09-02 14:00 恒山下小雨，景区一直不开放，等了半个小时 09-02 14:23 驾车离开恒山景区停车场(停车费10元) 09-02 15:19 到达桑干河国家湿地公园，购票入园(高速费12.35+门票40元) 09-02 15:51 从桑干河国家湿地公园出发 09-02 16:33 到达大同古城(高速费12.87元) 09-02 16:48 拿了紫泥369和龙聚祥的排队票,人巨多,开始等号 09-02 16:53 龙聚祥门口买了5串羊肉串(15元) 09-02 17:03 将车开到附近的充电桩充电(充电费37.26元) 09-02 18:02 足足排了1个半小时的队,开始紫泥369吃饭(餐费172元) 09-02 19:10 开始逛大同古城,摆摊游戏打气枪(游戏费20元) 09-02 19:57 开车前往大同沃尔玛 09-02 20:15 到达大同沃尔玛超市，采购水果零食和土特产(199.92元) 09-02 20:59 离开沃尔玛去酒店 09-03 07:30 酒店大堂吃早餐 09-03 08:11 开车从洒店去恒山景区 09-03 09:22 到达恒山景区停车场(高速费25.</description>
    </item>
    
    <item>
      <title>华为mate60pro开卖,国产麒麟9000S芯片是否打破封锁</title>
      <link>/book/%E5%8D%8E%E4%B8%BAmate60pro%E6%89%8B%E6%9C%BA%E7%AA%81%E7%84%B6%E5%BC%80%E5%8D%96%E5%9B%BD%E4%BA%A7%E9%BA%92%E9%BA%9F9000s%E8%8A%AF%E7%89%87%E6%98%AF%E5%90%A6%E6%89%93%E7%A0%B4%E5%B0%81%E9%94%81/</link>
      <pubDate>Wed, 30 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%8D%8E%E4%B8%BAmate60pro%E6%89%8B%E6%9C%BA%E7%AA%81%E7%84%B6%E5%BC%80%E5%8D%96%E5%9B%BD%E4%BA%A7%E9%BA%92%E9%BA%9F9000s%E8%8A%AF%E7%89%87%E6%98%AF%E5%90%A6%E6%89%93%E7%A0%B4%E5%B0%81%E9%94%81/</guid>
      <description>华为mate60pro手机突然开卖  昨天(2023-08-29)华为商城的一条消息突然冒了出来 华为开卖Mate60pro 这个本来预计在9月底才开发布会的新品突然就开始卖了 时间点选在美国商务部长雷蒙多访华期间(08-27到的北京) 这个消息为什么惊爆在于 这次搭载的麒麟9000S芯片是在美国制裁华为后的国产芯片 目前可收集的信息是：7nm,5G&amp;ndash;&amp;gt;国产 华为这波真的太牛逼了！！！！！！！！ 因为如果9000S芯片能量产的话 国内的芯片被卡脖子问题不说解了 但至少是可以呼吸了 你就说现在这9000s的手机芯片跟高通骁龙对使用者来说有多明显的差别 而且，技术这东西一旦掌握了 就有了底气可以一直在此基础上迭代发展 别人进步的时候，你也可以跟着进步 哪怕进步得没对方快 但是对方卡脖子就没那么容易了 虽然对华为的一些宣传策略不太感冒 但是如果真的做到了国产7nm芯片 只能是：服气 因为7nm的芯片已经可以满足99%的国内芯片使用市场了 华为要是真的做到了 这颗芯片和华为是可能会被写进历史书里的  官网截图     国产麒麟9000S芯片 这个芯片为啥这么惊喜  在这个芯片出来之前国内没有量产7nm芯片的能力 中芯国际工艺只能到14nm，没有更多消息证明中芯国际突破量产7nm工艺，在官网能查到的也只有14nm资料 台积电和三星的代工厂，华为被制裁显然用不了 但是7nm的9000S芯片被量产出来 也就侧面验证了国产（中芯国际）已经可以做出7nm的工艺了 这个突破就很厉害了  麒麟9000S芯片谁生产的  目前收集到的信息：华为与中芯国际合作生产的  麒麟9000S芯片性能如何  麒麟9000S的CPU主频2.62GHz 麒麟9000S的GPU为Maleoon910，主频750MHz 强于骁龙888 略低于骁龙8Gen1 略低于麒麟9000 一句话：基本上是性能不是最强的，但也差不到哪去  DUV还是EUV光刻机  EUV光刻机荷兰那还是封锁状态 中芯国际这次用的还是DUV 而7nm是DUV理论上能做到的极致 中芯应该是掌握了用DUV做7nm芯片的技术 且有效的提高了良品率达到了可以量产  麒麟9000S芯片 一点疑虑  一点疑虑“这芯片是国产的吗？” 这次华为太低调了 从头到尾没有宣传“国产芯片”的重大突破 包括官宣的公告也在强调其他的内容 那么这颗芯片是不是“纯国产” 是存在疑问的 退一步讲 哪怕这个国产芯不一定有想象中的那么纯 但是往前挤的态势还在 有了技术突破和积累 我们看到芯片贸易战的一点亮光 希望总是让人兴奋的  </description>
    </item>
    
    <item>
      <title>docker安装和常用命令</title>
      <link>/ops/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/</link>
      <pubDate>Sun, 27 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/</guid>
      <description>安装  centos7 安装  安装需要的软件包  yum-util 提供yum-config-manager功能，另外两个是devicemapper驱动依赖的  yum install -y yum-utils device-mapper-persistent-data lvm2 设置yum源 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 可以查看所有仓库中所有docker版本，并选择特定版本安装  yum list docker-ce --showduplicates | sort -r Loading mirror speeds from cached hostfile Loaded plugins: fastestmirror docker-ce.x86_64 3:24.0.5-1.el7 docker-ce-stable docker-ce.x86_64 3:24.0.4-1.el7 docker-ce-stable docker-ce.x86_64 3:24.0.3-1.el7 docker-ce-stable docker-ce.x86_64 3:24.0.2-1.el7 docker-ce-stable docker-ce.x86_64 3:24.0.1-1.el7 docker-ce-stable docker-ce.x86_64 3:24.0.0-1.el7 docker-ce-stable docker-ce.x86_64 3:23.0.6-1.el7 docker-ce-stable docker-ce.x86_64 3:23.0.5-1.el7 docker-ce-stable docker-ce.x86_64 3:23.0.4-1.el7 docker-ce-stable docker-ce.x86_64 3:23.0.3-1.el7 docker-ce-stable docker-ce.x86_64 3:23.0.2-1.el7 docker-ce-stable docker-ce.</description>
    </item>
    
    <item>
      <title>日本排放核废水的前因后果</title>
      <link>/book/%E6%97%A5%E6%9C%AC%E6%8E%92%E6%94%BE%E6%A0%B8%E5%BA%9F%E6%B0%B4%E7%9A%84%E5%89%8D%E5%9B%A0%E5%90%8E%E6%9E%9C/</link>
      <pubDate>Thu, 24 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%97%A5%E6%9C%AC%E6%8E%92%E6%94%BE%E6%A0%B8%E5%BA%9F%E6%B0%B4%E7%9A%84%E5%89%8D%E5%9B%A0%E5%90%8E%E6%9E%9C/</guid>
      <description> 北京时间2023-08-24中午12:00 也是日本福岛第一核电站启动核污染水排海的时间  起因  2011年3月11日，日本福岛县发生了里氏9级地震 福岛核电站的降温装置，导致降温机组出现停机。几十万度的反应堆堆芯瞬间升温，引发了著名的福岛核泄露事故 这是继1986年苏联切尔诺贝利核事故之后 世界上最严重的核事故之一 核泄露发生后东京电力公司开始引海水灌注反应堆，将堆芯温度给降下来 这些废水，不同于平时核电站的外循环用到的冷却水 这些水是接触到了核污染的水，叫做：核废水 这些核废水非常危险 经过处理后核废水中仍然残留了74种放射性物质 其中包括的氚、碳-14、锶-90、钚-239等 这些物质都有较长的半衰期 可以在环境中持续存在数千年甚至数万年 如果通过食物网进入人体内 会对身体健康导致不可避免的危害性 包含癌症 基因遗传基因突变等病症 所以全世界的人都知道这些核废水很危险 于是日本在海边建了很多的罐子来存放这些核废水 存了一罐又一罐，全都封存在储水罐里 福岛的核事故还没处理完 新的核废水还在不断产生，储水罐不可能永远建造下去 而且这些罐子一直放在那也不是个办法   核废水的处理方案      第一个方案是，排。经过一定的稀释处理后，再直接排回大海。代价：污染海洋 第二个方案是，烧。把核废水送进锅炉里烧，排到空气里去。代价：污染空气 第三个方案是，埋。从地表钻洞，然后搞一根深入地底达2500米的管线，把核废水排入地底2500米深处。代价：污染地下水 第四个方案是，电解。将核废水经过电解变成氢气和氧气，然后再排放进大气 。代价：污染空气 第五个方案是，固化。混入水泥，形成这样一个个水泥块，埋进土里。代价：污染地下水 5个方案里越往后，成本越大 只有第一个方案最偏宜 而且对日本本土的伤害最低 很明显对日本来说 其他方案都不如第一个 虽然对海洋生态确实会带来伤害 但是西方国家（美国）不反对 也没人能拦得住他 且日本百分之八十左右的日本人是支持日本政府决定将核废水直接排放入海。  核废水的影响  这是今年5月份的日本核废水的检测报告 https://www.iaea.org/sites/default/files/first_interlaboratory_comparison_on_the_determination_of_radionuclides_in_alps_treated_water.pdf  从里面看放射性物质还是很多很重的 不是日本媒体宣传的只有氚超标这么简单 数据上来这些核废水是非常非常危险的 那么影响有多大呢？ 首先看日本的排放方案是分批次排放 充分利用海洋的稀释能力 再加上排放前可能会做一些净化处理 所以总体上方案是可信的 不要低估海洋的稀释能力 那些水放在整个海洋里也是非常非常小的存在 大约是一条河里的倒进去一杯咖啡 因为日本海附近洋流的顺时针运动 最先影响的是俄罗斯，加拿大，美国 反而离日本近的中国，韩国受其影响小一些  海鲜还能不能吃？  能吃 但没必要 因为我本就不喜欢吃海鲜 现在还多了一个理由：放射性超标  </description>
    </item>
    
    <item>
      <title>宇宙的尽头是什么</title>
      <link>/book/%E5%AE%87%E5%AE%99%E7%9A%84%E5%B0%BD%E5%A4%B4%E6%98%AF%E4%BB%80%E4%B9%88/</link>
      <pubDate>Wed, 23 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E5%AE%87%E5%AE%99%E7%9A%84%E5%B0%BD%E5%A4%B4%E6%98%AF%E4%BB%80%E4%B9%88/</guid>
      <description> 应该是中学开始 有个问题就一直困扰着我 抛开鬼怪神说 现实世界的东西都是真实存在的物质 都是真实存在的 在空间里真实存在的东西 不管你感觉到有多大 它最终是可以被计量的 海洋一望无际 无边无界 但是只要视角足够大 最终海洋是有边界的 地球也是有边界的 后来人们也发现天上的星星月亮 也是真实存在的星球 人们可以接触到它 它是真实存在物质 记录片里会告诉我们在宇宙中 人类有多尘埃 地球有多小 这些都没问题 现实存在的这些星体，星系 未来科技进步了终究是可以达到的 哪怕是几亿光年的星球 这个星球也是真实存在的“物质” 但宇宙呢 这么多星球漂浮其中 它是个什么东西 它有边界吗 真实的世界里不会有一个“无限大“的物质 一只蚂蚁在森林里 它会认为森林是无限大的 但是只要把蚂蚁放大到一点 视线拉远一点 森林的尽头有河有山有人家 蚂蚁的森林不是无限大的 人类的地球也不是无限大的 星球的宇宙为什么是无限大的 而无限大就有个奇怪的地方 假设有种生物一直超宇宙的一个方向前进 那么它最终会一直看到宇宙中的各种种样的物质 这些物质都是什么 而假如这个生物最终会卡在一个边界上 那宇宙的尽头是什么？ 宇宙这个东西的外面又是什么东西？  </description>
    </item>
    
    <item>
      <title>DTCC参会者视角：2024年数据库大会和DBA之夜</title>
      <link>/dba/2023dtcc%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%A7%E4%BC%9A%E5%92%8Cdba%E4%B9%8B%E5%A4%9C/</link>
      <pubDate>Wed, 16 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/2023dtcc%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%A7%E4%BC%9A%E5%92%8Cdba%E4%B9%8B%E5%A4%9C/</guid>
      <description> 刚参加完DTCC2023的DBA之夜回来 原计划21:30结束的活动 快22点才收尾 还赶上北四环的下大雨 慢悠悠的开回来了 趁热呼劲写点内容  旧识  今天遇到了好多旧时的同事 艺龙的，去哪儿的 甚至还遇上了搜房的同事 不期而遇的快乐 感觉十分亲切 可以凑一起吧啦吧啦聊得起劲 疫情原因DTCC已经好几年没正经举办了 虽然人数不如2019年前 人气也不如以往 对我来说 在满地的年轻面孔里 看到几位旧相识 是件很开心的事了  国产数据库  依旧最火的话题 得益于国家的信创支持 把饭已经喂到我们这一行的嘴边了 聪明机灵的已经吃得撑起来了 先是政府事业单位 有钱的金融单位 国有企业 正在快速把数据库国产化 这一波的行情 预计还会持续3-5年 主要是去O，去DB2 然后用商业数据库的版权费 来支持国产DB的发展 这个不光是可以把核心数据系统 掌握在自己手里 也确实可以省掉很多的费用 就算是国产改造阶段产生了更多的费用 也是落在自己人手里不是吗  开源协议  MySQL和PG 国产数据库99%的源头 还有Mongo和Redis的协议约束 有个观点非常好 在国内的知识产权现状下 只要你掌握了开源数据库的代码 协议是约束不了你的 这个是现实情况 而且会持续很多年不会变 国外的开源协议 在国内当前的环境下 确实比较吃亏 我们也确实在占国外的偏宜 一方面用着国外的开源代码 一方面用这些代码改造了国产数据库 再代替国外的商业数据库软件 讲道理 这个我们在占便宜 但是现实是 一大批国内的数据库内核开发者已经养成了 现在拦也拦不住了 OB,TIDB这些年的代码自主量已经越来越少的开源的影子了  PG和MySQL  两个门派之争 我肯定是支持MySQL啊 主观客观上都得是MySQL 有时候简单也是优势 还有现在这覆盖规模和先发优势 没有大的变化 MySQL在国内的发展前景远胜于PG 但华为高斯类的PG改版活跃 还有平安之类的大企业在用 PG派也是有可能壮大的 然后天天骂PG的姜老师 人气还是旺 不少人就是过来看热闹的 只是今年的火药味不足 只有姜老师一个人在那冷嘲热讽 少了上次PG同学们的针锋相对 场面还是平淡了些 两派相争 目前还是周搞的世纪之争 战况激烈深入人心  然后  今天竟然有人说我新换的表带：有点娘炮 这个词让我耿耿于怀 得再换一个表带了  </description>
    </item>
    
    <item>
      <title>训练一个自己的AI机器人</title>
      <link>/ai/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E7%9A%84ai%E6%9C%BA%E5%99%A8%E4%BA%BA/</link>
      <pubDate>Tue, 08 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%AE%AD%E7%BB%83%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E7%9A%84ai%E6%9C%BA%E5%99%A8%E4%BA%BA/</guid>
      <description> 试用入口
 AI私有化训练的好处  已知ChatGPT之类的大语言模型的数据是在训练时的数据集基础上产生的人工智能 这些AI训练完成后，就不再更新，所以ChatGPT的数据和认知停留在2021年9月份 私有化训练的好处是 可以让AI学习你想要的知识，更新AI的能力 可以训练公司或个人的数据，满足特定的业务需求   训练公司的产品操作手册，用于客服AI    训练一些行业信息，提供信息咨询    用最新的数据，更新AI的知识库    用个人的聊天记录，训练出AI模仿这个人的对话语气    &amp;hellip;    训练需要哪些条件  训练AI一方面需要提供有价值的数据（这个就看积累了） 这些数据最好是文字格式的电子文件:word,excel,pdf,txt&amp;hellip;其中txt格式最佳 以及训练用到的工具  一台或几台高性能带显卡的电脑或服务器（硬件越好，训练得越快） 一个现成的AI(可以是国外的ChatGPT，也可以是自己部署的私有化AI) 如果训练的数据很重要，为了数据隐私可以选用私有化部署AI   如果你没有以上的工具，或不知道如何做私有化训练 请参考以下内容，我们做了一个免费的工具帮你训练一个属于你的私人定制AI  如何训练一个自己的AI机器人 第1步：填写AI名称 填写AI名称 第2步：上传数据  支持pdf,txt等格式 上传的数据越多，训练后的AI对相关的回答越智能 默认上传的数据是真实可信的，ai训练时不会去判断数据是否正确  上传数据 第3步：等待AI训练完成  到这一步后就只需要等着就行 而且需要等很久，建议隔一天再过来看进度。 这里为了演示，我上传的文件很少，也用了 AI训练非常费资源,文件件越多越大，训练用时越长。  等待AI训练完成 训练完成  这里上传了107MB文件，训练用了1个小时。  第4步：等待管理员审批  训练完的AI并不是立即可以用的 管理员（也就是我）会检查一下是否有不合适的内容（只要不出现敏感内容，都会通过的） 审批通过后，会收到一条短信提醒 这个训练好的AI就可以使用了  第5步：开始与新机器人对话  这里我们训练的文件里有大量的2022年国内家电行业的pdf文件 所以AI可以很准确的回答出相关的问题   总结  我们做了一个AI私有化训练平台 平台上的每个用户都可以训练自己的数据，形成属于自己的AI机器人 受制与网络带宽，如果你要训练的数据集很大，请用移动硬盘寄给我 请不要上传训练敏感内容（政治，色情，宗教&amp;hellip;） AI训练用时较长，上传完数据后要耐心等待 训练完成的AI机器人，可以在平台上自由使用  </description>
    </item>
    
    <item>
      <title>怎么通俗的理解向量和数字化</title>
      <link>/ai/%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E5%90%91%E9%87%8F/</link>
      <pubDate>Thu, 03 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E6%80%8E%E4%B9%88%E7%90%86%E8%A7%A3%E5%90%91%E9%87%8F/</guid>
      <description>从《向量数据库》说起  上周同学来望京聊天时 我说最近在做：向量数据库 然后就向他解释 什么是向量 当时我们在楼下公园里遛弯 路边停了一排摩托车 我指着一辆比较酷的摩托给他举例子 意外的很容易他就听懂了 向量的核心是：把世间万物都数字化 一个人，一个美女，一辆车，一条狗 一群僵尸，一段记忆，一句话&amp;hellip; 不管是什么 都用数字来表达  怎么做到向量的数字化呢  比如说：我 怎么用数字来描述我这个人呢 第1个维度： 在这个维度的设定是（物种:人：5001,狗:1001，猪:1002,花:2001,草:2002&amp;hellip;） 我是个人：不是狗，不是猪，不是花，不是草 所以得到一个数字:5001 第2个维度: 在这个维度的设定是（性别，男：1，女:0 ,不男不女:0.5,又男又妇:2,其他:3） 我是纯爷们,得到数字1,现在我的向量是[5001,1] 第3个维度是身高 于是我的向量:[5001,1,174] 第4个维度是籍贯 于是我的向量:[5001,1,174,551] 第5个维度是体重 于是我的向量:[5001,1,174,551,70] 第6个维度是小学学校名对应的数字 于是我的向量:[5001,1,174,551,70,22] 第7个维度是喜欢的运动 第8个维度是喜欢的食物 第9个维度是喜欢的书 第10个维度是脸长 第11个维度是脸宽 第12个维度是眼睛大小 第13个维度是鼻孔大小 第14个维度是腿上有几根毛 第15个维度是肌肉密度 第16个维度是帅气程度 第17个维度是脾气 &amp;hellip;. 于是我的向量:[5001,1,174,551,70,22,....] 维度越多 对我的描述越具体 最终我们定义一下，比如700个维度 那么最终我们用[5001,1,174,551,70,22,....]类似的700个数字定义了我 这一组数字就是我的向量表达  向量 有什么用？  刚才我们用向量表达了我这个人 这样会有什么好处呢 假设： 因为我的帅气英俊加上气宇不凡 有众多的追求者 但是我已经名花有主了 这时候追求者们 对我这个优秀的稀缺资源已经不能再被占有了 于是她们想找个：最接近我的人 这时候怎么办 去向量数据库里匹配向量和我重合度最高的人 这时候就会得到一批非常接近我的人 快去抢吧&amp;hellip; 机会不等人  向量 崩塌  这时候就会有人质疑了 会不会出现一个人的向量和我无限接近 只有某一项或少数几项和我的不一样的人 比如一个各个向量都接近的女版的我？ 比如会不会有条狗跟我的向量表达很近？ 如果向量的维度不够 是会出现的 但是维度足够多 就会避免出现此类向量崩塌的场景 一个女版的我 一定会在其他的很多维度上和我产生明显差异 比如头发，运动能力，比如胸围，比如&amp;hellip; 这就是向量的维度多的好处 大力出奇迹 维度多，细节到位 向量的匹配越真实  向量与AI  到这里是不是已经理解了向量是什么了 再从一句话来说 “帮我写一篇今天晚饭的健康食谱” 这一句话 在向量维度也是用一组数字来表达 首先做分词 再对每个词的词性做数字化 词之间的关联数字化 最终这句话会对应成一个向量表达 [2,82,234,5,22,2197,74.</description>
    </item>
    
    <item>
      <title>一个很会骂人的AI</title>
      <link>/book/%E4%B8%80%E4%B8%AA%E5%BE%88%E4%BC%9A%E9%AA%82%E4%BA%BA%E7%9A%84ai/</link>
      <pubDate>Fri, 28 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E4%B8%80%E4%B8%AA%E5%BE%88%E4%BC%9A%E9%AA%82%E4%BA%BA%E7%9A%84ai/</guid>
      <description>骂人的AI  中午吃饭时看到一个项目 说有人做了一个很会骂人的AI 好奇心有点重 能有多会骂人？ 就去试了几句 真的是张口就骂 给我整得一愣一愣的 当AI批上市巾的外衣 骂起人来也是顺风顺水。 原项目地址是：https://github.com/vastxie/Happy-ChatGPT 我看了这个项目的源码 找到它让AI变坏的关键点 复制出来一个粗欲的AI 放在我的AI平台上了 给没试过被AI骂过的朋友体验下 小心上头哈。  AI平台的进度  目前在做定制机器人的开发。 目标是给平台上的每个用户都可以训练自己的AI 用户提供文件 我们提供免费的训练平台 目前核心功能已完成 预计8月份上线试用  AI生成视频的解答  AI生成视频功能上线后 有很多的用户在平台上制作短视频 甚至7月26日有人加我微信说 生成的视频播放不了 检查发现 生成的视频把服务器磁盘空间占满了 于是做出以下策略 AI生成的视频保留7天 AI生成的图片保留30分钟 过期会自动删除 如果觉得有用，请自行在有效期内下载保留 另外平台上的生成图片，视频功能是免费的 有些用户会反复生成同一个主题的视频和图片 请节约有限的资源 不要恶意浪费  </description>
    </item>
    
    <item>
      <title>Google医疗大模型Med-PaLM突然进化到接近临床医生水平</title>
      <link>/ai/google%E7%9A%84%E5%8C%BB%E7%96%97%E5%A4%A7%E6%A8%A1%E5%9E%8Bmed-palm%E5%B7%B2%E6%8E%A5%E8%BF%91%E4%B8%B4%E5%BA%8A%E5%8C%BB%E7%94%9F%E6%B0%B4%E5%B9%B3/</link>
      <pubDate>Mon, 17 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/google%E7%9A%84%E5%8C%BB%E7%96%97%E5%A4%A7%E6%A8%A1%E5%9E%8Bmed-palm%E5%B7%B2%E6%8E%A5%E8%BF%91%E4%B8%B4%E5%BA%8A%E5%8C%BB%E7%94%9F%E6%B0%B4%E5%B9%B3/</guid>
      <description>Med-PaLM的进化  7月12日《Nature》发表文章，详解了医疗大模型Med-PaLM的进化过程 研究人员表示，当回答医学问题时，微调后的医疗大模型Med-PaLM表现良好，一组临床医生对其回答的评分为92.6%，与现实中临床医生的水平（92.9%）
  论文原地址：https://www.nature.com/articles/s41586-023-06291-2 有兴趣的同学可以看一下原文   记住这组数字：  在MultiMedQA评估基准下 临床医生对ai的正确率评估是：92.6% 而现实中临床医生正确率水平是：92.9% Google的Med-PaLM大模型还没有正式开放 从他们发表的论文上看 医疗领域的正确率这次提升非常明显 在AI+医疗领域目前Med-PaLM是最先进的AI 比通用大模型:GPT4的正确率要高很多  92.6%是什么水平  临床医生正确率：92.9% Med-PaLM进化版正确率：92.6% GPT4正确率：85.1% 初代版的Med-PaLM正确率：67.2% ChatGPT(GPT3.5)正确率:60.2% 这是个了不起的数据   AI的正确率有哪些影响  ai正确率的影响参考我3个月前的一篇文章: 《职业选择的新思考：ChatGPT是否会取代你的职位？》  AI+医疗领域  在AI+医疗领域，谷歌走在前列。 其最新的医疗大模型Med-PaLM 2为首个在美国医疗执照考试中达到专家水平的大模型。 据华尔街日报报道，自4月份以来，该模型一直在美国梅奥诊所等顶尖私立医院进行测试。 我和几个同学先前讨论过几次 一致认为：教育，律师，会计，医疗 这几个领域是AI最好落地的场景 而这4个领域一定会有专门训练且优化过的专业AI出现 专业的大模型在特定场景下比现在的chatgpt等通用大模型要好用 而伴随着谷歌这次抽风式的进化 AI医疗方面的应用落地 AI检测心电图、X光片会更快的一些医院中投入应用 而AI问诊,AI用药咨询也将快速落地  怎么使用Med-PaLM进化版  目前只有Google的测试结果 没有开放试用 而且这些专业领域的大模型一般会和指定的公司和商家合作 不会对大众开放 我在个人网站上增加了两个医疗ai机器人 基于ChatGPT 正确率一般，跟google此次的进化版有点弱 有想体验的同学也可以试用一下   </description>
    </item>
    
    <item>
      <title>MySQL压力测试之MySQLslap</title>
      <link>/mysql/mysql%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E4%B9%8Bmysqlslap/</link>
      <pubDate>Mon, 10 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95%E4%B9%8Bmysqlslap/</guid>
      <description>使用语法如下： mysqlslap [options] 参数 [options]  &amp;ndash;auto-generate-sql, -a 自动生成测试表和数据，表示用mysqlslap工具自己生成的SQL脚本来测试并发压力。 &amp;ndash;auto-generate-sql-load-type=type 测试语句的类型。代表要测试的环境是读操作还是写操作还是两者混合的。取值包括：read，key，write，update和mixed(默- 认)。 &amp;ndash;auto-generate-sql-add-auto-increment 代表对生成的表自动添加auto_increment列，从5.1.18版本开始支持。 &amp;ndash;number-char-cols=N, -x N 自动生成的测试表中包含多少个字符类型的列，默认1 &amp;ndash;number-int-cols=N, -y N 自动生成的测试表中包含多少个数字类型的列，默认1 &amp;ndash;number-of-queries=N 总的测试查询次数(并发客户数×每客户查询次数) &amp;ndash;query=name,-q 使用自定义脚本执行测试，例如可以调用自定义的一个存储过程或者sql语句来执行测试。 &amp;ndash;create-schema 代表自定义的测试库名称，测试的schema，MySQL中schema也就是database。 &amp;ndash;commint=N 多少条DML后提交一次。 &amp;ndash;compress, -C 如果服务器和客户端支持都压缩，则压缩信息传递。 &amp;ndash;concurrency=N, -c N 表示并发量，也就是模拟多少个客户端同时执行select。可指定多个值，以逗号或者&amp;ndash;delimiter参数指定的值做为分隔符。例如：- &amp;ndash;concurrency=100,200,500。 &amp;ndash;engine=engine_name, -e engine_name 代表要测试的引擎，可以有多个，用分隔符隔开。例如：&amp;ndash;engines=myisam,innodb。 &amp;ndash;iterations=N, -i N 测试执行的迭代次数，代表要在不同并发环境下，各自运行测试多少次。 &amp;ndash;only-print 只打印测试语句而不实际执行。 &amp;ndash;detach=N 执行N条语句后断开重连。 &amp;ndash;debug-info, -T 打印内存和CPU的相关信息。  优点：  可以针对某些特定类型的语句进行测试， 例如：  mysqlslap --no-defaults -h 127.0.0.1 -P 3307 --query=敏感列无索引where查询.sql --create-schema=db_test --concurrency=10,20 mysqlslap --no-defaults -h 127.</description>
    </item>
    
    <item>
      <title>北大法律大模型ChatLaw发布:中国法律界的智能助手</title>
      <link>/ai/%E5%8C%97%E5%A4%A7%E6%B3%95%E5%BE%8B%E5%A4%A7%E6%A8%A1%E5%9E%8Bchatlaw/</link>
      <pubDate>Fri, 07 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E5%8C%97%E5%A4%A7%E6%B3%95%E5%BE%8B%E5%A4%A7%E6%A8%A1%E5%9E%8Bchatlaw/</guid>
      <description>7月3日 北京大学信息工程学院袁粒课题组+北大兔展AIGC联合实验室两个团队联合发布了ChatLaw 可能是目前国内比较先进的法律大模型 从实际体验来看 1.在国内法律领域比ChatGPT回答得更专业 2.在其他回答上很勉强，比ChatGPT弱很多,有点呆 3.本地部署非常费资源,对机器要求较高 4.性能不佳，每次回答容易卡住且不流畅 考虑到6月27日幂律智能联合智谱AI法律垂直大模型 PowerLawGLM。 还有再早一点开源社区也发布了一些AI法律微调模型 由于ChatGPT的普及，LLMA的开源 而法律行业的规律性，可文本化，可预测性比较强 大语言模型的崛起为普通人的咨询法律相关问题提供了很多便利  关于ChatLaw 信息汇总  官网地址：https://www.chatlaw.cloud/ 论文地址：https://arxiv.org/pdf/2306.16092.pdf GitHub 地址：https://github.com/PKU-YuanGroup/ChatLaw 根源上这个模也还是基于meta公司的LLaMA模型(小扎的这次开源给国内的大语言模型贡献巨大，现在市面上大多数开源模型都是基于Meta开源的LLAMA） 北京大学信息工程学院袁粒课题组 北大-兔展AIGC联合实验室联合发布 主要团队成员： Jiaxi Cui Zongjian Li Yang Yan Bohua Chen Li Yuan  这是官方宣传视频   这是实际体验&amp;hellip;  问了一个踩到狗屎的问题，等了5分钟，还卡在这   ChatLaw的版本  共有三个版本，分别如下： ChatLaw-13B，为学术 demo 版，基于姜子牙 Ziya-LLaMA-13B-v1 训练而来，中文各项表现很好。但是，逻辑复杂的法律问答效果不佳，需要用更大参数的模型来解决； ChatLaw-33B，也为学术 demo 版，基于 Anima-33B 训练而来，逻辑推理能力大幅提升。但是，由于 Anima 的中文语料过少，问答时常会出现英文数据； ChatLaw-Text2Vec，使用 93w 条判决案例做成的数据集，基于 BERT 训练了一个相似度匹配模型，可以将用户提问信息和对应的法条相匹配。  普通人怎么用上ChatLaw  方法1.官网上申请内测 https://www.</description>
    </item>
    
    <item>
      <title>Langchain的生态链开源产品</title>
      <link>/ai/langchain%E7%9A%84%E7%94%9F%E6%80%81%E9%93%BE%E5%BC%80%E6%BA%90%E4%BA%A7%E5%93%81/</link>
      <pubDate>Wed, 05 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/langchain%E7%9A%84%E7%94%9F%E6%80%81%E9%93%BE%E5%BC%80%E6%BA%90%E4%BA%A7%E5%93%81/</guid>
      <description>Langchain生态 低代码  Langflow: LangFlow是LangChain的UI Flowise - LangchainJS UI：拖放 UI 以使用 LangchainJS 构建自定义的 LLM 流程 Databerry：用于语义搜索和文档检索的无代码平台 LangchainUI: 开源聊天人工智能工具包 Yeager.ai：Yeager.ai Agent 是第一个 Langchain Agent 创建者，旨在帮助您轻松构建、原型设计和部署 AI 支持的代理  服务  GPTCache：用于为 LLM 查询创建语义缓存的库 Gorilla：LLM 的 API 商店 LlamaHub：社区制作的 LLM 数据加载器库 EVAL：带有 Langchain 的弹性多功能代理。 将执行您的所有请求。 Auto-evaluator：使用Langchain进行问答的轻量级评估工具 Langchain Visualizer：LangChain工作流程的可视化和调试工具 LLM策略：使用LLM实现策略模式 datasetGPT：使用 LLM 生成文本和会话数据集的命令行界面。 spellbook-forge：使您的 LLM 提示可执行并受版本控制。 自动评估器: Langchain 自动评估器 Jina：使用 Jina 进行生产的 Langchain 应用程序 Gradio 工具: Gradio LLM 代理 steamship-langchain：Steamship 适配器，使 LangChain 开发者能够在 Steamship 上快速部署应用程序🐍 LangForge：用于创建和部署 LangChain 应用程序的工具包 BentoChain: LangChain 在 BentoML 上的部署 LangCorn: 使用 FastApi 自动为 LangChain 应用程序提供服务 Langchain 服务：带有 Qdrant 矢量存储和 Kong 网关的自定 Langchain 设置 Lanarky: 使用 FastAPI 交付可用于生产的 LLM 项目 Dify：一个用于插件和数据集的 API，一个用于快速工程和可视化操作的界面，所有这些都用于创建强大的 AI 应用程序。 LangchainJS Worker：cloudflare 上的 LangchainJS Worker Chainlit：在几分钟内构建 Python LLM 应用程序 Zep：Zep：LLM/Chatbot 应用程序的长期内存存储 Langchain Decorators：LangChain 顶部的一层，为编写自定义 langchain 提示和链提供语法糖 FastAPI + Chroma：ChatGPT 的示例插件，利用 FastAPI、LangChain 和 Chroma AilingBot：快速将Langchain上构建的应用集成到Slack、企业微信、飞书、钉钉等IM中。  代理  CollosalAI Chat：使用 RLHF 实施 LLM，由 Colossal-AI 项目提供支持 AgentGPT：使用 Langchain 和 OpenAI (Vercel / Nextjs) 的 AI 代理 本地 GPT：受到私有 GPT 的启发，将 GPT4ALL 模型替换为 Vicuna-7B 模型，并使用 InstructorEmbeddings 代替 LlamaEmbeddings ThinkGPT：增强 LLM 并超越其极限的代理技术 Camel-AutoGPT：法学硕士和自动代理（如 BabyAGI 和 AutoGPT）的角色扮演方法 私有GPT：利用GPT的力量与您的文档进行私密交互，100%私密，无数据泄露 RasaGPT：RasaGPT 是第一个构建在 Rasa 和 Langchain 之上的无头 LLM 聊天机器人平台。 SkyAGI：LLM 代理中新兴的人类行为模拟能力 PyCodeAGI：一个小型 AGI 实验，根据用户想要构建的应用程序生成 Python 应用程序 BabyAGI UI：让在网络应用程序中使用babyagi更容易运行和开发，比如ChatGPT SuperAgent：将 LLM Agent 部署到生产环境 Voyager：具有大型语言模型的开放式实体代理 ix：自治 GPT-4 代理平台 DuetGPT：对话式半自主开发助手，无需复制粘贴即可进行 AI 结对编程。 生产环境中的多模式 LangChain 代理: 部署 LangChain Agent 并将其连接到 Telegram DemoGPT：DemoGPT 使您只需使用提示即可创建快速演示。 它在 Langchain 文档树上应用了 ToT 方法。 SuperAGI：SuperAGI - 开发优先的开源自主人工智能代理框架 自主 HR 聊天机器人：一个自主代理，可以使用现有工具自主回答 HR 相关查询 BlockAGI：BlockAGI 进行迭代、特定领域的研究，并输出详细的叙述性报告来展示其发现  模板  AI：Vercel 模板，用于使用 React、Svelte 和 Vue 构建人工智能驱动的应用程序，对 LangChain 提供一流的支持 create-t3-turbo-ai：基于 t3、Langchain 友好的样板，用于构建类型安全、全栈、LLM 驱动的样板 使用 Nextjs 和 Prisma 的 Web 应用程序 LangChain.</description>
    </item>
    
    <item>
      <title>MySQL5.7实例无限重启故障定位及解决</title>
      <link>/mysql/mysql5.7%E5%AE%9E%E4%BE%8B%E6%97%A0%E9%99%90%E9%87%8D%E5%90%AF%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D%E5%8F%8A%E8%A7%A3%E5%86%B3/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql5.7%E5%AE%9E%E4%BE%8B%E6%97%A0%E9%99%90%E9%87%8D%E5%90%AF%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D%E5%8F%8A%E8%A7%A3%E5%86%B3/</guid>
      <description>故障开始时间：2023-06-30 09:18 故障实例：BI大数据业务
 环境  系统：CentOS Linux release 7.8.2003 (Core) MySQL: 5.7.28-log MySQL Community Server (GPL) 部署：多实例部署，当前实例bufferPool:8G 集群：3台主机  主：51 备：52 (和51做双主) 从：53 （同步自52）    故障现象  收到报警，该实例频繁报连接异常和恢复 检查发现该MySQL实例频繁重启 1.该实例访问量很小，不是资源不足引起 2.和研发确认该实例相关的业务最近未发生变更 3.DBA内部确认最近该实例没有做配置变更 4.报错时系统日志无异常报错 5.MySQL正常运行时可以提供服务，但1分钟左右就会自动shutdown 6.慢日志里没有异常SQL 7.MySQL错误日志里只有实例启动后自检的warinning 以及  2023-06-30T10:15:35.534553+08:00 0 [Warning] CA certificate ca.pem is self signed. 2023-06-30T10:15:35.546909+08:00 0 [Warning] Recovery from master pos 59075485 and file mysql-bin.***** for channel &#39;&#39;. Previous relay log pos and relay log file had been set to 416, /data/mysql******/relaylognew/relay-bin.</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_生成视频</title>
      <link>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_8/</link>
      <pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_8/</guid>
      <description>准备工作  1.打开www.top580.com 2.登入系统 这一步参见文档:注册和登录的详细说明  2.登入系统  这一步参见文档:注册和登录的详细说明  教学视频   </description>
    </item>
    
    <item>
      <title>定时收集Oracle索引信息</title>
      <link>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86oracle%E7%B4%A2%E5%BC%95%E4%BF%A1%E6%81%AF/</link>
      <pubDate>Mon, 26 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86oracle%E7%B4%A2%E5%BC%95%E4%BF%A1%E6%81%AF/</guid>
      <description>需求  把Oracle各实例的索引信息，统一收集存储到一张表里  建表 在dboop库中建表
 CREATE TABLE `info_indexs` ( `dbid` int NOT NULL DEFAULT &#39;0&#39;, `table_name` varchar(255) NOT NULL DEFAULT &#39;&#39;, `index_name` varchar(255) NOT NULL DEFAULT &#39;&#39;, `index_type` varchar(64) NOT NULL DEFAULT &#39;&#39;, `uniq_type` varchar(64) NOT NULL DEFAULT &#39;&#39;, `num_rows` int NOT NULL DEFAULT 0, `sample_size` int NOT NULL DEFAULT 0, `last_analyzed` datetime null, `column_name` varchar(2000) NOT NULL DEFAULT &#39;&#39;, `cstatus` smallint NOT NULL DEFAULT &#39;1&#39;, `dba_freshtime` datetime NOT NULL DEFAULT &#39;1990-01-01 00:00:00&#39;, PRIMARY KEY (`dbid`,`table_name`,`index_name`), KEY `idx_info_indexs_time` (`dba_freshtime`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 ; 实现数据字典收集入库 建Oracle采集任务  &amp;lt;!</description>
    </item>
    
    <item>
      <title>关于数据安全_DBA篇</title>
      <link>/dba/%E5%85%B3%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8_dba%E7%AF%87/</link>
      <pubDate>Thu, 15 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%85%B3%E4%BA%8E%E6%95%B0%E6%8D%AE%E5%AE%89%E5%85%A8_dba%E7%AF%87/</guid>
      <description>前言 明天有个会，大家一起商量一下怎么做好数据安全。提前整理一下思路：
什么是数据安全  数据安全是指保护数据不被非法获取、篡改、破坏或泄露的一种技术和管理措施。 在我这里，数据安全要更具体一点 1.存储安全：数据只要写到数据库里了，就不会丢  1.1 存储上：多节点存储，防止物理损坏 MySQL高可用组件之ProxySQL 1.2 数据按固定的周期进行全备和日志备份 数据库备份管理制度 1.3 自动化脚本检查备份成功和验证 1.4 保证数据被意外删除后，还能找回来 自动化流程:数据找回(一:MySQL数据闪回) 自动化流程:数据找回(二:Oracle部分) 1.5 保证数据和数据库备份是双机房异地存储 数据库备份管理制度 1.6 额外做孤岛备份，以防止内网机房的病毒大面积感染 孤岛备份机和勒索病毒   2.账号策略：只有指定权限的用户可以访问可控范围内的数据（到库级别）  要求研发分业务存储库，不要混用数据库 账号自动化管理，权限限制在可控范围内 账号密码不分发给研发，由运维人员统一配置（这点很重要，为第三步的访问控制提供前题）   3.访问控制：将业务人员和运维人员隔离  业务人员指研发，产品，测试，大数据，风控&amp;hellip; 运维人员：DBA 运维 只有运维人员可以接触到线上数据库，研发和其他人员均不可连接到数据库机器和实例 将研发等业务人员和数据库的接触限定在两个方式内：1.通过程序代码操作数据库 2.通过DBA的Web平台操作数据库 线上查询和线上变更。走DBA提供的平台执行 限制DBA等运维人员，了解业务逻辑，杜绝DBA直接查询和修改线上业务数据 我为什么要反对DBA参与业务(出报表/改数据)   4.安全审计：线上的数据异常，要有日志可查  数据变更日志（binlog，归档日志等） MySQL的7种日志(四):BinLog SQL上线日志 （记录变更新镜像和更新后结果，方便快速回滚） 数据库多环境SQL上线 异常日志和慢日志收集到es 服务器操作日志，数据库账号变更日志 个人查询日志，部分线上查询审计日志   5.数据加密：数据库里的敏感信息应该加密存储  哪些属于敏感信息：手机号.卡号.身份证号.住址&amp;hellip; 首先需要把敏感信息标识出来。  为此我们开发了一个工具，在用户建表或者修改表结构时，会识别出来 外加一个兜底脚本，定时扫描SQL查询结果，如果发现有敏感信息未标识的就会提示出来   敏感信息标识后，不管底层是否做了加密存储，DBA和大数据平台都可以对这些字段做针对性的掩码，防止信息泄露 数据的加密存储，这个单做一节，详细说说    数据加密  如上一节最后说的，我们已经将敏感信息识别出来了，现在怎么做数据的加密存储。根据实际情况展开来说  新项目的数据加密  如果有开发资源：架构组开发一套通用的加密服务，新项目调用 如果没有开发资源： 研发用通用的加密算法对敏感信息进行可逆的加密（例AES)后入库  老项目的数据加密改造 方法一：数据库里存的是加密数据，研发存放和读取都是明文数据  应付审计之法。 优点是：库里的数据确实是加密的 缺点是：研发和业务人员查询时是明文的 这个需要借助第三方中间件来实现：（例如SphereEx） 我头一次听SphereEx讲他们的中件层加密时，觉得这个思路非常棒 这可能是比较节约开发资源的，又可以应付审计的一种加密方式。 这是它的优点也导致了一个缺点：研发查出来数据库里的信息还是明文，数据防泄露效果差 只防住了DBA和运维人员的泄密，而更关键的业务泄露并没防住 加了中间层，稳定性待考证 加密收益： 2颗星 ，加密工作量：1颗星  方法二：数据库里存的是明文数据，研发读取到前台展示的时候是密文的  防前台泄密之法 在SQL层将所有的查询接口都改造一下，需要花费不少的时间（2-3周） 优点是，前台用户看到的数据是加密或掩码的。解密记录是可审计的，防止信息泄漏 缺点是，数据库明文存储了，治标了但没治本 加密收益： 4颗星 ，加密工作量：3颗星  方法三：数据库里存的是密文数据，研发读取到前台展示的时候是密文的  这个就是把旧项目彻底改造了，存数据和读数据的地方都要改一下 这个改造的工作量非常大，但是效果是最彻底的 最完整的方案是分成三个角色 DBA提供存储，架构组提供加解密服务，研发存储和读取的都是密文 其中架构组是核心，提供整套加解密服务 研发参与成本最大，需要在写数据和读数据时修改代码 加密收益： 5颗星 ，加密工作量：4颗星  </description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_生成流程图</title>
      <link>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_7/</link>
      <pubDate>Tue, 23 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_7/</guid>
      <description>准备工作  1.打开www.top580.com 2.登入系统 这一步参见文档:注册和登录的详细说明  2.登入系统  这一步参见文档:注册和登录的详细说明  生成流程图 功能介绍  提供一些关键词，让AI依据这些关键词生成指定的流程图 支持流程图,状态图,时序图,甘特图,类图,饼图,关系图  使用说明  平台是业务时间个人写的，刚刚试用，可能会有很多bug和使用不便的地方，欢迎和我反馈
 &amp;hellip;
enjoy it .</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_生成图片</title>
      <link>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_6/</link>
      <pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_6/</guid>
      <description>准备工作  1.打开www.top580.com 2.登入系统 这一步参见文档:注册和登录的详细说明  2.登入系统  这一步参见文档:注册和登录的详细说明  生成图片 功能介绍  提供一些关键词，让AI依据这些关键词生成指定的图片 支持卡通画,简笔画,素描画,中国画,水彩画&amp;hellip;  使用说明  平台是业务时间个人写的，刚刚试用，可能会有很多bug和使用不便的地方，欢迎和我反馈
 &amp;hellip;
enjoy it .</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_AI对话</title>
      <link>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_5/</link>
      <pubDate>Sun, 21 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_5/</guid>
      <description>准备工作  1.打开www.top580.com 2.登入系统 这一步参见文档:注册和登录的详细说明  AI对话 功能介绍  和ChatGPT进行对话，在对话中解决问题或生成各式各样的文章 支持：闲聊，散文，诗歌，小说，故事，笑话&amp;hellip;.  使用说明  平台是业务时间个人写的，刚刚试用，可能会有很多bug和使用不便的地方，欢迎和我反馈
 &amp;hellip;
enjoy it .</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com使用说明_用户注册和登录</title>
      <link>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_4/</link>
      <pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_4/</guid>
      <description>第一步，打开www.top580.com  界面如下图所示   第二步：点击按钮进入 用户登录页  首次加载时，需要大约10秒钟的准备时间 加载成功后，会进入登录页  如果已经注册过的用户，直接用手机号和密码登录 新用户点下方的点击进入试用申请  第三步：新用户注册（可选）  老用户无需此步 认真填写资料后点提交 密码会用【短信的方式】发送到你的手机上 这个密码很重要，记住它 暂时还没做忘记密码功能，如果密码忘了，就没了。切记 -  第四步：登入系统  已经注册过的用户，直接用手机号和密码登录  当你看到这个页面的时候，就表示你登入成功了   第五步：开始使用  进入系统后，就可以正常使用AI功能了 每个AI的使用方式和技巧，会单独出文章 -   平台是业务时间个人写的，刚刚试用，可能会有很多bug和使用不便的地方，欢迎和我反馈
 &amp;hellip;
enjoy it .</description>
    </item>
    
    <item>
      <title>人工智能平台top580.com_使用说明(汇总)</title>
      <link>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_3/</link>
      <pubDate>Fri, 19 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_3/</guid>
      <description> 我做了一个智能AI平台 免费使用.让更多的人接触ChatGPT类似的AI智能产品 为防资源滥用,每个用户每天可以和AI对话/生成/次数有限制 未认证用户:【20次/天】 实名验证用户:【50次/天】 用完次数后，需要等第二天再继续。 功能包括： ChatGPT的AI对话 ChatGPT生成图片 ChatGPT生成各种文档，创意文案，诗词&amp;hellip; ChatGPT生成流程图,时序图,类图,甘特图&amp;hellip; ChatGPT生成视频（调试中,暂时未开放） 网址是：www.top580.com 文档1:注册和登录的详细说明 文档2:ChatGPT对话的详细说明 文档3:AI生成图片的使用说明 文档4:AI生成流程图的示例 文档5:AI生成视频的视频讲解 更多功能和使用方式，正在准备中&amp;hellip;  如果你想了解目前该项目的进度:进度更新    </description>
    </item>
    
    <item>
      <title>人工智能平台top580.com_进度更新</title>
      <link>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_1/</link>
      <pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0_1/</guid>
      <description>试用入口
 当前进度 系统功能  域名转到cloudflare 已完成 阿里云主机注册 已完成 openai的api申请 已完成 azure的api申请 未开始 域名备案 已完成 用户注册 已完成 手机验证码平台 已完成 密码生成下发 已完成   用户登录 已完成 用户权限 已完成  通用AI  文本问答 进行中  结果记录代码 已完成 结果展示优化 已完成 翻译助手 未开始 超长文本 已完成   流程图 已完成 生成图片 已完成 生成音频 40% 生成视频 已完成 图片识别 已完成，代价太大，放弃  定制AI  私有数据上传 已完成 私有数据训练 已完成 私有ai问答 已完成 专用ai 进行中  医疗助手 已完成 症状助手 未开始 药品辅助 未开始 心理辅导 未开始   法律助手 进行中  文律文书生成 已完成 国内法规向量训练 已完成 案件线索 未开始 法规图谱 未开始 模拟法庭 进行中 30%   代码助手 未开始  代码生成 代码解释      其他  多平台ai接入 未开始 隐私离线ai 已完成   按日期更新 20230920  增加图生图功能（用一张图+文字生成一张新的图）  20230918  生成照片列表页改成首页即可见 增加生成3d图标和超现实主义照片的模型 禁用了平台上提擦边球图片的用户，并增加负面提示词防止生成色情图片  20230916  生成照片，脸容易变形的问题修复 这个bug是在去天津的路上修复的，这个bug太恶心了，出来的美女脸一变形跟鬼一样，实在是忍不了  20230915  妹子功能上线，发现脸部总是有点扭曲，待修复  20230914  在二手显卡的帮助下，生成妹子功能取得质的突破，不管是二次元，还是超写实，效果都比以前的好太多。 放弃了以前的生成图片功能和接口，改用全新的训练模型。预计3-5天可以完工。  20230911  生成声音功能规划，最终还是要文字部分区分开来。共用一个LOG表，和视频共用一套流程  20230910  生成视频页面改造，页面动态调整，为生成其他媒体内容改造。  20230908  用户反馈需要忘记密码功能，完成页面增加 70% ,没做完。  20230907  录制《AI私有化训练》教学视频，视频太长，效果不好。  20230906  调整AI生成各种功能时放在不同库表中存储的问题，共用一套库表，共用一个前/后端页面 又买了块二手显卡，用于生成细粒度的人物图（漂亮妹子）  20230905  忙了一段时间没有更新这个AI 继续更新：AI私有化增加：国产,本地模型选项  20230815  正式上线：AI私有化训练功能，离线AI问答功能  20230814  显卡周末到货了，今天花了大半天的时间在安装这个显卡 现有的服务器，台式机的电源都供不上（需要2个8针的750w供电） 叠了台式机和服务器的电源，也没带动 考虑退货了，就先将就着云上资源先用着  20230810  AI训练：完成AI训练完的机器人的问答测试 下单了一块显卡。打算提升一下训练速度  20230806  AI训练：远程调用的问题修复完成  20230802  发布AI训练到线上，发现远程调用有问题  20230730  AI训练：去掉训练完由用户确认的步骤  20230728  增加粗俗的AI,出口成脏，很会骂人 调整现有的ai对话prom类型。将类型人格化  20230725  AI训练：完成流程的初步调试 发现打包上传后出错（原因未定位）  20230724  AI训练：完成用户上传后，后台训练的部分 完成celery的任务互动，完成异步任务 完成内网传透，已节约阿里云费用  20230723  AI训练：AI训练流程增加审核节点  20230720  AI训练：AI训练流程（新建,训练，确认,完成） 制作流程页，主要是用户上传部分  20230718  增加了实时联网的ai：用搜索引擎查询用户输入，返回相应结果（未上线，待私有ai一起打包上线）  20230717  增加医疗ai。去掉文本对话中不常用到的4个ai  20230715  调试完成langchain和向量支持 完成私有ai训练的核心模块（重要）  20230711  生成视频功能出错,bug修复 07-09到07-11 共39个用户生成视频的请求失败。  20230708  私有数据上传页面（前端） 80% 模拟法庭初版页面 30%  20230628  生成视频功能正式开放使用  20230625  生成视频功能补全（上传到阿里云，视频在线播放，视频生成日志）  20230624  生成视频前端页面完成  20230622  生成视频功能完成核心代码（基本能用）  20230617  尝试网站微信扫码登录（用于代替手机短信验证） 失败：我的微信公众号不是用企业名义注册的，无法开通此功能 扫码登录的代码已完成，功能暂不可用  20230614  平台来了很多新用户，系统资源有点紧张 开了阿里云PolarDB，后台数据存储转向RDS 修改了图片生成功能，用户的输入将会被翻译成英语以后，再生成图片。（提高图片的准确性）   20230611  生成视频，去掉审核环节 修复注册页bug ：appuseid传输列表时报错。  20230607  修复ai不能连续对话的bug。ai可以记住历史聊天内容了 修复加载卡住的问题（首页加载时间依旧会过长，还需要做个加载动画） 修复注册页面直接跳到首页的bug ai生成视频功能开始做了  20230530  1.</description>
    </item>
    
    <item>
      <title>要做一个人工智能平台</title>
      <link>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0/</link>
      <pubDate>Fri, 12 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E8%A6%81%E5%81%9A%E4%B8%80%E4%B8%AA%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%B9%B3%E5%8F%B0/</guid>
      <description>功能  在做一个AI智能平台 可以对接多个ai平台 不仅仅是ChatGPT 还支持私有化数据训练 用户可以流畅的在国内访问 用户可以提交自己的私用数据做训练 用户训练的私有数据可以选择是否公开给其他人用 会有模块化的已定义好的ai可以直接用 支持图片,音频,视频,图表的生成 支持针对行业的定制化ai功能  成本  前期完全免费 接受用户反馈 不断迭代改进 等产品成熟了以后 估计在6月份以后 会收取一定的费用 以覆盖服务器的运行成本 理论上应该会非常偏宜 现在估计的运行成本 大约是500元/月 如果有100个用户的话 每个人收5元/月 应该问题不大 实在收不上来 自已负担也没问题  </description>
    </item>
    
    <item>
      <title>Oracle增加表审计</title>
      <link>/oracle/oracle%E5%A2%9E%E5%8A%A0%E8%A1%A8%E5%AE%A1%E8%AE%A1/</link>
      <pubDate>Wed, 10 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/oracle/oracle%E5%A2%9E%E5%8A%A0%E8%A1%A8%E5%AE%A1%E8%AE%A1/</guid>
      <description>增加表审计 # 查看现在有哪些审计对象 select * from dba_obj_audit_opts; # 开启审计 audit select,update,insert,delete on 用户.表名 by access # 关闭审计 noaudit select,update,insert,delete on 用户.表名 # 查看审计结果 select * from dba_audit_trail 清空审计 sqlplus / as sysdba truncate table sys.aud$; ``</description>
    </item>
    
    <item>
      <title>postgreSQL的安装和初始化(centos7)</title>
      <link>/dba/postgresql%E7%9A%84%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3centos7/</link>
      <pubDate>Wed, 26 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/postgresql%E7%9A%84%E5%AE%89%E8%A3%85%E6%96%87%E6%A1%A3centos7/</guid>
      <description>安装 # Install the repository RPM:sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm# Install PostgreSQL:sudo yum install -y postgresql15-server##如果需要安装postgresql15-develwget https://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/l/llvm5.0-devel-5.0.1-7.el7.x86_64.rpmwget https://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/l/llvm5.0-5.0.1-7.el7.x86_64.rpmwget https://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/l/llvm5.0-libs-5.0.1-7.el7.x86_64.rpm yum install -y centos-release-scl-rh llvm5*yum install -y postgresql15-devel# Optionally initialize the database and enable automatic start:sudo /usr/pgsql-15/bin/postgresql-15-setup initdbsudo systemctl enable postgresql-15sudo systemctl start postgresql-15安装extenyum install pgvector_15配置 修改监听 修改监听地址为任意地址  vi /var/lib/pgsql/15/data/postgresql.conf 在第60行处增加   54 #------------------------------------------------------------------------------55 # CONNECTIONS AND AUTHENTICATION56 #------------------------------------------------------------------------------57 58 # - Connection Settings -59 60 listen_addresses = &#39;*&#39;61 #listen_addresses = &#39;localhost&#39; # what IP address(es) to listen on;62 # comma-separated list of addresses;63 # defaults to &#39;localhost&#39;; use &#39;*&#39; for all允许所有IP访问  vi /var/lib/pgsql/15/data/pg_hba.</description>
    </item>
    
    <item>
      <title>微信上的ChatGPT机器人新功能语音的使用说明</title>
      <link>/ai/%E5%BE%AE%E4%BF%A1%E4%B8%8A%E7%9A%84chatgpt%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BC%80%E5%A7%8B%E6%94%AF%E6%8C%81%E8%AF%AD%E9%9F%B3%E4%BA%86/</link>
      <pubDate>Fri, 21 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E5%BE%AE%E4%BF%A1%E4%B8%8A%E7%9A%84chatgpt%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BC%80%E5%A7%8B%E6%94%AF%E6%8C%81%E8%AF%AD%E9%9F%B3%E4%BA%86/</guid>
      <description>上一节内容：
 我在微信上用chatGPT做了个大聪明
 20230421更新  微信ChatGPT机器人（大聪明）开始支持语音回复了 又是一个中看不中用的功能（文字问答才是最简单实用的） 触发语音回复的方式如下图所示  进阶技巧：  大聪明的声音是可以设置的，默认是个青年男子的声音，你可以切换成其他的声音  1.获取声音列表  如下图所示，共有25种声音可供选择，支持中，粤，日，韩，美 等主要语音  1:晓晓,女,中文-内地,活泼、温暖的声音，具有多种场景风格和情感 2:云扬,男,中文-内地,专业、流利的声音，具有多种场景风格。 3:云希,男,中文-内地,活泼、阳光的声音，具有丰富的情感，可用于 4:云健,男,中文-内地,适合影视和体育解说， 5:云夏,男,中文-内地,少年年男声， 6:晓伊,女,中文-内地,女声可定制:angry,sad等9种情绪 7:曉佳,女,粤语-香港,曉佳(HiuGaai),粤语女声 8:曉曼,女,粤语-香港,曉曼(HiuMaan),粤语女声 9:雲龍,男,粤语-香港,雲龍(WanLung),粤语男声 10:曉臻,女,中文-台湾,曉臻(HsiaoChen),湾湾女声 11:曉雨,女,中文-台湾,曉雨(HsiaoYu),湾湾女声 12:雲哲,男,中文-台湾,雲哲(YunJhe),湾湾男声 13:Yan,女,英语_香港,Yan,港式英语女声，不支持中文。 14:Sam,男,英语_香港,Sam,港式英语男声，不支持中文。 15:七海,女,日语_日本,七海(Nanami),日语女声 16:圭太,男,日语_日本,圭太(Keita),日语男声 17:선히,女,韩语_韩国,선히(SunHi),韩语女声 18:인준,男,韩语_韩国,인준(InJoon),韩语男声 19:Ana,女,英语_美式,美式英语，女童 20:Aria,女,英语_美式,美式英语，成年女性 21:Jenny,女,英语_美式,美式英语，成年女性 22:Michelle,女,英语_美式,美式英语，成年女性 23:Christopher,男,英语_美式,美式英语，成年男性 24:Eric,男,英语_美式,美式英语，成年男性 25:Guy,男,英语_美式,美式英语，成年男性 2.切换声音  如下图所示，收到回复后，以后大聪明回复你的语音就会使用新的声音  其他说明  语音回复功能的内容也是从chatGPT中获得的 语音回复比纯文字回复，会额外多用5-20秒用时 语音回复在群聊和私聊模式都可以用 另外，经过最近几个星期的测试，私聊功能对一些对隐私比较重视的朋友更友好。 继续开放一些私聊的名额，加大聪明为好友通过以后即可一对一聊天。  加群（推荐）： 加大聪明的微信，进行一对一私聊（不推荐）：  请不要在私聊和群里谈论或诱导机器人谈论敏感话题。
 </description>
    </item>
    
    <item>
      <title>MySQL5.7升级到8.0(二):配置和参数</title>
      <link>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B42%E9%85%8D%E7%BD%AE%E5%92%8C%E5%8F%98%E5%8C%96/</link>
      <pubDate>Thu, 20 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B42%E9%85%8D%E7%BD%AE%E5%92%8C%E5%8F%98%E5%8C%96/</guid>
      <description>研发：MySQL5.7升级到8.0(一):SQL语法变化
  DBA：MySQL5.7升级到8.0(二):配置和参数
   Note：这里面是升级到8.0,需要DBA参与修改部分或注意部分
 1.配置文件.cnf变化 以下参数变化 expire-logs-days =&amp;gt; binlog_expire_logs_seconds # 替换 expire-logs-days tx_isolation =&amp;gt; transaction_isolation tx_read_only =&amp;gt; transaction_read_only innodb_undo_logs =&amp;gt; innodb_rollback_segments have_query_cache = no # 永远为 NO expire-logs-days 后续可能废弃, 使用 binlog_expire_logs_seconds (目前还支持) 以下参数不再支持 innodb_stats_sample_pages innodb_locks_unsafe_for_binlog innodb_file_format innodb_file_format_check innodb_file_format_max innodb_large_prefix ignore_builtin_innodb skip-symbolic-links # 默认即 skip-symbolic-links. sync_frm # 8.0 版本去掉了 .frm 文件, 内置在 ibd 文件中 sql_log_bin # 仅支持会话级别设置 query_cache_xxx # 缓存相关的系统变量 metadata_locks_cache_size metadata_locks_hash_instances date_format datetime_format time_format max_tmp_tables 2.</description>
    </item>
    
    <item>
      <title>Linux服务器如何科学的访问国外的google,git,pypi等站点</title>
      <link>/ops/linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%A6%82%E4%BD%95%E7%A7%91%E5%AD%A6%E7%9A%84%E8%AE%BF%E9%97%AE%E5%9B%BD%E5%A4%96%E7%9A%84googlegitpypi%E7%AD%89%E7%AB%99%E7%82%B9/</link>
      <pubDate>Wed, 19 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%A6%82%E4%BD%95%E7%A7%91%E5%AD%A6%E7%9A%84%E8%AE%BF%E9%97%AE%E5%9B%BD%E5%A4%96%E7%9A%84googlegitpypi%E7%AD%89%E7%AB%99%E7%82%B9/</guid>
      <description>目的  当我们在Linux服务器上配置环境时（需要pip yum git wget go get ）经常会遇到外网资源受限 各种依赖关系非常乱，版本对不上，资源下不了。 国内的镜像站版本不全等等 非常影响工作效率和成果。 此时需要用科学的方式将这些开发资源快速配置好 以下是步骤 安装配置本地v2ray 需要使用时启v2ray,source代理 用完停v2ray,恢复正常环境  下载运行脚本 bash &amp;lt;(curl -L https://raw.githubusercontent.com/v2fly/fhs-install-v2ray/master/install-release.sh)
#!/usr/bin/env bash # shellcheck disable=SC2268 # The files installed by the script conform to the Filesystem Hierarchy Standard: # https://wiki.linuxfoundation.org/lsb/fhs # The URL of the script project is: # https://github.com/v2fly/fhs-install-v2ray # The URL of the script is: # https://raw.githubusercontent.com/v2fly/fhs-install-v2ray/master/install-release.sh # If the script executes incorrectly, go to: # https://github.</description>
    </item>
    
    <item>
      <title>安装MiniGPT-4</title>
      <link>/ops/%E5%AE%89%E8%A3%85minigpt-4/</link>
      <pubDate>Tue, 18 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%AE%89%E8%A3%85minigpt-4/</guid>
      <description>克隆项目 # 安装 git-lfs curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash yum install git-lfs git clone https://github.com/Vision-CAIR/MiniGPT-4.git git clone https://huggingface.co/lmsys/vicuna-13b-delta-v0 mv vicuna-13b-delta-v0 /data/MiniGPT-4/vicuna_weights 安装补丁 cd /data/MiniGPT-4/ cd ckp/ #https://drive.google.com/file/d/1a4zLvaiDBr-36pasffmgpvH5P7CKmpze/view #https://drive.google.com/u/0/uc?id=1a4zLvaiDBr-36pasffmgpvH5P7CKmpze&amp;amp;export=download wget https://doc-0s-08-docs.googleusercontent.com/docs/securesc/dbq4d7c2ha9354fqj5612le3gl157ov5/pcnbepjp7icjk54rrfgio602v738i398/1681803150000/09448752369347632296/09178445193473466964/1a4zLvaiDBr-36pasffmgpvH5P7CKmpze?e=download&amp;amp;ax=ALy03A4aDltm3f1TddEAJesIj7YsB_KKlm56C61-M72rzjuPpluLCR3UOUv28FXMkNc3q5s ## 修改配置 vim /data/MiniGPT-4/eval_configs/minigpt4_eval.yaml ckpt: &#39;/data/MiniGPT-4/ckp/&#39; vim /data/MiniGPT-4/minigpt4/configs/models/minigpt4.yaml llama_model: &amp;quot;/data/MiniGPT-4/vicuna_weights/&amp;quot; 安装依赖 pip3.9 install omegaconf #1681803684 yum install xz-devel -y yum install python-backports-lzma -y pip3.9 install backports.lzma yum install python-backports-lzma -y yum install -y xz-devel mesa-libGL python-backports-lzma vim /usr/local/python3.9/lib/python3.9/lzma.py pip3.9 install iopath pip3.</description>
    </item>
    
    <item>
      <title>Centos7安装nvidia显卡驱动</title>
      <link>/ops/centos7%E5%AE%89%E8%A3%85nvidia%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8/</link>
      <pubDate>Mon, 17 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/centos7%E5%AE%89%E8%A3%85nvidia%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8/</guid>
      <description>1.安装依赖
yum -y install epel-release yum -y install gcc binutils wget yum -y install kernel-devel 2.禁用Nouveau
检查是否开启Nouveau lsmod | grep nouveau 注意：无信息输出表示已被禁用无需在操作以下步骤； echo -e &amp;quot;blacklist nouveau\noptions nouveau modeset=0&amp;quot; &amp;gt; /etc/modprobe.d/blacklist.conf mv /boot/initramfs-$(uname -r).img /boot/initramfs-$(uname -r).img.bak dracut /boot/initramfs-$(uname -r).img $(uname -r) reboot lsmod | grep nouveau 注意：无任何信息输出表示禁用成功； 3.检查驱动
rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm yum -y install https://www.elrepo.org/elrepo-release-7.0-4.el7.elrepo.noarch.rpm yum -y install nvidia-detect nvidia-detect -v # 本来应该显示 This device requires the current.... # 结果显卡太老，显示 Probing for supported NVIDIA devices.</description>
    </item>
    
    <item>
      <title>职业选择的新思考：ChatGPT是否会取代你的职位？</title>
      <link>/ai/chatgpt%E7%9A%84%E5%8F%91%E5%B1%95%E4%BC%9A%E6%9B%BF%E4%BB%A3%E5%93%AA%E4%BA%9B%E8%81%8C%E4%B8%9A/</link>
      <pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/chatgpt%E7%9A%84%E5%8F%91%E5%B1%95%E4%BC%9A%E6%9B%BF%E4%BB%A3%E5%93%AA%E4%BA%9B%E8%81%8C%E4%B8%9A/</guid>
      <description>从AI回答的正确率说起   在微信上建了一个叫大聪明的chatGPT的机器人
  具体看这里: 《我在微信上用chatGPT做了个大聪明》
  中午科长说他经常会跟大聪明聊天沟通想法
  我劝他不要肓信ai的回复
  因为ai的首要目的不是“保真” ，而是“保回复质量”
    像chatGPT之类的大语言模型当然想追求答案的正确性
  但正确性对AI来说还是有点难
  他更像是个博学的学生
  什么都知道一点
  什么都能聊得起来
  但是做不到严格的“正确性”
  虽然给人的感觉是非常聪明
  但那是因为它会得多
  语言组织能力强产生形成的印象
  举例来说
  现在用得最普遍的ChatGPT3.5 和GPT-3.5 的正确率大约是58%
  ChatGPT4的正确率在64%左右，GPT-4的正确率在77%左右
   而人类借助于专业分工，常年的社会网络交流以及借助于资料阅读和工具可以做到（80%-99%)的正确率  现阶段的AI（80%正确率）  现在最顶级的AI(GPT-4)的正确率80% ，接近人类的普通专业人士 这个阶段AI只能是辅助工具 AI现阶段不能代替人类做决定 AI的每一个决定需要人工确认以保证结果是正确的 也就是说AI只是个辅助工具 用来做提升人类的工作效率 但是它的准确性不足以超过人类的信任区域 1.</description>
    </item>
    
    <item>
      <title>MySQL Group Replication(MGR集群)增加节点和迁移节点</title>
      <link>/mysql/mysqlgroupreplicationmgr%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9%E5%92%8C%E8%BF%81%E7%A7%BB%E8%8A%82%E7%82%B9/</link>
      <pubDate>Thu, 06 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysqlgroupreplicationmgr%E9%9B%86%E7%BE%A4%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9%E5%92%8C%E8%BF%81%E7%A7%BB%E8%8A%82%E7%82%B9/</guid>
      <description>MySQL Group Replication 多主结构的3节点需要切到另外三个节点上，任务需要先加3个节点到集群中，再删掉原来的3个节点。
 环境   原实例：
 172.0.2.30:3309 172.0.2.31:3309 172.0.2.32:3309    新实例：
 172.0.2.83:3309 172.0.2.84:3309 172.0.2.85:3309    修改host - 修改6台主机的/etc/hosts 172.0.2.30 dba-mysql3309-230 dba-mysql3309-230.dboop.com 172.0.2.31 dba-mysql3309-231 dba-mysql3309-231.dboop.com 172.0.2.32 dba-mysql3309-232 dba-mysql3309-232.dboop.com 172.0.2.83 dba-mysql3309-83 dba-mysql3309-83.dboop.com 172.0.2.84 dba-mysql3309-84 dba-mysql3309-84.dboop.com 172.0.2.85 dba-mysql3309-85 dba-mysql3309-85.dboop.com 旧实例上增加seed  检查状态  mysqlw -h 172.0.2.30 -P 3309 -e &amp;quot;show global variables like &#39;group_replication_group_seeds&#39;&amp;quot;; mysqlw -h 172.0.2.31 -P 3309 -e &amp;quot;show global variables like &#39;group_replication_group_seeds&#39;&amp;quot;; mysqlw -h 172.</description>
    </item>
    
    <item>
      <title>ChatGPT的胡说八道</title>
      <link>/ai/chatgpt%E7%9A%84%E6%8F%90%E9%97%AE%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Thu, 30 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/chatgpt%E7%9A%84%E6%8F%90%E9%97%AE%E6%8A%80%E5%B7%A7/</guid>
      <description>前段时间我建了一个微信群，里面接入了chatgpt和百度文心一言的机器人 建群的目的有两个  让没有用过chatgpt的朋友体验一下chatGPT的魔力 我看一下大家（尤其是非技术人员）怎么跟chatgpt对话的   目前这个群里大约有200多人，有各行各业的人，技术/行政/法务/管理/学生/自由职业/艺术&amp;hellip; 多数是随着我的文章加进来的 里面很热闹，有很多奇怪的人问了chatgpt奇怪的问题  问题分类   大家在群里跟chatGPT聊天大约分成以下几类：
 打招呼好奇类：一般是在试探chatGPT能干啥 探索类：比上一类要稍懂一些，而且已经有过一定的使用经验。他们可能是在网上或者自己想出来一些奇怪的问题来试探chatGPT 问时事，问新闻：把chatGPT当成一个对话机器人，问当前时事，问股票，问彩票，问各种新闻 解决问题：问当出现什么报错时（一般是技术问题） 应该怎么做？ 写文档：总结，创意，套话&amp;hellip; 调戏chatgpt：不知道从哪学来的奇怪的话术，在那调戏chatgpt    以上这些，只有5是chatGPT擅长的。
   （未完待续)</description>
    </item>
    
    <item>
      <title>ChatGPT向左,百度文心一言向右.md</title>
      <link>/ai/chatgpt%E5%90%91%E5%B7%A6%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E5%90%91%E5%8F%B3/</link>
      <pubDate>Tue, 28 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/chatgpt%E5%90%91%E5%B7%A6%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E5%90%91%E5%8F%B3/</guid>
      <description> “ChatGPT向左,文心一言向右”:同一条赛道上的两个方向，一个深耕技术，一个用心做产品，但是他们都会成功。
 新闻连着看  3月14日，OpenAI发布GPT-4  比震惊世界的chatGPT(3.5)再次升级：创造力提升，视觉输入，更长的上下文，更强的推理能力。   3月16日，百度发布文心一言  现场演示的时候选择直接放录好的视频，而且采用邀请码的方式没有开放试用，被质疑能力不足（事实上也确实如此） 在开放的试用过程中，被用户发现文字理解有点呆（这个是预期内的，肯定会离chatgpt会有不小的距离）以及更为严重的生成图片时疑似用了国外的图片生成程序（现实是肯定用了国外的图源做训练和标记了，是否用的国外开源产品改的，还不确定）   3月24日，OpenAI发布ChatGPT Plugins  官方演示了Web浏览器，代码解释器，检索信息等方面的Demo   3月27日，百度闭门会议悄悄发布AI服务平台“文心千帆”  现场演示了：对话机器人,三分钟做ppt,数字人直播带货,订机票和酒店等操作。    现状 目前都认可的事实：  ChatGPT的技术还是遥遥领先与世界上的其他竞争对手 百度的大语言模型文言一心已经做出来了，效果虽然赶不上ChatGPT，但至少在中文领域还是可以的（我是3月21日才开始试用文心一言的，并在当天将其接入了我的微信机器人群里）  各自的方向：  技术层面 GPT在一步一步的疯狂往前探索，据说GPT-5已经在内部做安全测试了。AIGC技术这块目前是没有对手的 百度还在努力追赶，可能不远的将来能达到chatGPT初代的中文水平（GPT-3.5），一般预测在3-12个月内可以实现 产品层面 OpenAI公司开始做Plugins，也就是说提供给第三方施展的空间，做一个生态圈，走的是用技术挣钱的方向 百度从文心千帆的闭门会议可以看到一个趋势，就是他自己把AIGC周边的产品都做了，准备往卖服务，用产品挣钱的方向走 形象点说 GPT是技术大牛，开放它的能力，众多第三方开发者一起跟着它做产品。大家一起跟着吃肉喝汤。 文心一言的文心千帆则是直接面向了商家客户，来我这里有现成的产品，你用不，收点钱。 做为开发者，肯定支持GPT这种开放策略，事实上早在几个月前，各种周边的GPT衍生功能产品已经满天飞了，各路开发人员都在给它做各种补充功能：生成PPT,生成视频,翻译软件,pdf检索等等，而官方不断的开放和迭代GPT的能力，语音识别能力，图片输入能力等等。形成了比较好的良性互动 但是从百度的角度，技术上肯定追不上ChatGPT了，但是它可以在产品上下功夫，将开源社区众多开发者的主意，用自己的原生力量打磨出来，做出一个又一个实用且好用的商业化产品，也确实是个更好的选择。挣钱嘛，不寒碜。而且  1.国内短时间还没有哪家ai大语言模型技术能追上百度 。 2.挣完钱以后可以反哺技术，加快追赶的脚步。 3.这是一种很取巧但是实用的路子，百度文心千帆演示的产品中技术难度都不大，采用或者说揉合了开源的或国外现成的很多技术 4.这些产品网上的很多个人开发者都可以短时间完成开发比如:ai生成ppt,ai生成数字人,ai生成行程，但百度拥有的资源做出来的产品肯定会更加好用和精艳。    图：文心千帆企业试用用户：
接下来，我们从百度的闭门会议中发布的ai产品中看看它的技术背景
百度的AIGC产品：文心千帆 场景一：企业办公场景：3分钟制作PPT  和金山wps结合，几句话就可以生成一个精美的ppt,亮点是可以从公司网站上提取信息下来 这个技术难度不大，我在今年2月10号左右，也用chatgpt做了个类似的功能，但是实话说生成的ppt非常丑，远不如这个好看 所以该演示难度对百度来说几乎没有，但是功能非常有用 演示的视频如下：  场景二：电商服务场景：快速生成ai视频  一键生成文案，再用ai数字人技术生成直播视频 技术难度：中等，如果百度是完全自研的那么这个技术难度就有了（事实上应该不是） 开源的方案有，可以用微软的edgeTTS生成语音和字幕，用midjourney生成形象,用DID生成视频 有理由怀疑百度直接用了这些现成的方案（这也是各种数字人用的一整套技术），当然内心里我更希望更有出息一点，自己搞一套 演示的视频如下：  场景三：旅游服务场景：生成旅游规划并完成订单 场景四：金融投资场景：归纳业务评估生成投资建议 总结  百度的文心一言，技术上可以做到国内最优，且始终和国外的先进技术存在很大的落差。 百度的文心一言产品会在商业上产生很大的成功。尤其是toB业务 大语言模型AI,会极大的提高很多职业的工作效率，快跟上，别掉队。  </description>
    </item>
    
    <item>
      <title>百度文心一言发布会,未来可期:为什么我对百度充满信心</title>
      <link>/ai/%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E5%8F%91%E5%B8%83%E4%BC%9A%E8%A7%82%E5%90%8E%E6%84%9F%E7%9C%8B%E5%A5%BD%E7%99%BE%E5%BA%A6/</link>
      <pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E7%99%BE%E5%BA%A6%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80%E5%8F%91%E5%B8%83%E4%BC%9A%E8%A7%82%E5%90%8E%E6%84%9F%E7%9C%8B%E5%A5%BD%E7%99%BE%E5%BA%A6/</guid>
      <description>百度文心一言发布会  3月16日14:00 百度文心一言发布会准时开始 当李厂长拿出ppt来演示，所有ai对话用录好的视频来播放时。 百度股票嗖嗖的下来了 因为一个ai产品不敢现场演示，也不敢开放试用。很难让人信服。  不看好的观点  在微博和微信群里看大家的反馈 大多数人都觉得不太看好百度的文心一言。 其核心点就在 1.于李彦宏现场露怯，主动说了不如chatGPT，其实也没人会认为百度能赶上ChatGPT 2.现场没有演示，demo部分直接放视频，这是非常明显的怕翻车 3.采用邀请码的方式，大概率只会在固定的群体里试用，没有对公众开放，也是信心不足的表现  我的观点  发布会没开之前，我在上班的路上想了几条，发布会的事   - 3月16日的发布会不会跳票，架子都搭好了，再不上去，股价还要不要了 - 李厂长将会亲自上台，演示百度all in ai不是说着玩的 - 不会开放给大家试用，需要排队等待试用机会 - 会拆分出来很多个不同的Ai，有写诗的，有谱曲的，有写小作文的，有写代码的，有总结分析的，有翻译的，有生活助手的，也有瞎聊的…怎么也得分成20个吧 - 现场演示将会很炸（预设好的demo，不给力是不可能的） - 比chatgpt更懂中文，无敌的中文语料库 - 比chatgpt更实时，数据是当天的 - 会演示：在线教育，写代码，写诗，法律援助，实时推荐， - 会开放api和训练合作，已经有商务合作的有…..巴拉巴拉，顺手展示合作伙伴的成功案例 - 会有图片和视频方面的处理能力 - 文心一言，文言一心…经常说错的同学不要担心，换个简单好记的名字 - 持续改进中，我们也是：未来可期..  正确率90%吧 核心点是我觉得百度这次是不会开放给大家用的 因为产品肯定没成熟 但这不妨碍百度在【生成式AI】这个赛道上领先 百度也是国内最有机会站在生成式AI舞台的公司。  看好百度的原因  大厂里第一个正式发布对标ChatGPT的产品。还隆重的开了个新闻发布会 发布会的演示里的那些流畅的对话可能是精心准备的，但可以理解百度的技术已经接近演示里的水平了 百度确实有着国内最强的AI技术储备，这么多年的ALL IN AI 也没白做 如李彦宏多次强调的那样，这么多年的搜索积累，在中文语言的处理上，处于独一无二的位置 百度有着无可比拟的中文语料优势：最大的中文搜索引擎/百度百科/知道 因为有先发优势，会有很多友商和百度合作，已经有很多很多厂商宣布在和文心一言内测合作 还有最重要的原因，发布会也提了：国内这块技术的不足导致的饥渴。 国内这块的技术需求在那，急需一个产品，哪怕是个半成品 ChatGPT国内用不了，就算用不了，也还是会担心中美关系恶化后的被卡脖子。以及数据泄露的问题   所以百度的文心一言，不管产品是否能成功，百度都能是最后的赢家。因为它站在不可替代的位置，国内没人挑战文心一言，它就能立于不败之地。  最后  群里有个聪明人，给GPT4提了一个非常搞笑的prompt：  @gpt4 百度也推出了一个LLM叫做文心一言。请你用一种阴阳怪气的语气对此做出评价，总体的思路是挖苦讽刺。要有具体的例子（比如过往失败的产品，提及那些产品的糟糕体验。注意不要直接说那些糟糕体验，而是通过反讽、幽默的类比等方式来说明）。  然后ChatGPT反馈：  你好，这是GPT-4。我听说了百度推出了一个LLM叫做文心一言 。我觉得这是一个很有创意的名字，就像百度的其他产品一样，比如百度云、百度贴吧、百度糯米等等。😊 文心一言是百度对标OpenAI的ChatGPT ，也就是GPT-3.</description>
    </item>
    
    <item>
      <title>GPT-4的新功能以及我基于GPT-4做的微信机器人</title>
      <link>/ai/gpt-4%E7%9A%84%E6%96%B0%E5%8A%9F%E8%83%BD%E4%BB%A5%E5%8F%8A%E6%88%91%E5%9F%BA%E4%BA%8Egpt-4%E5%81%9A%E7%9A%84%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA/</link>
      <pubDate>Wed, 15 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/gpt-4%E7%9A%84%E6%96%B0%E5%8A%9F%E8%83%BD%E4%BB%A5%E5%8F%8A%E6%88%91%E5%9F%BA%E4%BA%8Egpt-4%E5%81%9A%E7%9A%84%E5%BE%AE%E4%BF%A1%E6%9C%BA%E5%99%A8%E4%BA%BA/</guid>
      <description>GPT-4的发布  在震惊科技行业的AI聊天机器人ChatGPT发布近四个月后 OpenAI公司又发布了为ChatGPT提供支持的更为强大的下一代技术版本GPT-4 今天(3月15日)凌晨1点，ChatGPT创始人Sam Altman发布了一篇推特：   正式的发布了GPT-4 Sam用五个形容词评价它：capable（adj.能力强的）、aligned（adj.符合预期的）、flawed（adj.有缺陷的）、limited（adj.能力有限的）、impressive（adj.令人印象深刻的）  GPT-4 的升级  相对于GPT3.5，训练参数量上的差别如下：  它可以生成、编辑和与用户迭代创意和技术写作任务，例如创作歌曲、编写剧本或学习用户的写作风格。 它支持生成和处理多达32,768个token（约25,000个单词）的文本，这使得它能够比以前的模型创建或分析更长的内容。 它在各种专业和学术基准测试中表现出“人类水平”的性能，例如通过模拟律师资格考试、法学院入学考试（LSAT）、研究生入学考试（GRE）量化部分和各种AP科目测试。  据 OpenAI 透露，GPT-4 通过了所有基础考试而且是高分通过。例如，GPT-4 在模拟律师资格考试的成绩在考生中排名前 10% 左右，在 SAT 阅读考试中排名前 7% 左右，在 SAT 数学考试中排名前 11% 左右。相比之下，曾经令人震撼的 GPT-3.5 ，真实得分在倒数 10% 左右，GPT-4 的强大已经可想而知。 下面这张图是GPT-4在各项考试中的表现，碾压GPT-3    它可以处理图像输入，并对场景进行识别和描述。  可以看懂图片，并找到这张图片为什么搞笑：巨大的过时VGA接口给小巧的现代智能手机充电    它可以处理表格、图形和图表等数据，并进行分析和解释。     支持更多的数据类型输入      如何用上GPT-4.  现在ChatGPT plus会员有试用，但暂时还没有开放识图功能，且有条数限制 开发者在此页面申请：https://openai.com/waitlist/gpt-4-api 加入等待列表。收到邮件后就能用     微软bing的chat已经升级到4.</description>
    </item>
    
    <item>
      <title>不要再让时间溜走了，让AI来管理你的时间！</title>
      <link>/dba/%E7%94%A8ai%E5%B8%AE%E4%BD%A0%E5%9B%9E%E7%AD%94%E6%97%B6%E9%97%B4%E9%83%BD%E5%8E%BB%E5%93%AA%E5%84%BF%E4%BA%86%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Thu, 09 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E7%94%A8ai%E5%B8%AE%E4%BD%A0%E5%9B%9E%E7%AD%94%E6%97%B6%E9%97%B4%E9%83%BD%E5%8E%BB%E5%93%AA%E5%84%BF%E4%BA%86%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>问题  如果你不知道工作时间都去哪了 或写工作周报/OKR时不知从何下手 这时候可以每天花5分钟写个工作记录 用现在最流行的AI技术帮你分类一下 或许可以帮助到你  效果  你可以看到最近一段时间的工作时间分配 也可以看到汇总的工作安排 还可以结合考勤表/OKR表进行对比  需要做的工作  每天花5分钟，写几行工作记录 每个工作记录写一行，可以加个时间 0.5h(0.5小时) 起步 可以自己写工作分类项和okr类别，也可以让AI帮你归类（我用的是chatGPT做分类）  AI分类原理  这里用的是chatGPT 将最近的50条已经分好类的工作项，当作Prompt塞给chatGPT 然后要求AI返回这个工作项的分类 prompt如下：  work_prompt=f&amp;quot;&amp;quot;&amp;quot;&amp;quot;工作内容&amp;quot;和[工作分类]的对应关系如下:{contentstr}请在以下分类中:{typestr}为 &amp;quot; %s &amp;quot; 选择一个分类&amp;quot;&amp;quot;&amp;quot;得到的报表  我们说每天的，每个人的工作内容，是无规律的：信息 当我们人为的把它按一定的格式录入下来以后，这些信息收集起来就成了：数据 有了数据，可以用各种维度的展开，对比，这时候可以做：报表 可以有很多种维度，这个月的和上个月的表 可以用A的工作和B的比 最重要的是，它会让你的工作内容变得可回溯  为什么起这个标题？  这个标题也是ai帮我生成的  </description>
    </item>
    
    <item>
      <title>带着chatGPT的NewBing</title>
      <link>/ai/%E5%B8%A6%E7%9D%80chatgpt%E7%9A%84newbing/</link>
      <pubDate>Mon, 27 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E5%B8%A6%E7%9D%80chatgpt%E7%9A%84newbing/</guid>
      <description>NewBing的申请通过了。
应该是现在国内使用chatgpt最方便的方式了</description>
    </item>
    
    <item>
      <title>我在微信上用chatGPT做了个大聪明</title>
      <link>/ai/%E6%88%91%E5%9C%A8%E5%BE%AE%E4%BF%A1%E4%B8%8A%E7%94%A8chatgpt%E5%81%9A%E4%BA%86%E4%B8%AA%E5%A4%A7%E8%81%AA%E6%98%8E/</link>
      <pubDate>Fri, 24 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E6%88%91%E5%9C%A8%E5%BE%AE%E4%BF%A1%E4%B8%8A%E7%94%A8chatgpt%E5%81%9A%E4%BA%86%E4%B8%AA%E5%A4%A7%E8%81%AA%E6%98%8E/</guid>
      <description>update  2023-04-21 支持语音回复和跟读，有25种声音可选 语音的使用说明 2023-04-04 ChatGPT自带的生成图片功能太吓人了，图片上的人脸跟鬼似的（可能是为了版权考虑，把人脸做了特殊处理），看得受不了，图片部分换成了百度的文心一言  大群不再踢人，不再接收新人加入（认识的朋友还是可以加的），另起一个群接受新人加入，新起的群没有实名制要求. 两个群的权重是3:1 的关系，大群里的内容更容易得到更及时的反馈。   2023-04-02 做了以下修改  修改了机器人的回复框架，从底层重写了一版，回复速度更快（原始版本写得太2了） 群里的聊天可以分辨出是具体的人。（这个很重要&amp;hellip;) 修改了提示词，每次都要@大聪明太费劲了。现在以“请”开头的群聊，都会触发大聪明的回复 增加了生成图片的能力，例生成图片:驴肉火烧 生成语音，视频，ppt&amp;hellip;的功能也准备就绪了，炫酷有余，但实用性不佳。先试试图片的反馈。好玩就会继续跟进放开。 减少了一些功能  暂时停用了gpt4,bing,文心一言的功能，用得人少，稳定性不如GPT3.5 停用了私聊功能，不再让大聪明一对一的回复问题（不这样做的原因是，有人在私聊里发消息略微有点频繁，机器人忙着回复私聊信息，群里的消息回得卡顿了）  私聊功能是个很纠结的点，我相信一定有人更愿意在私聊窗口和机器人对话，而不喜欢在群里对话，但是对我来说这个功能就很累赘。会针对几个朋友保留这个功能     前几天在力哥的帮助和推荐下，群里开始要求实名制，甚至把我拉进去的朋友也给踢了。有朋友表示理解，也有朋友私聊我表示很生气。  支持力哥的想法。力哥是个很有想法的人，我也想学习一下他看待新事情的思维和办事方法。  核心点是&amp;quot;如果你的群里的人连改个名字都不愿意配合你，那这个群对他是没有价值的，你留他在群里干啥&amp;quot;   也希望这个群能更好玩，不要有戾气，否则就违背初衷没意思了，拉这个群就是给朋友们感受一下chatgpt的好玩的用法。 要不要多组几个群，喜欢认真的人在一个群里，喜欢瞎扯的人在另一个群里。互相不打扰。  谈正事的群里，会要求实名。 瞎聊天的群里，就各种随意。 区分是正事的群里的消息回复速度更优先一些，假如两个群里同时都有人在和大聪明交谈，那么机器人会更倾向于谈正事群（渣男算法&amp;hellip;）     关于群成本的问题 群里回复消息调用了chatgpt的api，这个确实是收费的，但是费用极低，可以忽略不计 以现在的提问频率，每天大约花费1/3个茶叶蛋的钱，这个还是负担得起的，不要慌&amp;hellip; 真正的成本在于投入时间维护和改造，好在我是个爱玩的人，这件事还挺好玩的，所以：问题不大。   2023-03-29 任何人都可以和大聪明发起一对一私聊了。不需要额外申请。 gpt4的接口报错太多，暂时禁用了，有时间再修复。 实在懒得去维护加群申请，力哥建议由他来管理群成员。 群管理员交给力哥了。 2023-03-23 修复百度文心一言回复不完整的bug ,增加文心一言生成图片功能  对于想和大聪明私聊的同学，可以一对一和大聪明发起对话了。 私聊中不需要再以 @大聪明 开头了，因为是一对一对话。 开通后会一直有效，不需要再次激活，以后任意时间给大聪明发的消息，默认都是机器人回复。 2023-03-23 21:08 此功能试用中，等稳定了，再继续开通。   2023-03-20 群满员了，现在需要加微信由管理员拉进群。 群主每天会在19:00-20:00 集中处理加群邀请  怎么加入群？  扫描这个二维码，加入群 免费使用，没有限制，只有一条，不要讨论或引导机器人讨论敏感话题免得群被封号。  加大聪明的微信，进行一对一私聊（不推荐）： 怎么提问？  把问题写出来，然后 @大聪明。等个几十秒大聪明调用GPT3.</description>
    </item>
    
    <item>
      <title>有趣的数据_ChatGPT的多语种训练数据集</title>
      <link>/ai/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BB%BA%E8%AE%AE%E7%94%A8%E8%8B%B1%E8%AF%AD%E5%92%8Cchatgpt%E6%B2%9F%E9%80%9A/</link>
      <pubDate>Tue, 21 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BB%BA%E8%AE%AE%E7%94%A8%E8%8B%B1%E8%AF%AD%E5%92%8Cchatgpt%E6%B2%9F%E9%80%9A/</guid>
      <description>为什么建议用英语和ChatGPT沟通 为什么chatGPT中文对话时候偶尔会感觉到他在胡编瞎造 为什么chatGPT中文提问专业问题时偶尔会出现英语回复  看openai公布的GPT-3训练数据集的语言占比,中文语料只占总训练量的0.1%
   lang 语言名 训练集 占比     en 英语 181014683608 92.65%   fr 法语 3553061536 1.82%   de 德语 2870869396 1.47%   es 西班牙语 1510070974 0.77%   it 意大利语 1187784217 0.61%   pt 葡萄牙语 1025413869 0.52%   nl 荷兰语 669055061 0.34%   ru 俄语 368157074 0.19%   ro 罗马尼亚语 308182352 0.16%   pl 波兰语 303812362 0.</description>
    </item>
    
    <item>
      <title>有趣的数据_海底光缆</title>
      <link>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E6%B5%B7%E5%BA%95%E5%85%89%E7%BC%86/</link>
      <pubDate>Mon, 20 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%9C%89%E8%B6%A3%E7%9A%84%E6%95%B0%E6%8D%AE_%E6%B5%B7%E5%BA%95%E5%85%89%E7%BC%86/</guid>
      <description>周末刷推，发现有人在说中国的互联网对外连接情况，原文说中国对外连接的海底光缆就那几条，一旦打起架来，断了就成局域网
查了下各方面的数据，总结分享给大家
从图说起 全球的海底光缆的分布图 亚洲的海底光缆的分布图 中国的海底光缆的分布图 以上信息来源于一个国外的网站：https://www.submarinecablemap.com/
一些数据  海底光缆作为当代国际通信的重要手段，承担了90%的国际通信业务，是全球信息通信的主要载体。  海底电缆TimeLine  1850年盎格鲁-法国电报公司开始在英法之间铺设了世界第一条海底电缆，只能发送莫尔斯电报密码。 1888年中国第一条海底电缆是清朝时期台湾首任巡抚刘铭传，在1886年铺设通联台湾全岛以及大陆的水路电线，主要作为发送电报用途 1902年环球海底通信电缆建成。 1987年，中国台湾第一条海底电缆完成，即台湾淡水与日本长崎之间。（已停用） 1988年，中国大陆的第一条海底电缆是在1988年完成的，即福州川石岛与台湾（淡水）之间，长177海里。（已停用）  海底光缆  1988年，在美国与英国、法国之间铺设了越洋的海底光缆（TAT-8）系统，全长6700公里。传输速率为280Mb/s*3对，中继站距离为67公里。这是第一条跨越大西洋的通信海底光缆，标志着海底光缆时代的到来。 1993年12月，第一个在中国登陆的国际海底光缆系统是中国——日本（C-J）海底光缆系统。 1997年11月，我国参与建设的全球海底光缆系统（FLAG）建成并投入运营，这是第一条在我国登陆的洲际光缆系统，  中国大陆地区的海底光缆 由于光缆之间存在重合，所以实际上，中国大陆与Internet的所有通道，就是4个入口11条光缆。
中国大陆的海底光缆登陆地点分别是：
 1、青岛（2条光缆） 2、上海（7条光缆） 3、汕头（5条光缆） 4、福州（1条光缆）  青岛1:Trans-Pacific Express（TPE，跨太平洋直达）  带宽：5.12Tbps 长度：17700km 经过地区：  青岛 上海崇明岛 日本千叶县南房总市（原丸山町） 韩国庆尚南道巨济市 台湾新北市淡水区 美国俄勒冈州尼多拿海滩    青岛2:EAC-C2C  带宽：10.24Tbps 长度：36800km 经过地区：  新加坡樟宜 香港将军澳 中国大陆青岛（后期加建） 台湾八里区 菲律宾Capepisa 南韩泰安郡 日本 志摩市 常陆那珂市阿字浦    上海1：Trans-Pacific Express（TPE，跨太平洋直达）  带宽：5.</description>
    </item>
    
    <item>
      <title>探索chatgpt</title>
      <link>/ai/%E6%8E%A2%E7%B4%A2chatgpt/</link>
      <pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E6%8E%A2%E7%B4%A2chatgpt/</guid>
      <description>  探索一下chatgpt的背景和应用场景
  以下是ppt截图
                                                                                              </description>
    </item>
    
    <item>
      <title>翻译Sam Altman的博客《How To Be Successful》</title>
      <link>/ai/%E7%BF%BB%E8%AF%91samaltman%E7%9A%84%E5%8D%9A%E5%AE%A2howtobesuccessful/</link>
      <pubDate>Sun, 12 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/%E7%BF%BB%E8%AF%91samaltman%E7%9A%84%E5%8D%9A%E5%AE%A2howtobesuccessful/</guid>
      <description>写在前面  这是上周看到的Sam Altman一篇博客. 原文连接是: https://blog.samaltman.com/how-to-be-successful 周末在家尝试翻译一下 别问为啥不用chatGPT翻译 周日下午15:35 开始翻译的,中间因为吃饭.看剧打断了几次 最终翻译完已经是晚上23:50了 最终用时在3个小时左右 所以机器翻译10秒的事. 我自己上手就需要2-3个小时哼哼哧哧的才能勉强完成.  作者简介  Sam Altman,1985年4月22日,出生于美国伊利诺伊州的芝加哥。 8岁时学会了编程。9岁时收到一台电脑作为生日礼物， 2005年，他选择从大学辍学，同好友合作创办了社交媒体公司 2012年，他以4300万美元的价格将其出售。 2011年，任Y Combinator的合伙人，成为世界上最富有的企业家和天使投资人之一。 2015年，与时任特斯拉和SpaceX首席执行官的埃隆·马斯克共同创立了OpenAI。他们成立这家非营利性人工智能公司的目标是——确保人工智能不会消灭人类。 现为Y Combinator 总裁、人工智能实验室OpenAI首席执行官。 美国《商业周刊》最优秀年轻企业家 被媒体称为ChatGPT之父。 他的博客地址是:https://blog.samaltman.com/  &amp;ndash; 以下是正文
《How To Be Successful》怎么才能成功  I’ve observed thousands of founders and thought a lot about what it takes to make a huge amount of money or to create something important. Usually, people start off wanting the former and end up wanting the latter.</description>
    </item>
    
    <item>
      <title>从一张图了解ChatGPT会改变哪些行业(职业)</title>
      <link>/ai/chatgpt%E5%8F%AF%E4%BB%A5%E6%94%B9%E5%8F%98%E5%93%AA%E4%BA%9B%E8%A1%8C%E4%B8%9A/</link>
      <pubDate>Wed, 08 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/chatgpt%E5%8F%AF%E4%BB%A5%E6%94%B9%E5%8F%98%E5%93%AA%E4%BA%9B%E8%A1%8C%E4%B8%9A/</guid>
      <description>从ChatGPT对外投资图说起  当我们讨论ChatGPT将改变哪些行业的时候,可以换个思路想 当前对ChatGPT能力理解最清楚的人是谁 肯定是ChatGPT的管理人员,董事会成员,和投资股东  拿了投资方微软的巨量资金后,ChatGPT将这部分钱用于研发和提升产品之余 也成立了openAI基金,投资了一系列的创业公司   看看ChatGPT投资了哪些公司 !
 从图上可以看到ChatGPT投资的公司,大致为成两类 上游: 可以让自己的ai进化 下游: 可以让自己的ai可以改进和提升的领域. 所以ChatGPT的管理团队认为可以有所作为的行业:  1.教育 2.记录/文书 3.法律    学生 (帮助)  由ChatGPT代写作业，已经成为美国大学的一种现象 调查发现，89%的美国大学生已经在用ChatGPT写作业。这就意味着，ChatGPT已经可以从事初级的乃至更高水平的学术研究。 对大学以下的中小学习题,chatGPT可以解答大量的基础题目,给出正确的答案和解题思路 可以用来做作业,也可以用来当成解题助手,家庭老师 同时因为ChatGPT的代写作业能力太强.相应的老师的反作弊工作也提上日程 所以我们看openAI基金投资了:Milo	家长虚拟助理  教师 (部分替代,挑战)  一方面老师们都开始担心学生使用ChatGPT这一技术作弊 另一方面,也要考虑考虑自己的工作安全,部分教师的工作(帮学生解答习题) 随着不断的进化,ChatGPT“迟早可以作为一名老师轻松地授课了”。 所以我们看openAI基金投资了:Speak	AI英语学习平台  记录/文书  录入员,会议记录类工作会被ai工具更有效率的替代 就像这张微软放出来的demo图中,会议中每个人的发言断点,会议提出来的工作项,提醒项,ai都能很好的完成 所以我们看openAI基金投资了:Mem Labs	记笔记应用   法律类工作：  律师助理和法律助理等法律行业工作人员也是在进行大量的信息消化后，综合他们所学到的知识，然后通过撰写法律摘要或意见使内容易于理解。 这和ai的训练路径是一样的,信息消化和学习,是ai最擅长的部分. 所以我们看openAI基金投资了:Harvey	Al法律顾问 参考这篇文章: https://www.lawnext.com/2022/11/stealth-legal-ai-startup-harvey-raises-5m-in-round-led-by-openai.html   会计类工作：  ChatGPT将会很轻松地把财务人员从银行对账、月末入款提醒、进销项差额提醒、增值税验证等这些枯燥重复、初级的工作中解放出来。 甚至，对于是一些专业的财务报告撰写也会带来翻天覆地的影响。 好处是这不仅极大缓解了会计人员的工作强度，而且其凭借客观、准确和及时的特点也很大程度上加强了会计信息的相关性和可靠性。 所以我们看openAI基金投资了:Kick	会计软件  其他类型工作 技术类工作：程序员们  ChatGPT可以快速生成部分基础代码,意味着一项工作在未来可以用更少的员工完成  媒体类工作：广告、内容创作、技术写作、新闻  ChatGPT可以快速生成部分基础代码,意味着一项工作在未来可以用更少的员工完成  客服人员类工作  智能客服的能力,会在ai能力提升,更多的代替人工客服.</description>
    </item>
    
    <item>
      <title>5分钟让你了解ChatGPT是什么</title>
      <link>/ai/chatgpt%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/</link>
      <pubDate>Tue, 07 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/ai/chatgpt%E7%AE%80%E5%8D%95%E5%85%A5%E9%97%A8/</guid>
      <description>开门见山  这是一个AI聊天机器人,2022年11月30日发布 如果你从来没有试过ChatGPT的对话 请记得要尝试去体验一下: https://chat.openai.com/chat  如果你不会科学上网 可以用这个地址体验: http://www.580top.com/dboop2023 (访问人数太多,临时加了ip限制,每个ip每天最多可提交10次左右的问题.) ​这是我业余时间为不懂技术的朋友做的一个页面 无需vpn,无需账号,直接就可以提问   火遍全网的chatGPT是什么? 专业的介绍  ChatGPT是OpenAI开发的一个大型预训练语言模型。它是GPT-3模型的变体，GPT-3经过训练，可以在对话中生成类似人类的文本响应。ChatGPT 旨在用作聊天机器人，我们可以对其进行微调，以完成各种任务，如回答问题、提供信息或参与对话。与许多使用预定义的响应或规则生成文本的聊天机器人不同，ChatGPT经过了训练，可以根据接收到的输入生成响应，从而生成更自然、更多样化的响应。  通俗的介绍: 你可以理解为它是一个聊天机器人,这个机器人会的东西比在此之前的聊天机器人更丰富一些,表现在:
 它有更强的语言理解能力 (更能听得懂人话) 它有更强的语言组织能力 (说的话更符合人类的预期) 它有强大的创作能力AIGC（利用人工智能技术自动生产内容）  以上三点都比在此之前的AI表现得更好,更丝滑,使整个ChatGPT的效果体验非常的惊人.所以突然爆火
ChatGPT是谁做的? 股权在哪家机构? 简单的说:始于Altman和Musk的10亿美元,目前受益最大的是微软.
 2015年，山姆·阿尔特曼与埃隆·马斯克、彼得泰尔、雷德霍夫曼等大佬在[罗斯伍德桑德希尔酒店]吃了一顿晚饭,决定创建一个新的人工智能实验室。 山姆·阿尔特曼（Sam Altman）: 时任知名初创公司孵化器Y Combinator负责人，现为OpenAI联合创始人兼首席执行官 埃隆·马斯克(Elon Musk): 硅谷钢铁侠,特斯拉,SpaceX &amp;hellip;  他们创立了OpenAI,马斯克和阿尔特曼立志用10亿美元的初始资金，打造对人类友好的人工智能，以非营利组织为主体，定期向公众开放AI研究成果和专利。 OpenAI研发的GPT-1、GPT-2模型均对外开源，向外部开发者共享代码和数据。 2018年，马斯克以理念不和为由宣布退出OpenAI，顺带把一些相关研发人员挖去了特斯拉。OpenAI一度被外界调侃成“特斯拉的AI技术人才输送站”。 OpenAI宣布重组，由非营利性的母公司OpenAI Inc和营利性的子公司OpenAI LP组成。至此，OpenAI也从非营利实验室转型为“利润上限”公司，这为投资者和大型科技公司的投资打开了通道，他们的回报上限为投资的 100 倍。 资本开始登场,2019年7月，微软宣布以10亿美元入资OpenAI。一个重要的前提是，微软有权将OpenAI的部分技术商业化，同时，双方达成一项多年的合作协议 2020年，GPT-3完成迭代出现了商业化用例，同年9月，微软宣布获得GPT-3模型的独家授权,其后微软多次追加投资。 2022年11月30日ChatGPT聊天机器人上线。发布仅一周的时间，就已经拥有超100万用户。发布2个月后活跃用户过亿  OpenAI的股权投资协议模式  OpenAI选择了—种新的股权投资协议模式。未来盈利后的OpenAI的利润分配将按照以下四个阶段进行。 第—阶段将优先保证埃隆马斯克、 彼得泰尔、 雷德霍夫曼等首批 投资者收回初始资本； 在第二阶段，微软将有权获得OpenAl75%的利润，直至收回其130亿美元投资； 第三阶段，在OpenAI的利润达到920亿美元后，微软在该公司 的持股比例将下降到49%, 剩余49%的利润由其他风险投资者和OpenAI的员工分享。 第四阶段，在利润达到1,500亿美元后，微软和其他风险投资者的股份将无偿转让给OpenAI的非营利基金。  国内的chatGPT能力:百度占优  chatGPT的火爆上线后,国内同类型产品的研发就已经在开展.</description>
    </item>
    
    <item>
      <title>自动化流程:数据找回(二:Oracle部分)</title>
      <link>/dba/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B_%E6%95%B0%E6%8D%AE%E6%89%BE%E5%9B%9E%E9%97%AA%E5%9B%9E_2/</link>
      <pubDate>Fri, 03 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B_%E6%95%B0%E6%8D%AE%E6%89%BE%E5%9B%9E%E9%97%AA%E5%9B%9E_2/</guid>
      <description>续上一篇： 自动化流程:数据找回(一:MySQL数据闪回)
上一篇说的是
 1.数据找回的作用 2.MySQL数据找回自动化流程功能演示 3.自动化流程的代码配置  这篇主要介绍Oracle的数据找回自动化流程.
一些改进  改进1:Oracle数据找回功能从利用OGG改到用归档日志  上一篇的结尾,说Oracle的数据找回.要用到OGG+Kafka的方案. ​最终经过对比验证,换了更好的Oracle原生归档日志方案   改进2:增加了操作类型的筛选项,这样可以针对指定的操作(例如:delete)进行redo和undo  Oracle数据回退功能的方案对比 当尝试做Oracle数据回退功能时有以下几个方案
 Oracle数据闪回功能 ,优:自带的功能,速度快,简单 缺:需要开启闪回段,闪回的时间太短(平均约30分钟内) 全备和日志还原,优:30天内数据可回退 ,缺:慢,需要资源大,复杂不利于自动化脚本 OGG,因为我们的环境大部分重点表都做了OGG到kafka的订阅, 所以可以直接扫描kafka的信息来获取指定表的变更, 优:资源占用少,快,开发难度低 缺: 如果要回退的这张表没有做OGG订阅怎么办? 归档日志解析, 优:7天内任意时间点可回退 ,缺: 每次回退的时间段不能太长  我们完成了方案3和方案4的代码开发工作,最终选用了方案4: 归档日志解析
 脚本化实现难度小 7天内任意时间点,能满足90%的业务需求 回滚速度快,举个例子:  研发提交流程,要求看A数据库里Order表在周三上午10:00-10:30的所有修改动作,提交申请后,大约10秒内就可以完成这个流程.   覆盖的范围大,线上/测试/开发 环境的所有库表都支持 占用资源少,对线上影响小  Oracle数据回退功能演示  在测试环境还原一次全库的历史快照只需要 0.4秒,非常的快速!!!  小结 当数据库有了这个功能以后:
 研发可以在改错数据的时候,快速提交流程回退数据 当有奇怪的线上数据问题时,可以很方便的生成redoSQL,重放过去某个时间段的数据修改 数据库有大量写入时,快速的定位是哪张表,什么样的SQL引起的  </description>
    </item>
    
    <item>
      <title>自动化流程:数据找回(一:MySQL数据闪回)</title>
      <link>/dba/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B_%E6%95%B0%E6%8D%AE%E6%89%BE%E5%9B%9E%E9%97%AA%E5%9B%9E/</link>
      <pubDate>Mon, 30 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%81%E7%A8%8B_%E6%95%B0%E6%8D%AE%E6%89%BE%E5%9B%9E%E9%97%AA%E5%9B%9E/</guid>
      <description>功能介绍 数据找回,又名数据闪回,误删除回滚,误操作回滚,数据重做&amp;hellip;
解决的问题  数据库里的数据被误删除/更新,需要定位修改的SQL语句,影响范围,回滚误操作,此时怎么办?
  方法1: DBA用前一天的数据库备份还原出来,然后找到更新前的SQL(这个通常需要花费几个小时的时间) 方法2: 如果发现误操作的时间比较及时,一般7天以内,可以通过解析数据库日志(MySQL的binlog,Oracle的archivelog)比较快速的找到误操作的SQL及时回滚  两个方法,都需要DBA参与
   方法 依赖文件 可恢复范围 操作用时     方法一 全备 1个月内 1-5小时   方法二 日志 7天 0.5小时    其中方法2,需要DBA找到并登录目标服务器,解析数据库日志,其中有研发和DBA的沟通时间,手动处理时间,我们可以通过自动化流程,让研发人员自助填写需求,在dba审批通过后系统自动执行.
自动化流程的作用  1.提高工作效率,不再需要DBA手动去查日志,解析日志&amp;hellip; 将原先几十分钟的工作,变成自动化 2.减少误操作,减少手动处理问题时的手滑,操作命令写错了的运维风险 3.减少沟通成本,自助化服务无需(dba和研发)反复沟通细节 4.一般发现有数据更新错了,需要很快的得到响应,自助化流程可以减少非工作时间联系DBA的响应时长 5.便于统计哪些项目成员或业务线出现这种误更新的失误.  功能预览 设计了以下的表单填写页面,用于收集信息 设计了两步审批  负责人审批: 如果该库有具体的DB负责人,则DB负责人审批.否则由提交申请的用户上级审批 DBA审批:DBA审批时会看到将要自动化执行的命令,并且可以在执行前修改部分参数.(通常默认的命令是最优的)  后端执行  执行是异步的.执行过程中会展示运行日志 执行完成后,会展示以下三块内容 1.后台运行日志 (用于DBA排查自动化任务时是否有异常) 2.SQL内容概览 (展示符合条件的表共有多少次ins/upd/del操作. 用于验证是否符合预期,该概览目前只支持MySQL,Oracle的还没实现) 3.解析出来的SQL文件 (最重要的部分)  代码实现 step1:表单代码 &amp;lt;step id=&amp;quot;1&amp;quot; name=&amp;quot;数据找回&amp;quot; displaytype=&amp;quot;input&amp;quot;&amp;gt;&amp;lt;form&amp;gt; &amp;lt;input name=&amp;quot;dbid&amp;quot; title=&amp;quot;选择数据库&amp;quot; inputtype=&amp;quot;select_group&amp;quot; desc=&amp;quot;&amp;quot; defaultvalue=&amp;quot;&amp;quot; &amp;gt;&amp;lt;style&amp;gt;&amp;lt;div-class&amp;gt;span24&amp;lt;/div-class&amp;gt;&amp;lt;input-class&amp;gt;input-normal,input-small,input-normal,input-normal&amp;lt;/input-class&amp;gt; &amp;lt;connstr&amp;gt;link:db_monitor&amp;lt;/connstr&amp;gt;&amp;lt;sqlstr&amp;gt;select b.</description>
    </item>
    
    <item>
      <title>2022年终总结:又是一年岁末时</title>
      <link>/book/2022%E5%B9%B4%E6%80%BB%E7%BB%93%E5%8F%88%E6%98%AF%E4%B8%80%E5%B9%B4%E5%B2%81%E6%9C%AB%E6%97%B6/</link>
      <pubDate>Wed, 18 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/book/2022%E5%B9%B4%E6%80%BB%E7%BB%93%E5%8F%88%E6%98%AF%E4%B8%80%E5%B9%B4%E5%B2%81%E6%9C%AB%E6%97%B6/</guid>
      <description>又是一年岁末时  农历腊月二十七 晚上18:30 明天这个时候就放春节假了 过节的氛围已经有了 本想着今天来摸会鱼 写个年度总结 结果白天被审计追着要材料 还帮同事找bug 鱼没摸成(可惜了,可惜了&amp;hellip;) 趁着晚高峰这会 就随便絮叨两句 聊聊这一年生活和工作上的事  生活  这一年过得很好 比起去年的郁郁葱葱 今年有阳光有彩虹 回忆里翻起来也满是乐呵呵的味道 当然也有很落寞的时候 很快就翻篇了 翻过去就又是阳光灿烂的一天 去了很多地方 看到了很多风景 遇到一些非常敞亮的人 会让人眼前一亮 会让人发自内心的开心 会让我觉得自己捡到了宝一样的感觉 会感叹为啥没早点遇上 人和人之间的差异真的大得吓人 因为我的性格里多少带着点盲目的自信 其实更喜欢和比我聪明的人相处 喜欢那种智商被压制后 需要自我反省的时刻 这是不是有自虐倾向 所以我不光自律,还会自虐  自律给我自由  说起自律 今年我的体重控制得很好 70KG上下来回波动 没有让自己胖起来 除掉和朋友一起吃饭时 私下里还是坚持比较健康清淡的饮食 这一年我吃了成吨的麦片 坚持锻炼 我的keep里的数据挺扎实的 但重要的还是 这一年我爬了很多的山  音乐:爱上摇滚  疫情的原因 今年开车上班的次数多了很多 路上没事就听歌 从年初大张伟的《穷开心》 再到杨紫琼的《爱似流星》 再到齐秦的《夜夜夜》 最后到郑钧的《私奔》《热爱》 年度歌曲是《私奔》 第一次听就喜欢的节奏 懒洋洋的调子 这算是摇滚吗? 不知道 但这歌难度太大 我是唱不上的 只能跟着哼 郑钧应该是好多年好多年前的歌手了 今年才进入我的视野 靠的就是这首《私奔》 全年这一首歌就听了200多次 后来看到郑钧和小齐参加了湖南台的综艺节目 不错,不错 都是我喜欢的歌手  影视:没啥好片子  都想不起来今年有哪个好看的电影了 能记起来的是一部老电影 《午夜巴黎》 最后雨中漫步的那一幕 莫名的触动 说不上为什么 就是突然的上头 其了的影视作品 也许在当时看着觉得不错 但是现在回头看看 留下很深印象的几乎没有 噢,不对不对 想起来一个 易烊千玺的《奇迹·笨小孩》 当时在电影院看到追着卡车跑 妹妹追上来时 那种不顾危险 互相抱怨和安慰时 那一刻还是挺感动的 算是非常不错的一个电影了  游戏:翻篇了  算是个比较爱玩游戏的人了 这一年算是真正戒了 以前爱玩的游戏《王者荣耀》 从今年4月1日以后 就再也没点开过 认真的 远离了那个江湖 风风雨雨都已是过往 伤心了 退了 不回去了 夏天的时候 微信上流行一种小游戏《羊了个羊》 因为变态的难度 好奇的玩了几把 不多 很快就发现了规律 嗖嗖的打了下卡 也就过去了 谈不上沉迷 再后来疫情封控 憋在家里无聊 买了个switch 在家玩《健身环大冒险》 虽然说它也是个游戏 但我完全把它当成健身来用了 放下游戏后 有更多的空闲时间 可以出去走走 挺好的改变 保持下去  运动:像个“爬山虎”  疫情原因不能出远门 于是我把北京周边的山翻了一个又一个 其中最喜欢的凤凰岭去了3次 事件的起因是 2022年4月5号开始 那天早上6点多去了第一个山:虎峪 严格的说 那不是个正常的山 但是山脚下的桃花还是很好看的 留下了不错的记忆 后面的周末就经常去爬山 最喜欢的是凤凰岭 去了三次(夏天,秋天,冬天) 每次去走不同的路线 秀丽的山景 馒头一样的大石头 京西小黄山并不虚名 然后发现凤凰岭可以办年卡 已经把2023年的年卡买好了:) 然后还有千灵山的仙风道骨 石林峡的玻璃栈道 都留下了非常好的回忆 今年 我是个名符其实的爬山虎 挺好的变化 会继续坚持  工作  比起生活上的风生水起 今年在工作上感到了明显的不顺利 有客观的原因 也有自己主观的懈怠  客观原因:经济不景气  疫情的原因公司的业务不好 公司业务不好的时候 压缩成本 节约人员开支 这是门面上的 会精简一些人员 会减少员工福利(包括薪水) 这是面上的减少大家的工作积极性 过去的标准是 优秀的员工 努力干活和提升的员工 肯定会有不断的升职加薪机会 胡萝卜就放在那 问你卷不 今年别说胡萝卡了 你搁那蹲着都嫌碍事 互相着磨着裁员和被裁员 失去了那种拼命干活,拼命挣钱的氛围 希望来年经济大环境能有所改变 大家都忙起来  客观原因:技术重要性下降  因为经济环境的恶化 现在的企业 至少民营企业 核心问题是活下去 而不再是做大做强的扩张 这时候技术的重要性就下降了 高并发,高性能,高稳定性&amp;hellip; 这些技术难题 比起活下去 不值一提 重要的是为了提升业务营收能力 用户体验和系统稳定性 重要但不再是核心竞争力 打个比方: 有个技术改进方案 故障处理时间从1个小时缩到5分钟 还有个技术改进方案 故障处理时间从5分钟缩短到1秒 第2个方案 在经济不景气的时候 就显得不那么重要了 事实上方案会带来技术难度更大的挑战 通常也会意味着更多的人力和技术投入 在行业上行的时候 追求的高并发,高性能 这些技术含量“相对较高”的工作 确实不那么重要了 经济下行的时候,技术重要性降低 这对技术路线的人:比如说我 是个很大的打击 空有一身武艺 准备去华山论剑的 甚至觉得自己可以抢一本《女阴正经》回来的 到了2022年才发现 只需要站在门口检查核酸 而对面大厦的保安 他也会查核酸 他比你的工资要少得多 差不多就是这样的感觉  主观原因:带团队能力差  今年是真切的感受到了带团队的难处 如果你的团队里刚好有不和谐的因素 过往的经历 让我觉得团队里最重要的是和谐 然而今年发生了一件非常有意思的事 当我们复盘那段“离职事件“时 一旁的同事说 你说的这个不是刺头 这是搅屎棍啊 给我的教训是 一方面认识到自己对团队的情绪管理差 另一方面是认识到自己的心软 没有狠下心来 在发现问题的时候及时止损 相比于钻研技术 带团队可太麻烦了 一点也没意思  主观原因:自己的懈怠  这一年工作上确实懈怠了 几个点 从去年11月开始 写工作日记的习惯就放下了 这个从2017年6月坚持下来的习惯 这一年就这么懈怠了 有时候写周报的时候(这个是任务,不是主动的) 会在那憋.</description>
    </item>
    
    <item>
      <title>HDD、SSD、SAS、SATA、PCIE、NVME</title>
      <link>/dba/hddssdsassatapcienvme/</link>
      <pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/dba/hddssdsassatapcienvme/</guid>
      <description>我的习惯   先说当我接到采购需求的时候,我通常会跟运维同事说我要选购下面的磁盘
  pcie ssd 偶尔会强调要更高性能接口的ssd
  ssd盘 大部分时候数据库都需要ssd
  sas盘 这时候的意思是采购sas接口的硬盘,一般用来存放日志类的数据
  sata盘 这是用来存放数据冷备份时候才用的盘
  这是我的习惯事实上提法非常不标准,但是好像负责采购的和运维的同事也都能听得懂.事实上服务器硬盘
  按存储介质分类:HDD盘,SSD盘
  按接口标准分类:IDE,SATA,SAS,PCIE
   下面的内容从多个网上的文章里整理而来,加了些个人看法
 硬盘接口 IDE接口 (已淘汰)  IDE接口，又叫ATA接口、PATA接口、并口。 最早是在1986年由康柏、西部数据等几家公司共同开发。 数据线长得是一条像布条的东西，传输数据慢， 由三部分组成：电源接口、跳线接口（用于区别主盘和从盘）、数据接口。  SCSI接口 (淘汰)  早此年IDE接口应用于PC，对应的服务器的接口是SCSI接口。 SCSI1:最早于1986年提出的,最大传输速率为5MB/s，支持7个设备。 SCSI2:Fast SCSI,1994年,10MB/s（10MHz,最大7个设备)。 SCSI2:Wide SCSI,1996年,20MB/s (10MHz,最大15个设备)。 SCSI3:1995年将总线频率大大地提高，并降低信号的干扰。  SATA接口 (还有不少)  SATA接口,又叫串口硬盘 2003年出现的，可以算是最为主流的硬盘接口形态。由于存在时间很长，SATA接口兼容性极强，几乎所有种类的主板都有SATA接口。 市面上固态硬盘SATA接口在性能标准上，一般采用SATA Ⅲ标准，理论最高速度为6Gbps。 大部分基于SATA接口的固态硬盘的读取性能正常会在500MB/S以上。 SATA数据接口（7针）电源接口（15针） 在SATA接口的基础上，后面又衍生出了两款产品。  SATA接口衍生:mSATA接口 (已淘汰)  mSATA接口是早期为了适应于超极本这类超薄设备而基于SATA开发的。可以看作SATA接口的mini版。 物理形态上有两种尺寸：全高（30mm50mm）和半高（30mm25mm）。  SATA接口衍生:SATA-e接口 (已淘汰)  SATA + PCI-Express的混合体，理论带宽达10Gbps，比SATA3.</description>
    </item>
    
    <item>
      <title>将没有parentid的二维表转换成json的树状结构(python版)</title>
      <link>/ops/%E5%B0%86%E6%B2%A1%E6%9C%89parentid%E7%9A%84%E4%BA%8C%E7%BB%B4%E8%A1%A8%E8%BD%AC%E6%8D%A2%E6%88%90json%E7%9A%84%E6%A0%91%E7%8A%B6%E7%BB%93%E6%9E%84python%E7%89%88/</link>
      <pubDate>Fri, 06 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B0%86%E6%B2%A1%E6%9C%89parentid%E7%9A%84%E4%BA%8C%E7%BB%B4%E8%A1%A8%E8%BD%AC%E6%8D%A2%E6%88%90json%E7%9A%84%E6%A0%91%E7%8A%B6%E7%BB%93%E6%9E%84python%E7%89%88/</guid>
      <description>需求  数据库里有这样的二维表  (id,country,province,city) (1,&amp;quot;a&amp;quot;,&amp;quot;aa&amp;quot;,&amp;quot;aa1&amp;quot;), (&amp;quot;2&amp;quot;,&amp;quot;a&amp;quot;,&amp;quot;aa&amp;quot;,&amp;quot;aa2&amp;quot;), (&amp;quot;3&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;bb&amp;quot;,&amp;quot;bb1&amp;quot;), (&amp;quot;4&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;bb&amp;quot;,&amp;quot;bb2&amp;quot;), (&amp;quot;5&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;bb&amp;quot;,&amp;quot;bb3&amp;quot;), (&amp;quot;6&amp;quot;,&amp;quot;c&amp;quot;,&amp;quot;cc&amp;quot;,&amp;quot;cc1&amp;quot;), 转换成在json中可用的树状结构
[{ &amp;quot;id&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;a|aa&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;a&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;aa&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: 1, &amp;quot;parent_id&amp;quot;: &amp;quot;a|aa&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;aa1&amp;quot;, &amp;quot;leaf&amp;quot;: true }, { &amp;quot;id&amp;quot;: &amp;quot;2&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;a|aa&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;aa2&amp;quot;, &amp;quot;leaf&amp;quot;: true }] }] }, { &amp;quot;id&amp;quot;: &amp;quot;b&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;b&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;b|bb&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;b&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;bb&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;3&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;b|bb&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;bb1&amp;quot;, &amp;quot;leaf&amp;quot;: true }, { &amp;quot;id&amp;quot;: &amp;quot;4&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;b|bb&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;bb2&amp;quot;, &amp;quot;leaf&amp;quot;: true }, { &amp;quot;id&amp;quot;: &amp;quot;5&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;b|bb&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;bb3&amp;quot;, &amp;quot;leaf&amp;quot;: true }] }] }, { &amp;quot;id&amp;quot;: &amp;quot;c&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;c&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;c|cc&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;c&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;cc&amp;quot;, &amp;quot;leaf&amp;quot;: false, &amp;quot;children&amp;quot;: [{ &amp;quot;id&amp;quot;: &amp;quot;6&amp;quot;, &amp;quot;parent_id&amp;quot;: &amp;quot;c|cc&amp;quot;, &amp;quot;text&amp;quot;: &amp;quot;cc1&amp;quot;, &amp;quot;leaf&amp;quot;: true }] }] }]  本来以为很好写的一小段,写起来发现还挺麻烦的  难点  二维表转json tree 还是比较常见的写法,但是这个二维表里没有parentid,所以上下级关系需要用country,province 两列来对齐生成  代码  我写了一个python版的实现   class jsontree_str_(): def __init__(self) -&amp;gt; None: pass def get_jsonstr_parentid(self,rows,columns): sb_rows=[] columnsi=len(columns) if len(rows)==0 or columnsi&amp;lt;3: return sb_rows dict_ids={} for row in rows: for i in range(1,columnsi): idstr=&amp;quot;|&amp;quot;.</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(四):BinLog</title>
      <link>/mysql/mysql%E7%9A%84binlog%E6%97%A5%E5%BF%97/</link>
      <pubDate>Tue, 27 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84binlog%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续：
 MySQL的7种日志(一):概况 MySQL的7种日志(二):RedoLog MySQL的7种日志(三):UndoLog MySQL的7种日志(四):BinLog  1.什么是binlog  又名:MySQL归档日志,MySQL二进制日志 记录所有数据库表结构变更（DDL例如CREATE、ALTER TABLE…）以及表数据修改（DMLINSERT、UPDATE、DELETE …）的所有操作。 默认情况下，二进制日志并不是在每次写的时候同步到磁盘。因此，当数据库所在地操作系统发生宕机时，可能会有最后一部分数据没有写入二进制日志文件中，这会给恢复和复制带来问题。  2.binlog的作用  时间点的恢复：某些数据的恢复需要二进制日志，例如，在一个数据库全备文件恢复后，用户可通过二进制日志进行即时点（point-in-time）恢复。 主从复制：通过复制和执行二进制日志使一台远程的 Mysql 数据库（一般称为 slave）与一台 MySQL 数据库（一般称为 master）进行实时同步。 变更审计：用户可以通过二进制日志中的信息来进行审计，回溯是否对数据库的修改。 误操作回滚：当误修改(ins/upd/del)发生时,可以用binlog解析出修改前后的语句,用于快速回滚 异构数据同步：通过解析binlog,可以将MySQL的变更通知到异构数据源(kafka,es,mongo,redis,mq&amp;hellip;) 事务存储引擎的崩溃恢复。MySQL采用事务的两阶段提交协议。当 MySQL 系统发生崩溃时，事务在存储引擎内部的状态可能为 prepared 和 commit 两种。对于 prepared 状态的事务，是进行提交操作还是进行回滚操作，这时需要参考 binlog：如果事务在 binlog 中存在，那么将其提交；如果不在 binlog 中存在，那么将其回滚，这样就保证了数据在主库和从库之间的一致性。  3.binlog 和 redolog 区别  适用对象不同： binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用 redolog 是 InnoDB 引擎特有的 写入内容不同：  binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：语句 ROW：记录行数据最终被修改成什么样了 MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redolog 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；   写入方式不同： binlog 是可以追加写入的。“追加写” 是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志 redolog 是循环写的，空间固定会被用完 作用不同  4.</description>
    </item>
    
    <item>
      <title>mongodb性能监控指标详细解释</title>
      <link>/dba/mongodb%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A/</link>
      <pubDate>Thu, 08 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/mongodb%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A/</guid>
      <description>常用监控项及说明 当我们监控mongodb实例时，大约有300多项的监控指标，通常我们可以关注以下的20项指标就够了
   监控项 说明     mongodb_memory 内存占用（MiB）   mongodb_mongod_op_latencies_latency_total 累计操作耗时（毫秒）   mongodb_mongod_op_latencies_ops_total 累计操作次数   mongodb_op_counters_total 累计接收的操作请求次数（即使操作不成功也会增加）   mongodb_connections 连接数   mongodb_mongod_metrics_cursor_open 打开游标数量   mongodb_mongod_metrics_document_total 累计文档操作次数   mongodb_mongod_global_lock_current_queue 当前排队等待获取锁的操作个数   mongodb_mongod_metrics_query_executor_total 查询和查询计划评估过程扫描的（索引或文档）条目总数   mongodb_asserts_total 累计断言错误次数   mongodb_mongod_metrics_get_last_error_wtime_num_total 累计getLastError操作数量   mongodb_mongod_wiredtiger_cache_bytes 当前缓存数据大小（byte）   mongodb_mongod_wiredtiger_cache_bytes_total 写入或读取的缓存数据大小（byte）   mongodb_mongod_wiredtiger_cache_pages 当前缓存页数量   mongodb_mongod_wiredtiger_cache_evicted_total 累计缓存移除页数量   mongodb_extra_info_page_faults_total 累计缺页中断次数   mongodb_ss_network_bytesOut 累计发送网络流量（byte）   mongodb_ss_network_bytesIn 累计接收网络流量（byte）   mongodb_mongod_replset_member_replication_lag 副本集成员主从延迟（秒）    mongodb_memory  mongodb_memory 指标表示 MongoDB 数据库实例使用的内存量。这个指标可以帮助监控系统管理员查看 MongoDB 的内存使用情况，并对系统的内存进行优化。 resident 和virtual resident 指的是进程在物理内存中占用的空间，即进程实际使用的物理内存。 virtual 指的是进程在虚拟内存中占用的空间，即进程所占用的总内存，包括物理内存和交换空间。 通常来说，如果 resident 值很大，说明进程实际使用的物理内存很多，这可能表示系统的内存不足，或者进程的内存使用不合理。如果 virtual 值很大，说明进程占用的总内存很多，这可能表示进程在使用较多的交换空间，或者系统的总内存不足。 总之，resident 和 virtual 指标可以帮助你了解进程对内存的使用情况，从而为进行性能优化提供重要的参考信息。 获取代码：  例：mongodb_memory{job=&amp;quot;mongodb&amp;quot;, service=&amp;quot;mongodb&amp;quot;, team=&amp;quot;dba&amp;quot;, type=&amp;quot;resident&amp;quot;}34957mongodb_memory{job=&amp;quot;mongodb&amp;quot;, service=&amp;quot;mongodb&amp;quot;, team=&amp;quot;dba&amp;quot;, type=&amp;quot;virtual&amp;quot;}49537mongodb_mongod_op_latencies_latency_total   mongodb_mongod_op_latencies_latency_total 指标中的 read、write、command、transactions 分别表示 MongoDB 数据库实例中的不同操作类型的平均响应时间。</description>
    </item>
    
    <item>
      <title>《超越期待》读书笔记</title>
      <link>/book/%E8%B6%85%E8%B6%8A%E6%9C%9F%E5%BE%85%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Thu, 24 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%B6%85%E8%B6%8A%E6%9C%9F%E5%BE%85%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</guid>
      <description>所谓商人就是做生意的人，我把自己当作从商资本，从十几岁开始就一直在经营自己。做生意，要先从让对方了解自己开始。学会建立自我品牌。 为对方提供超越期待的东西，让更多人开心、感动，才能收获惊喜。 &amp;mdash;-《超越期待》【日】松浦弥太郎 著，人民邮电出版社
 看闲书  2022年11月24日18:30 感恩节 在盒马上订了一只烧鸡和火锅显示还在配送中 窝在沙发上看了本日本人写的闲书（非计算机技术的书我都叫它闲书） 《超越期待》作者是一个成功的日本商人松浦弥太郎 用半自传体的方式写了作者对待人生和生意的一些经营原则 很久没看非技术类的书了 看进去了就写个读书笔记  01.人生就是一场生意  人生就是一场生意，生意的本质就是看清自己，尽可能地帮助他人。 事实上，“利他”的背后其实是利己，有舍才有得。 有人认为“利他”是心灵鸡汤，实际上这是人生另一种心界，无私的心界。 超越期待这4个字，不仅仅是让他人感受到我们可以提供给对方惊喜的东西， 更重要的一点是，我们先要超越自己、先要让自己走出人生的困惑。 只有这样，我们才能带给他人足够的价值，才能为他人提供超越期待的惊喜。 慢慢积累个人品牌，从赋能他人做起吧。  02.带着经营者意识  哪怕只是公司普通职员经营者意识也很重要 经营者意识和公司职员看待事物的角度完全不同 在时间的利用上 经营者意识很少去想如何能让自己轻松一点 更多的是想着怎样将潜力发挥更大 并积极采取行动 在同事关系上 经营者就算是工作中有不满 对同事有抱怨 也要思考如何顺利的把事情完成 减少对人表达负面情绪 经营者意识不是以公司为家 工作的意义不止是个人的成长 工作的意义还在于这份工作对公司的贡献和价值 学会用经营者的角度去看问题 “利他”的同时让自己“物尽其用”成长  03.不发言即是风险  在一些会议上或类似的场合 有些人喜欢一言不发或者说些无关紧要的话 没有人会去批评不发言的人 很多时候人们会意识不到这样会有负面评价 不发言 不敢表达自己的观点的原因 可能是觉得自己的观点不重要 或是担心自己轻浮的发言带来负面印象 这时候不发言或附合是最保险的策略 但是这样的结果 1.失去了表达自己观点的机会 2.失去了交流沟通的机会（越是陌生多的场合，这种交流越重要） 3.失去了别人了解你的机会 在很多的人生教育里 都告诉我们沉默不语是件好事 它代表的是知道一切但是不炫熠 这更适合的是生活 但在需要更多主动沟通交流的工作场所 并不是一件非常好的方式 在陌生的场合勇敢的表达自己的观点 坦诚的讲出来 哪怕你还没有整理好思路也不要怕  04.</description>
    </item>
    
    <item>
      <title>Redis的缓存一致性/缓存溢出/缓存雪崩/缓存穿透/缓存击穿</title>
      <link>/dba/redis%E7%BC%93%E5%AD%98-%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E6%BA%A2%E5%87%BA%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F%E5%87%BB%E7%A9%BF/</link>
      <pubDate>Fri, 11 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/redis%E7%BC%93%E5%AD%98-%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7%E6%BA%A2%E5%87%BA%E9%9B%AA%E5%B4%A9%E7%A9%BF%E9%80%8F%E5%87%BB%E7%A9%BF/</guid>
      <description>尝试用一个例子来描述高并发系统下的缓存设计，一边举例子一边描述和解决以下问题。
 为什么要用缓存? 缓存一致性问题? 缓存溢出问题? 缓存雪崩问题? 缓存穿透问题? 缓存击穿问题?  问题：  假设我们有5000万条商品信息存储在数据库中，现在这些信息要展示给用户看，我们需要做什么？
 答案1:直连数据库  代码中直接访问数据库，读取数据，展示给用户看，这个方法可以吗？ 答案是：访问量少的时候可以，系统访问量大了就崩了。 事实上大多数的内部系统和ToB业务，访问量不大，直接用数据库就解决问题了 如果业务访问量上来了，这时候频繁访问数据库，就会造成很明显的瓶颈。 这也是大多数“古典“网站和系统，用户访问一多就崩溃的原因 在设计系统的时候没有考虑：高访问量，高并发 一般认为预计访问量有超过2000次/秒，直连数据库的方案就不太建议了 为了避免数据库被打崩，我们就需要考虑在数据库和代码层之间加上一个缓存 有很多种缓存，下面以用得最多的Redis来举例子  答案2:加缓存（例如Redis)  现在我们用了Redis在数据库和业务之间做缓冲 需要访问一个商品的时候  1.业务传过来一个商品id 2.在redis中查找是否有这个id的信息，有就直接返回 3.如果redis中没有找到，去数据库里读取，读取到了信息存入redis，并返回给用户   因为多了一层redis，程序性能得到了极大的优化 访问变快了（纯内存的redis比MySQL要快很多） 不会因为大量的访问被堵死了（单节点的Redis可负担的简单QPS大约是10万，MySQL大约是0.4万） 现在系统的瓶颈解决了，那么接着往下想 如果此时数据库的信息被更新了，Redis中的缓存信息怎么办？ 可能有同学认为，数据库更新了，也把Redis信息同步更新/或删除了不就行了 事实上你细想一下，就没那么简单了 这就是引出了一个问题：缓存一致性问题  缓存一致性问题  当修改一条商品信息，MySQL和Redis缓存都需要修改，两者之间会有先后顺序，可能导致数据不一致。
  当我们需要修改商品时，需要考虑3个问题：  1.先更新缓存还是先更新数据库？ 2.更新缓存的时候,是更新(update)缓存，还是删除(delete)缓存？ 3.怎么更新缓存保证一致性？    1、先更新缓存还是先更新数据库？  如果先更新缓存，写数据库失败，则缓存为最新数据，数据库为旧数据，缓存为脏数据。 之后其他查询马上进来就会拿到这个数据，但是这个数据在数据库中是不存在的。 数据库中不存在的数据缓存并返回给客户端是没有意义的。 所以不能先更新缓存。只能是：DB First  2、更新缓存的时候,是更新(update)缓存，还是删除(delete)缓存？  这里推荐是修改商品的时候，直接删除(delete)缓存 原因是update缓存通常比delete缓存需要更多的资源 为了得到一条商品的完整信息，可能会join几张表得到一个json，组装起来set到redis中的代价，会比直接del一个rediskey要大得多 而在一个高并发系统中，我们要尽可能的保证整个修改是尽可能快的完成(代价是一次缓存失效)  3.</description>
    </item>
    
    <item>
      <title>MySQL中间件对比:ProxySQL/MaxScale/ShardingSphere</title>
      <link>/mysql/mysql%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%AF%B9%E6%AF%94/</link>
      <pubDate>Thu, 03 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E4%B8%AD%E9%97%B4%E4%BB%B6%E5%AF%B9%E6%AF%94/</guid>
      <description>MySQL中件间对比 目标：对比以下3款MySQL中件间
 ProxySQL MaxScale ShardingSphere  一.Proxy基础能力 兼容性  ProxySQL:良好 MaxScale:mairdb更友好，MGR支持不好 ShardingSphere:良好  发行方  ProxySQL:sysown MaxScale:mairdb ShardingSphere:京东  发布/更新/生态  ProxySQL:c++, 5.1k stars,最新版v2.4.4 1个半月前发布 MaxScale:c,1.6k stars,最新版22.08.2 3个半月前发布 ShardingSphere:java,17.6k stars,最新版5.2.1 16天前发布  运维便利性  ProxySQL:简单 MaxScale:普通 ShardingSphere:复杂（配置文件多且复杂）  二.Proxy高可用架构支持 主从架构下的从库故障  如何探测到   ALL: 每N秒主动探测一次
  如何响应故障   ALL:探测不到，主动处理: - ShardingSphere:从路由表中标记下线 - proxysql:从group中标记下线 - MaxScale:标记下线
  切换后状态   ALL:复制拓扑可用
  影响时长   ALL:1-5 秒</description>
    </item>
    
    <item>
      <title>翻到一个旧网站580top.com的尸体</title>
      <link>/book/580top%E7%BD%91%E7%AB%99/</link>
      <pubDate>Thu, 27 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/580top%E7%BD%91%E7%AB%99/</guid>
      <description> 整理一个废弃的移动硬盘 发现以前做的一个网站的文件 10几年前的我审美也是在线的 网站风格很清新 现在看来也不过时 虽然核心的动态文件，数据库都没了 但是生成的静态文件还在 于是我把它复活了   快速旧网站导航 SQLServer Oracle MySQL  MySQL基础知识 MySQL高可用性和HA MySQL性能与优化 MySQL故障与解决 MySQL源码解读   其他数据库 系统运维 DBA考试 DBA话题  </description>
    </item>
    
    <item>
      <title>MySQL高可用组件之ProxySQL</title>
      <link>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Bproxysql/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Bproxysql/</guid>
      <description>ProxySQL是什么 介绍  MySQL一款开源的中间件的产品 支持读写分离 支持 Query 路由功能 支持动态指定某个SQL进行缓存 支持动态加载（无需重启ProxySQL服务） 故障切换和SQL过滤功能。  ProxySQL初始化 安装 wget https://github.com/sysown/proxysql/releases/download/v2.4.4/proxysql-2.4.4-1-centos7.x86_64.rpm rpm -ivh proxysql-2.4.4-1-centos7.x86_64.rpm 产生报错： warning: proxysql-2.4.4-1-centos7.x86_64.rpm: Header V4 RSA/SHA512 Signature, key ID 8217c97e: NOKEY error: Failed dependencies: gnutls is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28()(64bit) is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28(GNUTLS_1_4)(64bit) is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28(GNUTLS_3_0_0)(64bit) is needed by proxysql-2.4.4-1.x86_64 libgnutls.so.28(GNUTLS_3_1_0)(64bit) is needed by proxysql-2.4.4-1.x86_64 执行： yum install -y gnutls rpm -ivh proxysql-2.4.4-1-centos7.x86_64.rpm 检查安装情况  rpm -ql proxysql /etc/logrotate.</description>
    </item>
    
    <item>
      <title>MySQL高可用组件之orchestrator</title>
      <link>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Borchestrator/</link>
      <pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B9%8Borchestrator/</guid>
      <description>orchestrator是什么 介绍  GitHub公司的用go语言编写并开源的一个MySQL高可用管理工具 MySQL高可用性和复制拓扑管理工具，支持复制拓扑结构的调整，自动故障转移和手动主从切换等。 提供Web界面展示MySQL复制的拓扑关系及状态，通过Web可更改MySQL实例的复制关系和部分配置信息 同时也提供命令行和api接口，方便运维管理。 相对比MHA来看最重要的是解决了管理节点的单点问题，其通过raft协议保证本身的高可用。  特点  自动发现MySQL的复制拓扑，并且在web上展示。 重构复制关系，可以在web进行拖图来进行复制关系变更。 检测主异常，并可以自动或手动恢复，通过Hooks进行自定义脚本。 支持命令行和web界面管理复制。  功能限制  slave不能手动提升为master 不支持多源复制 不支持并行复制 不支持与PXC联合使用  注意事项  对主机名依赖严重，习惯用ip来管理实例的，需要注意确保主机名可解析 主从拓扑结果目前不支持两个实例互为主备或环形结构 自动故障转移，只负责将主从切换了，把从设置为主，其他的变更（dns或proxy变更以及运维信息的变更等需要自己写hook脚本）  orchestrator的工作原理 orchestrator的探测机制  orchestrator会每隔InstancePollSeconds（默认5s）时间用以下SQL去被监控的实例上读取实例状态  show global status like &#39;Uptime&#39; select @@global.hostname, ifnull(@@global.report_host, &#39;&#39;), @@global.server_id, @@global.version, @@global.version_comment, @@global.read_only, @@global.binlog_format, @@global.log_bin, @@global.log_slave_updates show master status show global status like &#39;rpl_semi_sync_%_status&#39; select @@global.gtid_mode, @@global.server_uuid, @@global.gtid_executed, @@global.gtid_purged, @@global.master_info_repository = &#39;TABLE&#39;, @@global.binlog_row_image show slave status select count(*) &amp;gt; 0 and MAX(User_name) !</description>
    </item>
    
    <item>
      <title>2022年视觉下的元宇宙</title>
      <link>/book/2022%E5%B9%B4%E8%A7%86%E8%A7%89%E4%B8%8B%E7%9A%84%E5%85%83%E5%AE%87%E5%AE%99/</link>
      <pubDate>Tue, 11 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/2022%E5%B9%B4%E8%A7%86%E8%A7%89%E4%B8%8B%E7%9A%84%E5%85%83%E5%AE%87%E5%AE%99/</guid>
      <description>01 什么是元宇宙  元宇宙就是「metaverse」 verse 是宇宙 meta 来自遥远的古希腊。 metaphysics，翻译叫做形而上学 我们生活的现实宇宙就是 universe 对应的虚拟宇宙（元宇宙）就是 metaverse 所以元宇宙就是说区别真实存在的宇宙，我们用意念幻想出来的一个虚拟的宇宙世界（可以理解成人们虚拟出来的一个平行世界） 北京大学陈刚教授、董浩宇博士这样定义元宇宙：“元宇宙是利用科技手段进行链接与创造的，与现实世界映射与交互的虚拟世界，具备新型社会体系的数字生活空间。” 电影《头号玩家》  02 元宇宙概念的形成   1981年年弗诺·文奇的《真名实姓》和 1984 年的威廉·吉布森的《神经漫游者》科幻小说构建了科幻的世界框架被称为赛博朋克（Cyberpunk）
  1992年的科幻小说《雪崩》，作者 Neal Stephenson 描绘了一个虚拟的平行数字世界（metaverse），人们用阿凡达（Avatar）在其中生活、工作、娱乐。
  《雪崩》创造了 metaverse 这个单词来命名 cyberspace。
  一般人们认为1992年的小说《雪崩》是元宇宙的起源,因为它创造了：metaverse这个词
  这本小说引入国内时，译者将 metaverse 翻译成了「超元域」。所以「元宇宙」这个概念，只是科幻小说中对于人类未来的一种假设
  2003年游戏《Second Life》发布，它在理念上给我们部分解放了现实世界所面临的窘境，这句话怎么理解。就是我们在现实世界中最痛苦的一件事是不能快速调整自己的身份，而在虚拟世界当中，我们可以通过拥有自己的分身来实现，所以《Second Life》给了我们过一种新生活的可能性。
  2021年是元宇宙元年。
  2021年初，Soul App提出构建“社交元宇宙”。
  2021年3月，被称为元宇宙第一股的罗布乐思（Roblox）正式在纽约证券交易所上市；
  2021年5月，微软首席执行官萨蒂亚·纳德拉表示公司正在努力打造一个“企业元宇宙”；
  2021年8月，英伟达宣布推出全球首个为元宇宙建立提供基础的模拟和协作平台；
  2021年8月，字节跳动斥巨资收购VR创业公司Pico；
  2021年10月28日，美国社交媒体巨头脸书（Facebook）宣布更名为“元”（Meta），来源于“元宇宙”（Metaverse）；</description>
    </item>
    
    <item>
      <title>MySQL的参数对比方法</title>
      <link>/mysql/mysql%E7%9A%84%E5%8F%82%E6%95%B0%E5%AF%B9%E6%AF%94%E6%96%B9%E6%B3%95/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E5%8F%82%E6%95%B0%E5%AF%B9%E6%AF%94%E6%96%B9%E6%B3%95/</guid>
      <description>什么时候需要对比MySQL参数  迁移时，从一个集群到另一个集群 升级时，从一个版本到另一个版本 巡检时，需要关注重点参数是否有人为修改过 其他时候，自己去想  怎么对比 实例少时，比如两个集群的几组实例  可以去每台机器上把重点参数打印出来 手动对比  大量数据库实例的对比，需要用脚本工具实现 第一步，建一个收集表 CREATE TABLE `info_variables` ( `instanceid` int NOT NULL DEFAULT &#39;0&#39;, `var_key` varchar(100) NOT NULL DEFAULT &#39;&#39;, `var_value` varchar(1000) NOT NULL DEFAULT &#39;&#39;, `linkname` varchar(50) NOT NULL DEFAULT &#39;&#39;, clustertype varchar(10) not null default &#39;&#39;, `_timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`instanceid`,`var_key`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; 第二步，建一个收集任务  我是在dboop平台上配置的，执行show global variables再 insert 到info_variables表  第三步，清理指标 delete from info_variables where var_key like &#39;wsrep%&#39;; delete from info_variables where var_key like &#39;performance_schema_%&#39;; delete from info_variables where var_key like &#39;ssl_%&#39;; delete from info_variables where var_key like &#39;log%&#39;; delete from info_variables where var_key like &#39;group%&#39;; delete from info_variables where var_key like &#39;validate%&#39;; delete from info_variables where var_key like &#39;gtid%&#39;; delete from info_variables where var_value like &#39;%/%&#39;; delete from info_variables where var_key in ( &#39;datadir &#39;, &#39;hostname &#39;, &#39;innodb_data_home_dir &#39;, &#39;innodb_log_group_home_dir &#39;, &#39;innodb_undo_directory &#39;, &#39;port &#39;, &#39;relay_log &#39;, &#39;relay_log_basename &#39;, &#39;relay_log_index &#39;, &#39;report_port &#39;, &#39;server_id &#39;, &#39;slave_load_tmpdir &#39;, &#39;socket &#39;, &#39;tmpdir &#39; ); 第四步，形成报告  可以按需求出报告或报表 甚至做可视化的参数对比页面  重点关注参数 时间类：  explicit_defaults_for_timestamp time_zone  自增主键：  auto_increment_increment auto_increment_offset innodb_autoextend_increment innodb_autoinc_lock_mode  连接属性：  join_buffer_size max_tmp_tables wait_timeout max_allowed_packet max_connections  字符编码：  character_set_server transaction_isolation collation_connection collation_database collation_server  mode:  sql_mode   以上参数的变化和不一致，可能会在迁移或升级过程中带来严重的后果，需慎重。</description>
    </item>
    
    <item>
      <title>Python入门之书上没有的东西</title>
      <link>/ops/python%E5%85%A5%E9%97%A8%E4%B9%8B%E4%B9%A6%E4%B8%8A%E6%B2%A1%E6%9C%89%E7%9A%84%E4%B8%9C%E8%A5%BF/</link>
      <pubDate>Mon, 26 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/ops/python%E5%85%A5%E9%97%A8%E4%B9%8B%E4%B9%A6%E4%B8%8A%E6%B2%A1%E6%9C%89%E7%9A%84%E4%B8%9C%E8%A5%BF/</guid>
      <description>分享分  为Python零基础同学做了一场入门分享 准备了1个小时的ppt 现场因为有很多演示，共花了90分钟 还好今天没人过来抢会议室。  ppt 说重点：  三天打鱼两天晒网的学习Python是不现实的，每天抽小半个小时敲代码，坚持下去，一两个星期就能入门。 最先要学的不是基础知识，而是环境配置和工具的选择 基础知识不用啃得太死，差不多理解了有印象就行，后面随着练习会不断强化，自然就学会了 练习为王，千万不要死看书，不动手。 多写，多练习，遇到报错顺着问题解决，一开始有卡住的找朋友同事帮忙，后面要学着自己搜索 多利用搜索引擎，随着技能的提升，会不断的有新问题出现。 要针对一个具体的目标和小型项目来进行练习。  </description>
    </item>
    
    <item>
      <title>vscode在mac上用PyQt5制作窗口应用</title>
      <link>/ops/python%E5%88%B6%E4%BD%9Cmac%E7%A8%8B%E5%BA%8F/</link>
      <pubDate>Tue, 20 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/ops/python%E5%88%B6%E4%BD%9Cmac%E7%A8%8B%E5%BA%8F/</guid>
      <description>环境准备  vscode python3 (我用的是python3.9)  安装PyQt5 pip3 install PyQt5 pip3 install PyQt5-tools VSCode中安装和配置pyqt插件 安装了后点设置
Qtdesigner：这里写入designer.app的完整路径 {这里写python的安装绝对路径}/site-packages/qt5_applications/Qt/bin/Designer.app/Contents/MacOS/Designer
至此就完成了环境准备
设计一个窗体程序 建一个项目文件夹，然后右键新建一个form 打开的design窗口，拖动控件，完成窗体设计 保存窗体到 form1.ui 直接cmd+s 保存即可，这里的form1名字可以随意起
编译form1.ui到py文件 右键选compile
这里还可以继续调整生成py文件（如果有需要的话）
新建一个启动文件 main.py import sys from PyQt5.QtWidgets import QMainWindow,QApplication,QWidget from Ui_form1 import Ui_Dialog #导入你写的界面类 class MyMainWindow(QMainWindow,Ui_Dialog): #这里也要记得改 def __init__(self,parent =None): super(MyMainWindow,self).__init__(parent) self.setupUi(self) if __name__ == &amp;quot;__main__&amp;quot;: app = QApplication(sys.argv) myWin = MyMainWindow() myWin.show() sys.exit(app.exec_()) 调试和生成程序 调试代码 用pyton单应用启动调试main.py ，顺利的话会出现刚刚设计好的窗体。 如果有报错的话，解决它
安装pyinstall  pip3 install pyinstaller 编译和生成一个可执行文件  sudo pyinstaller --windowed --onefile --clean --noconfirm main.</description>
    </item>
    
    <item>
      <title>万物不如MySQL_万物皆可Join</title>
      <link>/dba/%E4%B8%87%E7%89%A9%E7%9A%86%E5%8F%AFsql/</link>
      <pubDate>Thu, 25 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E4%B8%87%E7%89%A9%E7%9A%86%E5%8F%AFsql/</guid>
      <description>当前数据库架构越来越复杂
  数据库MongoDB,Redis,Es,Kafka,Postgresql&amp;hellip;
  加上传统的关系型数据库（MySQL,Oracle,SQLServer)
  你是否因为各种数据库的查询语言不同而头晕眼花，到处撞墙！
   你是否各种分库分表后，不同的数据库之前没办法join联合查询而一蹶不振   你是否业务同学发给你一个excel，让你查这些订单的明细而不知所措，来回倒腾。   你是否在焦急的等待着BI大数据同事帮你把不同数据源的表都抽到一起才能join出想要的数据？   怎么办？怎么办？ 没办法!!! 拆开的数据库没办法放在一台服务器上 各种数据库也没办法统一成一种 大数据部门的同步任务正在走流程 走完的流程，他们也不能保证数据同步任务不中断 Excel不是数据库不能用SQL 怎么办？怎么办？ 这种混乱就没人能治吗？ 不要让这些问题挡住你前进的脚本 dboop平台的统一查询平台横空出世 不再区分数据库类型 所有的数据库种类都支持MySQL语法 是的，你没有听错 不管什么类型的数据库 统统只需要记住MySQL语法了 Oracle,SQLServer,MongoDB,kafka DBA在运维的每一种数据库 都可以当成MySQL一样使用了  kafka当成MySQL Mongo当成MySQL  而且这些表都是可以互相join ,union 的   现在我们来休验一下这神奇的黑科技 第一步 我们有个这样的excel 第二步 把excel上传到平台上 第三步 得到一个可以查询的excel文件 第四步 用excel join MySQL 这就是我们说的： 几个问题 问题1:查询会不会影响线上业务  绑定了dba的高可用架构系统，可以自动路由到专门给bi取数服务的专用只读实例上。不会对线上应用产生影响 理论上bi抽数进程会和它产生资源抢占，但是因为bi抽数多数是凌晨进行，两个并不冲突
 问题2:查询的性能怎样  快，非常快，普通的单表查询0.</description>
    </item>
    
    <item>
      <title>说能源</title>
      <link>/book/%E8%AF%B4%E8%83%BD%E6%BA%90/</link>
      <pubDate>Wed, 10 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E8%AF%B4%E8%83%BD%E6%BA%90/</guid>
      <description>山顶的人工水库  昨天在群里讨论股票 讨论化工能源的前景时 说起周末去北京郊区爬山 山顶上有个巨大的人工水库 这个水库的作用是 用电低峰时把水从山下抽到山顶上来 等想发电的时候 就放水发电 在感叹电力储能设备如此窘迫的时候 几个同学都不信有这么傻的事情 从经济价值/能量损失/投资收益 都觉得不可思议 是王多鱼的傻项目 要不是亲眼看见 也会怀疑这种储能方案的可行性 初见时也是觉得这种方案有点傻 后来搜了一下相关资料 这个抽水蓄能的方案 尽然是当前电力储能的最优解 无论是储能效率 还是绿色环保尽然还都占了不少优势 大毛和二毛的战争还在打得要死要活的 欧洲的能源依赖于俄罗斯 所以制裁的时候反而会伤到自身的能源安全 同时全世界的能源价格都在爆涨 所以，来说说能源吧  战争的尽头是能源  远古时候人类的战争：食物 再后来战争为了：土地 而现代战争就成了：能源 有了能源人类可以生存 有了充足的能源可以生存得很好 有了无尽的能源 人类可以毁天灭地 可以削山填海无所不能 可以在室内种大麻 可以填海造田 可以在沙漠里人造绿洲 如果有无尽的能源 人类可以把海水都给冻成冰 也可以把南极变成大火炉 可以去想去的任何地方 以前学政治课本的时候 里面描绘的物质极大丰富的主义 只要有无尽的随时可以取用的能源  能源的尽头是电力  可以用的能源可以有煤，石油，天然气 也可以是不太能直接用的太阳，风，势能 就像所有的程序需要统一接口一样 未来的能源会越来越趋像于统一的接口 那就是电 第二次工业革命是电力 人类开始使用电 第三次工业革命是计算机技术 人类更加依赖电 迟迟没有到来的第四次大变革 觉得可能的方向有 1.核能技术的快速成熟和民用 2.生物技术的快速发展（人类生命大幅延长，智力开发更进一步） 3.储能设备的革命 4.人工智能的革命性突破（是真的代替超越人类，不是现在这种半吊子） 1,3是人类的电力供给提升 2,4是人类对电力需求增大 还是会围绕着能源来进行 而且会越来越标准 也就是人类对烧煤，烧气的需求会越来越少 而对电的依融会越来越大 而事实上现有的油气煤 除掉油用来做化工提炼外 主要的作用也是用来燃烧发热 很大的比例已经用来发电了 油气煤最终会变成电为人类服务 之所以我们还在用煤/用气 是因为转化成本和储存成本还过高  电力的难题是储能  电和石油，天然气不一样 它生产出来很快就得用掉 暂时没有好的储存方式 这也就导致用电高峰时缺电 用电低峰时电力过剩 前些年去草原自驾游 看到到处都有巨大的风电在转 和当地人聊天说这些电会统一到电站调好了再并网 风电和现在正火的光伏电站 都会有个问题 不稳定 风的大小，太阳光的大小 都是不可控因素 而需求是相对稳定的 但是供给是非常不稳定的 其实水电也有这种情况，只是好控制一些 当然现在发电的主力还是烧煤 所以这些绿色发电的不稳定还不明显 如果未来这些绿色能源进一步提升 那电力储能问题会更加明显 说起能源安全的时候 会发现 如果电能可以低成本的长期保存 那么人类的能源会缓解很多 可以在能源充足的时候（水/风/太阳） 充分发电储存 而可利用的途径其实非常非常多 只是这些电的稳定性不高 所以不好利用 从早些年的铅酸电池 到现在的锂电 再到电动车，电动汽车的普及 只是人类储电的一点小进步 就可以让街上跑了这么多绿牌车 如果储能技术有突破的话 未来的能源结构会发生更明显的调整 人类的能源开采能力会提升（开采出来直接存储） 人类的能源利用率会提升（低峰的电力可以储存） 人类的能源焦虑会减少  图1:电力储能市场装机结构 2021 年，我国已投运电力储能项目累计装机 4610 万千瓦，占全球市场总规模的 22%，同比增长 30%。</description>
    </item>
    
    <item>
      <title>定时收集存储过程函数视图信息入库(Oracle,MySQL)</title>
      <link>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%87%BD%E6%95%B0%E8%A7%86%E5%9B%BE%E4%BF%A1%E6%81%AForaclemysql/</link>
      <pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/oracle/%E5%AE%9A%E6%97%B6%E6%94%B6%E9%9B%86%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E5%87%BD%E6%95%B0%E8%A7%86%E5%9B%BE%E4%BF%A1%E6%81%AForaclemysql/</guid>
      <description>需求 下午接到运维转来的一个权限申请流程：大数据部门研发要求开通保垒机权限。以方便在保垒机上安装SQL客户端去查看存储过程和视图的内容
保垒机直连数据库查询，这种不可控的方式，早在去年我就把这个历史问题给禁止掉了，没想到现在还有人要求开通
经过沟通得知，对方想查看Oracle数据库里的一些老的存储过程的代码。而DBA平台上只有表结构相关的数据字典，没有存储过程和视图的数据字典
所以，别慌，不就这点需求吗，马上就可以加上。
 为什么DBA平台上的数据字典不包括存储过程和视图？ 因为存储过程/函数/视图 也是被我禁掉的，研发人员上线不可以写存储过程和视图。 所以就没想过要在DBA运维平台上做这块功能
 但是因为
 历史原因，以前的Oracle数据库上已经存在很多的视图和存储过程 第三方原因，公司采购的一些第三方服务和软件，带了存储过程和视图 这些被禁止使用的数据库对象，也需要做统一维护  拆解  这些数据库对象的信息用定时任务收集线上的表结构到本地，存为两份 一份入库，做为快照信息，展示给用户。 一份落本地文件，上传到git，用git做版本管理   为什么不在用户请求查看某个数据库对象的信息时，实时查询给用户？
 1.因为历史原因，我们有的库有几万个数据库对象，当用户选择一个库时，list列表加载很慢，所以一开始设计的时候，我们做了快照 2.一份快照，还可以用作数据库对象的git版本管理   建表 在dboop库中建表
 CREATE TABLE `info_objects` ( `objectid` int NOT NULL AUTO_INCREMENT, `dbid` int NOT NULL DEFAULT &#39;0&#39;, `TABLE_SCHEMA` varchar(64) NOT NULL DEFAULT &#39;&#39;, `object_name` varchar(255) NOT NULL DEFAULT &#39;&#39;, `object_type` varchar(64) NOT NULL DEFAULT &#39;&#39;, `object_text` longtext, `cstatus` smallint NOT NULL DEFAULT &#39;1&#39;, `dba_freshtime` datetime NOT NULL DEFAULT &#39;1990-01-01 00:00:00&#39;, PRIMARY KEY (`objectid`), UNIQUE KEY `idx_infoobjects_id` (`dbid`,`object_name`,`object_type`), KEY `idx_info_objects_time` (`dba_freshtime`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 实现数据字典收集入库 建Oracle采集任务  &amp;lt;!</description>
    </item>
    
    <item>
      <title>MySQL的事务隔离和MVCC</title>
      <link>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8Cmvcc/</link>
      <pubDate>Thu, 28 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E5%92%8Cmvcc/</guid>
      <description>0.前言:为什么要写这篇文章？ 事务隔离和mvcc的重要性 不同于很多MySQL的原理，只需要DBA掌握，事务对于研发人员也是必须掌握的知识点和原理。并发程度越高，数据库里的锁和事务越明显，越重要。所以：数据库事务和mvcc是研发和DBA都要熟练掌握的另一方面的原因是现有的资料对mvcc写得不够直观 现有的对mvcc原理的讲解停留在画图阶段，我觉得光画图还不够，要实打实的一个字节一个字节的看MySQL真实的数据文件是怎么实现的。利用自研的MySQL数据文件分析工具（ 参考：innodb存储格式 )。可以很直观的把mvcc实现的底层逻辑给展示出来。
 以下两篇文章，可以协助你更好的理解本章节的内容
  MySQL行格式(compact,redundant,dynamic,compressed) ) MySQ事务id:trx_id )  环境准备  MySQL版本:8.0.22 事务隔离级别:REPEATABLE-READ (默认隔离级别)  建一张表dboopuser并insert几条数据 drop table dboopuser; create table dboopuser( userid int unsigned not null primary key , age smallint unsigned not null default 0, username varchar(20) not null default &#39;&#39;, userimg varchar(255) not null default &#39;&#39; ) ENGINE=InnoDB COMMENT=&#39;测试user表--用于mvcc测试20220727&#39; ; insert into dboopuser(userid,age,username,userimg) values(9527,25,&#39;cccccccccc&#39;,&#39;http://www.dboop.com/img/user/2002_innodbtrx_527.jpg&#39;); insert into dboopuser(userid,age,username,userimg) values(9528,15,&#39;dddddddddddddd&#39;,&#39;http://www.dboop.com/img/user/2002_innodbtrx_528.jpg&#39;); insert into dboopuser(userid,age,username,userimg) values(9529,25,&#39;eeeeeeeeeeeeeeeee&#39;,&#39;http://www.</description>
    </item>
    
    <item>
      <title>google和twitter的镜像</title>
      <link>/book/google%E5%92%8Ctwitter%E7%9A%84%E9%95%9C%E5%83%8F/</link>
      <pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/google%E5%92%8Ctwitter%E7%9A%84%E9%95%9C%E5%83%8F/</guid>
      <description>今天在大学的群里，F同学发了一个截图说王思聪在说
我说这是假消息，因为王思聪的推特账号不是这个sicongwang001名字，然后抓了张图
对***上网这件事也是有很多人不太擅长，于是做了两个镜像，提供给大家一个随时随地可以访问外网的跳转。
镜像站如下：
 Google（用来查资料): 地址就不公开了，有风险，有需要的单独找我吧 Twitter (用来看新闻): 地址就不公开了，有风险，有需要的单独找我吧  enjoy it</description>
    </item>
    
    <item>
      <title>show engine innodb status 工具化实现</title>
      <link>/mysql/mysql%E7%9A%84showinnodbstatus/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84showinnodbstatus/</guid>
      <description>为什么要写这个工具 当MySQL出现性能问题时，DBA经常得去innodb status ，
但是innodb status的输出信息非常丰富也很复杂。滚了几个屏幕的指标，像这样的得翻好几页，几百行的结果。
 哪些是重要的指标 指标具体代表什么意思 指标的值是否正常  非常考验DBA的眼力。
考虑到以上的不方便，写了个小脚本把这些指标提取出来，并将指标对应的中文意思和合理取值范围做了详细的备注。
实现思路  当用户选中MySQL实例，并点击show engine innnodb statutus按钮时 后台进程去该实例执行 show engine innnodb statutus 语句 返回结果做正则筛选，将各种指标和值保存在一个字典中 提前准备好一个指标的字典，字典里记了该值的中文说明和取值范围（取值范围这个还没做好） 两个字典一合并，输出一个分好类的可视化结果  指标提取和定义 脚本内容是定义了一个数据字典去翻译这些指标
{ &amp;quot;background_thread&amp;quot;:(&amp;quot;后台进程:除掉用户请求的活动会话，MySQL后台进程也会定时的进行一系列工作。&amp;quot;,[(&amp;quot;master_thread_loops_active&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;后台master线程avtive执行次数合计值&amp;lt;/b&amp;gt;,后台master线程的每次循环时会选择一种状态来执行(active、shutdown、idle),active次数/idle次数 比值越高，代表系统的写操作越繁忙。&amp;quot;), (&amp;quot;master_thread_loops_idle&amp;quot;,&amp;quot;&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;后台master线程idle执行次数合计值&amp;lt;/b&amp;gt;,和上一个指标连起来看,idle次数越高，代表系统的写操作越少。所以该指标值越大，系统写资源越充足&amp;quot;), (&amp;quot;master_thread_log_flush_and_writes&amp;quot;,&amp;quot;Bytes&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;后台master线程刷新redo日志&amp;lt;/b&amp;gt;,定期刷新redo日志，和参数innodb_flush_log_at_timeout控制刷新时间&amp;quot;) ] ) ,&amp;quot;bufferpool_memory&amp;quot;:(&amp;quot;缓冲池:有关已读和已写页面的统计信息。可以从这些数字中获得缓冲池的使用情况。&amp;quot;,[ (&amp;quot;total_large_memory_allocated&amp;quot;,&amp;quot;Bytes&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;分配给InnoDB Buffer Pool的总内存&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;dictionary_memory_allocated&amp;quot;,&amp;quot;Bytes&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;分配给InnoDB数据字典的内存&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;buffer_pool_size&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;分配给IBP的内存，单位pages&amp;lt;/b&amp;gt;,每页16k&amp;quot;) ,(&amp;quot;buffer_pool_hit&amp;quot;,&amp;quot;/1000&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool命中率&amp;lt;/b&amp;gt;每1000次请求有*次命中buffer pool,非常重要&amp;quot;) ,(&amp;quot;free_buffers&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool Free List 总大小，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;database_pages&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool LRU List 总大小，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;old_database_pages&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool old LRU 总大小，单位pages(冷端)&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;modified_db_pages&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;Buffer Pool中脏页的数量，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pending_reads&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;等待读入Buffer Pool的页数量，单位pages&amp;lt;/b&amp;gt;,理论上不应该有等待队列&amp;quot;) ,(&amp;quot;pending_writes_lru&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;LRU中buffer中等待被刷的脏页数，单位pages&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pending_writes_flush_list&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在checkpoint期间要刷新的缓冲池页数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pending_writes_single_page&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在缓冲池中写入挂起的独立页的数目&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_made_young&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;热点页数&amp;lt;/b&amp;gt;,在缓冲池LRU list中年轻的总页数(移动新页面到sublist的头部)&amp;quot;) ,(&amp;quot;pages_made_not_young&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;old sublist中的page数，冷端的page数&amp;lt;/b&amp;gt;,在缓冲池LRU列表中不年轻的页面总数(保留旧页面在sublist中，不改变)&amp;quot;) ,(&amp;quot;pages_made_young_per_sec&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒LRU链中被young的page数&amp;lt;/b&amp;gt;,oungs/s度量标准仅用于old pages，基于对page的访问次数，而不是页的数量。对页进行多次访问都会被计算。如果见到非常低的值，可能需要减小延迟或增加old page LRU list 的比例。增大后，页面需要更长的时间才会移动到尾部，这就增加了再次访问page，从而使他们made young的可能性增大&amp;quot;) ,(&amp;quot;pages_made_non_young_per_sec&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒LRU链中未被young的page数&amp;lt;/b&amp;gt;，可以一定程度上看出库的繁忙程度和命中率,Not young，如果在执行大表扫描时未看到较高的non-young和non-youngs/s，请增加innodb_old_blocks_time。&amp;quot;) ,(&amp;quot;pages_read&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool中读取的page总数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_created&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在bufferpool中创建的page数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_written&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool写入的page数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_read_per_sec&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool中读取的page数/秒&amp;lt;/b&amp;gt;, 比较重要，代表库的繁忙程度&amp;quot;) ,(&amp;quot;pages_created_per_sec&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;在bufferpool中创建的page数/秒&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_written_per_sec&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;从bufferpool写入的page数/秒&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;pages_read_ahead&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒平均预读操作次数&amp;lt;/b&amp;gt;k&amp;quot;) ,(&amp;quot;evicted_without_access&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒驱逐的page数&amp;lt;/b&amp;gt;k&amp;quot;) ,(&amp;quot;random_read_ahead&amp;quot;,&amp;quot;page/s&amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;每秒钟随机预读操作的次数&amp;lt;/b&amp;gt;&amp;quot;) ,(&amp;quot;lrn_len&amp;quot;,&amp;quot; &amp;quot;,&amp;quot;not count&amp;quot;,&amp;quot;&amp;lt;b&amp;gt;LRU的长度&amp;lt;/b&amp;gt;&amp;quot;) ] ) .</description>
    </item>
    
    <item>
      <title>MySQL复制参数_slave_rows_search_algorithms及无主键表的处理</title>
      <link>/mysql/mysql%E5%A4%8D%E5%88%B6%E5%8F%82%E6%95%B0slave_rows_search_algorithms/</link>
      <pubDate>Mon, 04 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%A4%8D%E5%88%B6%E5%8F%82%E6%95%B0slave_rows_search_algorithms/</guid>
      <description>0.起因 线上MySQL实例，今天报大量延时，且卡住不动。（表现为seconds_behind_master不断上涨，从库gtid不动）
同时二级从库有复制SQL进程报错
[ERROR] [MY-010584] [Repl] Slave SQL for channel &#39;&#39;: Could not execute Update_rows event on table 「表名」; Can&#39;t find record in &#39;「表名」&#39;, Error_code: 1032; handler error HA_ERR_END_OF_FILE; the event&#39;s master log mysql-bin.000****, end_log_pos *******, Error_code: MY-001032 1.排查问题 排查问题时
 确认该实例上的从库不提供线上实时业务访问（业务可以接受延时）。不需要做从库切流量动作 先是看了一下从库的多线程复制是database级的，开启多线程复制到logical_clock ,问题并没有恢复       set global slave_parallel_type=&amp;lsquo;logical_clock&amp;rsquo;; set global slave_parallel_workers=4; start slave sql_thread;```
 排除掉是线程数不够的原因 发现processlist中是在等Applying batch of row changes (update)  确定是卡在sql进程，再看relaylog确实持续增长800M(表示该实例写入不频繁)   解析relaylog 发现是普通的update语句大约有8000次左右  这个量级的update且是row模式，理论1分钟内就追上了。   查看锁datalocks，发现有大量的行数50几万，都是同一个表的  slave的sql进程不应该有这个量级的行锁。   查看表结构发现这个表是无主键的表，里面大约有50几万条记录，无主键无索引  2.</description>
    </item>
    
    <item>
      <title>图数据库nebula源码编译安装</title>
      <link>/dba/nebula%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/</link>
      <pubDate>Wed, 29 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/nebula%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/</guid>
      <description>环境准备 当前机器环境centos7
硬件要求    类型 要求     CPU 架构	x86_64   内存 4 GB   硬盘 10 GB，SSD    环境依赖（针对centos7)    软件名称 版本 备注     glibc 2.17 及以上 执行命令ldd &amp;ndash;version检查版本。   g++ 8.5.0 及以上 执行命令g++ -v检查版本。   cmake 3.14.0 及以上 执行命令cmake &amp;ndash;version检查版本。    yum安装准备 cd /etc/yum.repos.d/mv CentOS-Base.repo CentOS-Base.repo.backwget -O CentOS-Base.repo http://mirrors.</description>
    </item>
    
    <item>
      <title>图数据库nebula性能监控</title>
      <link>/dba/nebula%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7/</link>
      <pubDate>Thu, 23 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/nebula%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7/</guid>
      <description>目的 nebulaGraph官方自带的Nebula Dashboard 监控工具里的监控指标和筛选粒度已经足够丰富。但基于以下原因，还是在此基础上自己做了层监控
 缺少关键指标的定义，对除DBA以外的用户不友好。我们希望把性能数据让研发同学也能看到 缺少一个对所有服务器的横向对比和集中展示Dashboard 与我们现有的DBA监控不在一个平台，需要多平台切换使用 没有性能指标评分，同样的指标，没有给出是否合格的标准和提示 官方已经提供了可以直接读取性能数据的接口 http://%s/stats，在些基础上二次开发监控的难度会很低，预估开发工作量1pd ,实际开发工作量1.5pd  1.指标筛选 官方提供的性能接口里有几百项指标，我们从graph,storage,rockdb 三个层面，筛选了60几个重点的，需要关注的指标，筛选的依据是
 是否能从指标定位到资源或性能问题（响应时间等） 是否在排查问题出现时，有助于定位异常（命令次数，qps等) 是否有类似的指标已经存在。  最终，挑选了以下指标 2.编写收集代码（Python） 这部分因为有官方接口，所以直接请求就可以了
for nodeid,nodehost in grpahlist: dict_result=self.get_nebula_stats(nodehost) #入库 dict_result def get_nebula_stats(self,nodehost) dict_result={} urlstr=&amp;#39;http://%s/stats?format=json&amp;#39;%(nodehost,) mlist=self.get_urldata(urlstr) for dictc in mlist: for k,v in dictc.items(): if k in dict_graph: keyname=dict_graph[k][0] dict_result[keyname]=v return dict_result 3.性能评价 这是非常重要的一步，沿用我在2005年做的数据库性能模型的方法（参考：https://github.com/51ak/DatabaseRating/）
 对我们的每一项指标，我们需要对其取值范围进行判断，给其打分：优，良，中，劣。 标记为:weight 对每一项指标，对其权重做标记（0-5）,标记为：height 由weight和height计算出这个实例的健康程度 但是我们的nebula服务只有一个集群，不需要太细化，所以我们只做了weight标记  做weight标记的逻辑是定义如下的一个区间列表
dict_graph={ &amp;#34;num_active_queries.sum.60&amp;#34;:(&amp;#34;num_active_queries&amp;#34;,(-4,5,10,50,1000000)), &amp;#34;num_active_sessions.sum.60&amp;#34;:(&amp;#34;num_active_sessions&amp;#34;,(-4,5,10,50,1000000)), &amp;#34;num_opened_sessions.rate.60&amp;#34;:(&amp;#34;num_opened_sessions&amp;#34;,(-4,100,500,10000,10000000)), &amp;#34;num_queries.rate.60&amp;#34;:(&amp;#34;num_queries_rate&amp;#34;,(-4,100,500,10000,10000000)), &amp;#34;num_queries.sum.60&amp;#34;:(&amp;#34;num_queries_sum&amp;#34;,(-4,5000,50000,900000,100000000)), &amp;#34;num_sentences.rate.60&amp;#34;:(&amp;#34;num_sentences_rate&amp;#34;,(-4,100,500,10000,10000000)), &amp;#34;num_sentences.sum.60&amp;#34;:(&amp;#34;num_sentences_sum&amp;#34;,(-4,5000,50000,900000,100000000)), &amp;#34;query_latency_us.</description>
    </item>
    
    <item>
      <title>图数据库nebula实时慢日志收集和展示</title>
      <link>/dba/nebula%E6%85%A2%E6%9F%A5%E8%AF%A2%E7%9B%91%E6%8E%A7/</link>
      <pubDate>Fri, 17 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/nebula%E6%85%A2%E6%9F%A5%E8%AF%A2%E7%9B%91%E6%8E%A7/</guid>
      <description>目的 因为我们的图数据库从neo4j社区版转到nebula graph方向。最近在项目压测的时候，一开始很平稳，运行一段时间后，NQL会越来越慢，发现性能巨差。nebula经常卡死，表现为：
 nebula-storaged.service和nebula-graphd.service服务经常挂掉。 业务侧反馈执行什么NQL都很慢 nebula show query里发现有大量（300-500个）running的进程。平时很快的NQL也卡在那  系统资源
 内存紧张 ，大量的虚拟内存被占用 io,cpu也较高  在做了一些参数调优后，发现状态有所缓解，但是一段时间后，还是会慢慢卡死，从现象上推测是有一种或几种慢NQL多次执行后，把系统资源消耗完了，导致大面积的堵塞。所以面临的需求还是要有个慢查询排查工具。
参考上一次做oracle慢日志收集展示的方法， https://www.dboop.com/oracle/oracle%E6%80%8E%E6%A0%B7%E5%AE%9E%E6%97%B6%E6%94%B6%E9%9B%86%E5%B1%95%E7%A4%BA%E6%85%A2%E6%9F%A5%E8%AF%A2/
变化的是：
 因为nebula的集群就一个，不需要做oracle慢日志的大表套小表，数担据量不大就建了一张monitor_nebula_slow表存放数据. nebula抓到的慢查询里的NQL是没有去参数化的，需要自己做去参数化，把相同类型的NQL，标识为同一个md5id   1.建一张表，每隔1分钟（时间可调，但我们的场景1分钟足够了） CREATE TABLE `monitor_nebula_slow` (`logid` int unsigned NOT NULL AUTO_INCREMENT,`SessionID` varchar(50) NOT NULL DEFAULT &#39;&#39;,`ExecutionPlanID` varchar(50) NOT NULL DEFAULT &#39;&#39;,`User` varchar(50) NOT NULL DEFAULT &#39;&#39;,`Host` varchar(50) NOT NULL DEFAULT &#39;&#39;,`StartTime` datetime DEFAULT NULL,`DurationInUSec` int unsigned NOT NULL DEFAULT &#39;0&#39;,`Status` varchar(50) NOT NULL DEFAULT &#39;&#39;,`Query` varchar(5000) NOT NULL DEFAULT &#39;&#39;,`_timestamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,`md5id` varchar(64) NOT NULL DEFAULT &#39;&#39;,PRIMARY KEY (`logid`),KEY `ix_monitor_nebula_slow` (`_timestamp`)) ENGINE=InnoDB AUTO_INCREMENT=13231 DEFAULT CHARSET=utf8mb32.</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(三):UndoLog</title>
      <link>/mysql/mysql%E7%9A%84undo%E6%97%A5%E5%BF%97/</link>
      <pubDate>Sat, 28 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84undo%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续：
 MySQL的7种日志(一):概况
  MySQL的7种日志(二):RedoLog
 1.什么是undolog  undo：撤销或取消，以撤销/回滚操作为目的，返回指定某个状态的操作。 undolog：数据库事务开始之前，会将要修改的记录存放到 Undo 日志里，当事务回滚时或者数据库崩溃时，可以利用Undo日志，撤销未提交事务对数据库产生的影响。 undolog在事务开始前产生；事务在提交时，并不会立刻删除undolog，innodb会将该事务对应的 undo log 放入到删除列表中，后面会通过后台线程purge thread进行回收处理。 undolog属于逻辑日志，记录一个变化过程。例如执行一个delete，undo log会记录一个insert；执行一个update，undo log会记录一个相反的update。  2.undolog的作用  实现事务的原子性 当事务回滚时或者数据库崩溃时，利用Undo日志，撤销未提交事务对数据库产生的影响。事务处理过程中，如果出现了错误或者用户执行了 ROLLBACK 语句，MySQL 可以利用 Undo Log 中的备份将数据恢复到事务开始之前的状态。 实现多版本并发控制（MVCC） Undo Log 在 MySQL InnoDB 存储引擎中用来实现多版本并发控制。事务未提交之前，Undo Log 保存了未提交之前的版本数据，Undo Log 中的数据可作为数据旧版本快照 供其他并发事务进行快照读。（构建read view视图）  3.undolog的存储 3.1 物理存储位置 找到具体存放的位置 MySQL5.6.3 之前的版本undolog存储在系统共享表空间里，后续的版本推荐存话在单独的文件中
mysql&amp;gt; show global variables like &#39;%undo%&#39;; +--------------------------+-------------------------+ | Variable_name | Value | +--------------------------+-------------------------+ | innodb_max_undo_log_size | 1073741824 | | innodb_undo_directory | /data/mysql3306/innolog | | innodb_undo_log_encrypt | OFF | | innodb_undo_log_truncate | ON | | innodb_undo_tablespaces | 2 | +--------------------------+-------------------------+ 5 rows in set (0.</description>
    </item>
    
    <item>
      <title>redis高可用模式双比</title>
      <link>/dba/redis%E7%9A%84%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB/</link>
      <pubDate>Fri, 27 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/redis%E7%9A%84%E6%95%85%E9%9A%9C%E8%BD%AC%E7%A7%BB/</guid>
      <description>redis高可用模式 常见的高可用模式对比：
主从复制时代 主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(Master)，后者称为从节点(Slave)；数据的复制是单向的，只能由主节点到从节点。
主从复制的优点 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。
主从复制的工作原理  1）若启动一个Slave机器进程，则它会向Master机器发送一个“sync command”命令，请求同步连接。 2）无论是第一次连接还是重新连接，Master机器都会启动一个后台进程，将数据快照保存到数据文件中（执行rdb操作），同时Master还会记录修改数据的所有命令并缓存在数据文件中。 3）后台进程完成缓存操作之后，Maste机器就会向Slave机器发送数据文件，Slave端机器将数据文件保存到硬盘上，然后将其加载到内存中，接着Master机器就会将修改数据的所有操作一并发送给Slave端机器。- Slave出现故障导致宕机，则恢复正常后会自动重新连接。 4）Master机器收到Slave端机器的连接后，将其完整的数据文件发送给Slave端机器，如果Mater同时收到多个Slave发来的同步请求，则Master会在后台启动一个进程以保存数据文件，然后将其发送给所有的Slave端机器，确保所有的Slave端机器都正常。  Sentinel哨兵时代 哨兵(sentinel)：是一个分布式系统，用于对主从结构中的每台服务器进行监控，当出现故障时通过投票机制选择新的 Master 并将所有 Slave 连接到新的 Master。所以整个运行哨兵的集群的数量不得少于3个节点。
从Redis2.8版本起，提供了一个稳定版本的Sentinel哨兵来解决高可用的问题，它的思路是启动奇数个Sentinel的服务来监控Redis服务器来保证服务的可用性。 为了保证监控服务器的可用性，我们会对Sentinel做集群部署，Sentinel既监控所有的Redis服务，Sentinel之间也相互监控。 Sentinel本身没有主从之分，地位是平等的，只有Redis服务节点有主从之分。 Sentinel通过Raft共识算法，实现Sentinel选举，选举出一个leader，由leader完成故障转移。
Raft共识算法： https://www.dboop.com/dba/raft%E5%8D%8F%E8%AE%AE/ 哨兵的核心功能：在主从复制的基础上，哨兵引入了主节点的自动故障转移。
Sentinel的作用  监控：哨兵会不断地检查主节点和从节点是否运作正常。 自动故障转移：当主节点不能正常工作时，哨兵会开始自动故障转移操作，它会将失效主节点的其中一个从节点升级为新的主节点，并让其他从节点改为复制新的主节点。 通知（提醒）：哨兵可以将故障转移的结果发送给客户端。  Sentinel的结构 哨兵结构由两部分组成，哨兵节点和数据节点。
 哨兵节点：哨兵系统由一个或多个哨兵节点组成，哨兵节点是特殊的redis节点，不存储数据。 数据节点：主节点和从节点都是数据节点。  Redis Cluster 时代 redis集群是在redis 3.0版本推出的一个功能，其有效的解决了redis在分布式方面的需求。当遇到单机内存，并发和流量瓶颈等问题时，可采用Cluster方案达到负载均衡的目的。并且从另一方面讲，redis中sentinel有效的解决了故障转移的问题，也解决了主节点下线客户端无法识别新的可用节点的问题，但是如果是从节点下线了，sentinel是不会对其进行故障转移的，并且连接从节点的客户端也无法获取到新的可用从节点，而这些问题在Cluster中都得到了有效的解决。
key与slot的关系是永远不会变的，会变的只有slot和Redis节点的关系。 如果想让很多个key同时落在同一个节点怎么办呢，只需要在key里面加入{hash tag}即可。 Redis在计算槽编号的时候只会获取{}之间的字符串进行槽编号计算，如下所示：
user{666}base=&amp;hellip; user{666}fin=&amp;hellip;
Redis-Cluster 特点  无中心结构。 数据按照slot存储分布在多个节点，节点间数据共享，可动态调整数据分布。 可扩展性，可线性扩展到1000个节点（官网推荐不超过1000个），节点可动态添加或删除。 高可用性，部分节点不可用时，集群仍可用。通过增加Slave做standby数据副本，能够实现故障自动failover，节点之间通过gossip协议交换状态信息，用投票机制完成Slave到Master的角色提升。 降低运维成本，提高系统的扩展性和可用性。  Redis Cluster的作用  （1）数据分区 数据分区(或称数据分片)是集群最核心的功能。  集群将数据分散到多个节点，一方面突破了Redis单机内存大小的限制，存储容量大大增加；另一方面每个主节点都可以对外提供读服务和写服务，大大提高了集群的响应能力。</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(二):RedoLog</title>
      <link>/mysql/mysql%E7%9A%84redo%E6%97%A5%E5%BF%97/</link>
      <pubDate>Fri, 13 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84redo%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 续上一篇： MySQL的7种日志(一):概况
上一篇我准备写MySQL日志还是2个月前的事，这两个月里生活发生了天翻地覆的变化，都没时间更新。
昨天跟朋友聊天立了flag说今天要续写一篇
于是中午吃饭时在纸上画了一个流程图。来介绍下MySQL里的RedoLog
1.几个问题 redolog和binlog一样记录的是数据修改后的记录。区别是什么，存在的意义是什么？  如果不要redolog，直接修改数据行不行？ 答：可以，但是随机读写性能差 先写redolog还是先改数据？答：先写内存里的数据，再写redolog，再写binlog，再写磁盘里的数据 先写redolog还是先写binlog? 答：先写redolog,再写binlog 如果写完redolog，还没来得写binlog时就停电了,怎么办？答：修改不要了，从undolog中回滚数据 如果写完redolog,binlog时还没来得数据落盘就停电了,怎么办？答：重做redolog，提交数据。修改有效  redolog和undolog的关系  答：redolog用来恢复丢失数据（恢复到最后一次提交位置）也称之为前滚操作，undolog是用来回滚到之前的版本，称之为回滚操作  relaylog的作用  答：redolog是用来做崩溃恢复使用的，这种崩溃恢复不需要我们人为的参与，MySQL自己内部自己实现了这种崩溃恢复的功能，我们只管享受这种功能给我们带来的服务即可，这种服务给我们的感受就是：MySQL数据库异常宕机的时候，重启服务之后，数据库中之前提交的记录都不会丢失数据仍然可以正常恢复，不管这种提交的记录是否已经更到具体的表所对应的磁盘page也中。  2.修改数据的流程  当我们要更新一条数据时，比如有一条SQL update userinfo set name=&#39;dboop&#39; where name=&#39;张三&#39;; 最直接的方法：从磁盘上找到对应的数据库文件，把它修改完存放到磁盘中。  方法是可以的，很多简单的程序修改文件也是用的方法，但是性能差。   而数据库中一般会有以下几种方式来写入数据修改  按页组织数据，一些关联近的数据存放在一个页中，MySQL中默认一页是16k 读取和修改数据都是需要先把页加载到内存中,MySQL是放到innodb_buffer_pool中 先改内存，再合适的时候再写入磁盘 先改日志再改数据 日志也是先写内存中的日志buffer，再合适的时候刷入磁盘    下图是简化版的一个数据修改，真实的流程比这复杂很多，这里的数据修改不只是update，按页组织的insert/update/delete操作都是对页修改
3.Redolog在数据库意外崩溃时的作用 当故障发生时，数据库意外当机，有部分内存中已修改的页（脏页）没来得及刷新到磁盘里。
在写入redo log时，会顺便记录XID，即当前事务id。在写入binlog时，也会写入XID。
如果在写入redo log之前崩溃，那么此时redo log与binlog中都没有，是一致的情况，崩溃也无所谓。
如果在写入redo log prepare阶段后立马崩溃，之后会在崩恢复时，由于redo log没有被标记为commit。于是拿着redo log中的XID去binlog中查找，此时肯定是找不到的，那么执行回滚操作。
如果在写入binlog后立马崩溃，在恢复时，由redo log中的XID可以找到对应的binlog，这个时候直接提交即可。
总的来说，在崩溃恢复后，只要redo log不是处于commit阶段，那么就拿着redo log中的XID去binlog中寻找，找得到就提交，否则就回滚。
在这样的机制下，两阶段提交能在崩溃恢复时，能够对提交中断的事务进行补偿，来确保redo log与binlog的数据一致性
4.Redolog的刷盘 4.</description>
    </item>
    
    <item>
      <title>钱也分很多种</title>
      <link>/book/%E9%92%B1%E4%B9%9F%E5%88%86%E5%BE%88%E5%A4%9A%E7%A7%8D/</link>
      <pubDate>Fri, 15 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E9%92%B1%E4%B9%9F%E5%88%86%E5%BE%88%E5%A4%9A%E7%A7%8D/</guid>
      <description>选中上下文，就知道是什么钱了。
中国 . 元 中国澳门 . 元 中国台湾 . 元 中国香港 . 元 美国 . 美元 美国 . 美元_背面 英国 . 英镑 韩国 . 圆 日本 . 元 德国 . 马克 俄罗斯 . 卢布 法国 . 法郎 欧洲 . 欧元 葡萄牙 . 埃斯库多 瑞士 . 法郎 荷兰 . 盾 西班牙 . 比塞塔 加拿大 . 元 意大利 . 里拉 印度 . 卢比 澳大利亚 . 元 新加坡 . 元 泰国 . 铢 越南 .</description>
    </item>
    
    <item>
      <title>我为什么要反对DBA参与业务(出报表/改数据)</title>
      <link>/dba/%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%AF%B9dba%E6%94%B9%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Tue, 12 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%88%91%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%8D%E5%AF%B9dba%E6%94%B9%E6%95%B0%E6%8D%AE/</guid>
      <description>上午有业务人员在钉钉上call我
 A：要修改库里的一批资产的属性 我：这事为啥找我? A：有同事说数据库的事情都要找DBA。 我：DBA不直接改数据,谁跟你这样说的，拉上他一起来说说。 A：不愿意透露这个人是谁，   首先表明态度：坚决反对DBA参与(出报表/改数据)等业务行为，DBA可以提供对应的平台和工具。
 以下行为
 DBA知道很复杂的业务逻辑，知道怎么把一个人客户的帐单做拆分（比业务和产品人员都知道的更清楚） 业务同学邮件发过来一批订单号，DBA同学可以帮他同时修改这些订单号关联的7，8张表的数据和状态（逻辑很复杂，但DBA知道怎么改得滴水不漏） 定期给业务人员出一些报表，这些报表是DBA用很复杂的SQL和各种临时表算出来的。逻辑很复杂，只有DBA能写出来 很多业务和产品上没来得及做的功能，业务会直接发工单给DBA，DBA写SQL帮业务临时完成 都是越界的，DBA不需要也不应该对业务“深入”的了解。  为什么要这样？ 从3个方面说：
 DBA职责 职业分工 数据安全性  一.DBA职责 DBA的首要任务是保护数据，维护高可用访问(而不是主动修改数据)
 制定并实施数据库安全规范/管理规范/访问规范等。 持续改进优化高可用架构（当发生硬件/网络/软件故障时，可以快速恢复） 建立自动监控系统,及时有效处理各种报警 建立自动检查，备份，HA，远程容灾和远程备份系统 发现协助业务优化数据库的库表结构，SQL写法等，共同提升数据库性能  二.职业分工 专业的人做专业的事
 出数据和导报表的工作，有大数据部门的同事，有数据分析师。 修改数据的工作，有各业务线的研发和产品人员，他们设计的系统，他们最理解怎么改，DBA可以提供- SQL上线平台，他们自助完成 如果DBA比大数据的同更懂得出报表，比业务线的研发人员更了解应该写SQL改数据是没有必要的， DBA可以分成:业务DBA和运维DBA,通常我们说的DB都是运维DBA,权限很大。如果有业务DBA会限制他的权限，不会有运维DBA这么大的权限。  三.数据安全性  DBA作为数据的最终维护人员，修改数据是最直接暴力的方式 普通员工没有直连数据库做操作的权限，只能通过上线代码和指定的平台(DBA提供的dboop平台)   DBA不参与业务(出报表/改数据),是否意味着DBA从工作中摘了出去? 也不是
 DBA提供数据查询平台,对短而小的数据查询导出需求,研发定位问题的查询，提供一个快速查询平台（有完整的权限管理，日志审计，行为约束，敏感数据脱敏） DBA提供便利的数据库SQL上线自助流程，方便研发人员快速上线SQL，有完整的日志，SQL审核,数据备份,权限控制,快速回滚。 DBA不需要了解表中的每个字段具体含义，每个表之间的关联，只需要知道这个库存放的数据是否重要，属于哪个业务线，负责人是谁，省下来的精力去做真正运维的工作  DBA有权限可以任意修改库里的数据，但是这个不合规，不应该这样做。正确的做法是提供一个平台，让真正了解业务的研发/产品/数据分析人员，去查询，去出报表，去改数据。</description>
    </item>
    
    <item>
      <title>数据库多环境SQL上线</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%9A%E7%8E%AF%E5%A2%83sql%E4%B8%8A%E7%BA%BF/</link>
      <pubDate>Fri, 08 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%9A%E7%8E%AF%E5%A2%83sql%E4%B8%8A%E7%BA%BF/</guid>
      <description>数据库多环境分类 MySQL数据库按环境区分有以下几种数据库  正式环境数据库：又称生产环境 验证环境数据库：上线验证环境，介于测试和生产之间的一个环境 测试环境数据库：又细分为   test0测试环境：默认测试环境 test1测试环境: test1测试环境简称t1环境，和test0环境是并行关系 test2,test3&amp;hellip; 未来会有更多的测试环境  开发环境：又称Dev环境，用于研发本地调试代码  Oralce测试环境参考MySQL的测试环境 数据库多环境上线的问题  SQL上线（建表/改表，改数据等） 需要在每个环境下都提个流程，研发的重复工作量大 研发可能会忘掉部分环境下执行某个SQL，造成各种环境和线上环境的差异越来越大  解决方式  提供一个多环境SQL上线流程，研发可以勾选每次上线要执行的环境 提供一个多环境数据库表结构对比工具 提供一个不同环境的数据库对象自动同步工具  多环境SQL上线流程 这里只演示MySQL多环境上线流程，Oracle的多环境上线流程和这个一模一样
步骤1.选择数据库 步骤2.填写上线内容  注意1：这里可以自由勾选要执行的数据库环境，红色的是线上环境，绿色的是测试和验证环境，蓝色的是开发环境 注意2：所有环境都会执行SQL验证，所有环境通过验证，才可以进入下一步  步骤3.流程审批  如果通过了系统的SQL审核，会出现如下图的界面，进入流程审批 3.负责人审批是必选步骤 4.测试审批是用户可选步骤 5.DBA审批会根据用户提供的SQL内容，系统判定是否有dba介入（建表/改表/大范围修改数据DBA会审批，否则会自动跳过这一步）  步骤4.用户执行  审批完成以后，由发起人自由选择在合适的时间，点击上线按扭   步骤5.执行结果  执行完成后，会显示每个环境的执行结果。   后台配置 这个流程也是在dboop平台上用xml配置的，后台界面如下</description>
    </item>
    
    <item>
      <title>Oracle运维DBA常用命令集合</title>
      <link>/oracle/oracle%E8%BF%90%E7%BB%B4dba%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Wed, 30 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/oracle/oracle%E8%BF%90%E7%BB%B4dba%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</guid>
      <description>-- 写在开始 linesize/pagesize set linesize 9999 set pagesize 9999 查看ORACLE SID
select * from v$instance select instance_name,host_name from v$instance; 查看数据库的运行天数
select INSTANCE_NAME,round(sysdate-STARTUP_TIME) run_day from gv$instance; 查看alert log
show parameter dump; 查看当前DB的scn
select dbms_flashback.get_system_change_number from dual; select current_scn from v$database; scn转时间戳
select scn_to_timestamp(991159) scn from dual; 时间转换为scn
select timestamp_to_scn(to_timestamp(&#39;2019-01-27 18:19:20.123456789&#39;,&#39;YYYY-MM-DD HH24:MI:SS.FF&#39;)) scn from dual; scn和时间的对应关系
select scn,to_char(time_dp,&#39;yyyy-mm-dd hh24:mi:ss&#39;) time from sys.smon_scn_time where rownum&amp;lt;10 order by 1,2; 查看数据库所有用户及用户状态
-- SYSADM用户是线上应用用户 select USERNAME,ACCOUNT_STATUS from dba_users; select current_scn from v$database; 查看存储过程数量</description>
    </item>
    
    <item>
      <title>什么是情绪价值</title>
      <link>/book/%E6%83%85%E7%BB%AA%E4%BB%B7%E5%80%BC/</link>
      <pubDate>Mon, 28 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%83%85%E7%BB%AA%E4%BB%B7%E5%80%BC/</guid>
      <description>下午在知乎上看到一段关于《情绪价值的问题》，记录下
回想一下周围较长时间的好友+交往较长时间的恋人，其实每段关系能长久维持必然是因为双方可以互相提供情绪价值，具体有以下几种：
1.治愈型价值：在我们伤心难过、迷茫心累的时候提供安慰和鼓励（比如失恋了找朋友哭诉，此时他就拥有了治愈你的情绪价值）
2.指导型价值：在我们不思进取、过度膨胀的时候提供考虑问题的新视角和解决问题的方法论（比如工作不顺找朋友聊天，他教你如何和领导同事相处，此时他就拥有了指导你的情绪价值）
3.分享型价值：在我们欣于所遇、需要知己的时候提供共同语言和思想碰撞（比如看到一首好诗找志同道合的朋友探讨，此时他就拥有了和你分享共同语言的情绪价值）
4.陪伴型价值：在我们空虚无聊寂寞冷的时候与我们作伴（比如陪着聊天，陪着吃饭，陪着出去玩，此时他就提供了陪伴着打发时光的价值）——这种价值有时效性，一旦异地了or有时差了，就自然而然得烟消云散了。
5.猎奇型价值：在我们平淡枯燥、无所事事的时候打开新世界的大门（如带从没有滑过雪的你滑雪，带从没参加过晚会的人参加晚会，此时他就拥有了带你尝试新事物新领域的情绪价值）
6.自娱型价值：在我们与他们的交往中能得到某些简单的快乐，且通常这种价值有附带的炫耀型情绪价值（如男友很帅、女友很美、朋友很牛逼，虽然没有什么实际的作用，但他拥有让人心里觉得满足的情绪价值）
7.怀旧型价值：因为之前有共同的经历且三观较为相符，在未来的生活中可以偶尔共同回忆往事形成怀旧型价值，从而形成长久稳定的低频率接触关系（如大学关系不错的朋友，虽然毕业去了不同城市，非密切联系，很难见面也很少互相倾诉，但仍然可以形成终生友谊）
8.自我实现型价值。在我们和他们的交往中，能够显示出自己的优越性获得嘚瑟的快感（如很多人喜欢和不如自己有钱有权有文化的人交往），或获得征服的快感（如追逐异性让对方爱上自己，或与人辩论）——这种价值也有时效性，一但对方比自己强了/被自己征服了/不配合自己炫耀，关系也会维持不下去。
人是受情绪支配的动物，无论是优秀还是平凡，都会需要安慰、指导、分享、陪伴和猎奇，所以每个人都是需要情绪价值的。
注意： 1.如果两个人能长久的从对方那里获取自己需要的情绪价值（关键是“各取所需”），那么不出意外这段关系就会一直维系下去。如果不能，只会有短期关系，而不会有长期稳态。
2.如果一方对另一方有情绪价值，而另一方对他却毫无情绪价值（关键是“毫无“），这就属于降维打击，这样的关系就算短暂存在也不会长久；至少互相各有一条，才能长期维系。
3.不要指望一个人提供所有的价值。正常情况下，每个人都会根据自己的性格和经历提供一条或几条。（比如，你有一个朋友和你一样喜欢钓鱼，他对你来说是“分享型价值”，除此之外你俩从来不聊别的，也聊不到一起去，但这不影响你们长期一起约着钓鱼。）
4.情绪价值是相对的，你对张三提供的价值，李四可能不感兴趣。即：你对张三来说很帅很美，对李四来说你平平无奇。所以很残酷的一点是：在有的人眼里，你就是毫无价值的，所以人家才不想理你。（比如，你长得小帅，是你们班班草，你给大家带来了“自娱型”价值，如果能把你追到手，你还会带来“自我实现型价值”，班里不少女生都追着你。但你去追林志玲就没戏，因为在林志玲眼里你毫无价值，你的那点帅气她周围满地都是）
5.抛开物质价值的情况下，情绪价值是真实的不是装出来的，装也装不了一辈子。
单独说说物质价值。
以上说的全都是不考虑物质价值的情况，但生活中很多关系确实是由物质价值维系的。物质价值和情绪价值是并列关系，同等重要，不存在高低之分。
（1）人可以为了物质委屈情绪，也可以为了情绪委屈物质。
为了攫取对方的物质价值（如乙方对甲方、下属对领导、情妇对金主等），大家愿意想办法拿出自己的价值来交换，要么提供物质价值进行利益互换，要么提供对方需要的情绪价值。而且，如果一直在“物质”上有求于人，甚至可以一直忍辱负重的持续输出情绪价值。
同样地，为了获取情绪价值，大家也愿意拿出物质价值来换（比如，有的领导喜欢被拍马屁，拍高兴了升职加薪也乐意）。
（2）性价值也属于物质价值。馋人家身子，和馋人家金钱、地位、资源等，本质上没两样。
（3）有一种物质价值是潜在物质价值。即：现在没有、但未来的某天可能会有。有时候为了这种物质价值，人们也愿意付出自己的一些价值，以备将来之需。（比如张三是某局局长，你虽然和他八竿子打不着，也不求他办事，但如果他需要你提供什么价值，你也不会拒绝，怕将来的某天会找他办事）
更新：再补充一个公式
在某人身上获取的总收益=总价值-总成本。
如果总收益≥0，关系可长期维系；反之关系必然破裂。这里的总成本=你付出的总价值（包括物质情绪）+特殊成本。
付出的总价值很好理解，不多说了，参考上面那堆即可。这里我主要想说一下“特殊成本”，其实主要就是道德成本，如良心谴责、社会舆论等。有两个典型的例子。
第一个是夫妻关系中的出轨。一个人但凡出轨，肯定是因为他能从第三者身上获取价值，这和上面说的都相符，也符合逻辑。但为什么不是人人都会出轨（假设所有人都能面临诱惑）？因为道德成本具有很强的个体差异性。有人道德底线很高，出轨给他带来的心理谴责太大，大到可以盖过出轨获得的总价值的地步，那他就不会出轨。但有人道德感薄弱，那这个成本对他来说就算不上成本，那他一旦可以通过出轨获取利益（物质/情绪），他就会连跑带颠的出轨。（所以大家找对象还是要找那种道德底线高的&amp;hellip;）
第二个是父母与子女的关系。父母和子女的关系比较特殊，因为直接血缘关系并不像普通亲戚关系，看不顺眼大不了不走动了。我更倾向于父母的这种直接血缘关系是一种硬性捆绑（因为投胎是无法选择的）。以下按父母情况分类：
（1）如果父母超级超级恶劣、提供不了任何情绪价值：在孩子有能力脱离父母独立生活之前，孩子因为物质价值也得听话。但是这种孩子一旦独立，很可能抛弃父母。这就是我说的“无物质价值+无情绪价值=无长期关系”。
（2）如果父母是正常父母甚至神仙父母：那即使孩子经济独立了，和父母之间也会存在怀旧价值+分享价值+指导价值等上面说的很多价值，可以一直维系，而且关系不错。
（3）最后一种比较特殊的，也是我想着重讲的，即父母很恶劣、但又不是超级恶劣。这种情况下，虽然孩子独立以后父母基本没有任何物质/情绪价值，但脱离关系的成本过高（包括心理压力、社会舆论等），基本还是会选择维持长久的关系。只不过是浮于表面、极其冷淡的关系。也就是我上面说的强制捆绑带来的特殊状态。其他亲戚/朋友/配偶，都不会出现这种情况。
首先感谢大家给了这么多赞，这里集中答评论区的几个问题：
1.有什么文献依据吗？
没有。这是我和朋友微信聊天的时候，我自己无意中聊到的话题。正好又在知乎看到了这个问题，我就顺手用当时聊天时候我想到的点回答了。（如果你觉得我真是一个小可爱，欢迎关注我！）
2.是原创吗？
必须是。我知乎粉丝才5000不到，也不做公众号引流，有抄袭的必要吗？
3.这和马斯洛修需求理论有什么异同？
马斯洛说的以一个人为出发点，对外界的所有需求的总和。各层呈递进关系，从生理需求、安全需求、社交需求、尊重需求、自我实现需求逐层递进的，即：前一个满足不了的情况下，人不会去追逐后一个需求。
情绪价值是对另一个人的需求。情绪价值和物质价值呈并列关系，我们不需要对方（一个人）同时给我们提供物质价值和情绪价值，也不需要他同时提供上述所有的情绪价值，每个人只要能提供至少1条，即可长期维持关系。
评论区有几个应用型问题，这里统一回答一下。
1.都为对方输出情绪价值，自己的不开心谁来消化？
在没有物质支撑的前提下，没有人会一直单方面输出情绪价值。情绪价值一定是在互换的，如果你觉得对方一直在白嫖你的情绪价值，只有两个可能：一、对方也对你输出了一些上述价值，但你没认识到；二、对方就是在白嫖你，你很快会忍无可忍不再输出下去，即这不是一段长期稳定的关系。
在考虑了物质价值的情况下，你输出情绪价值是为了攫取对方的物质好处，有不开心自己受着就行，应该的。当然，你也可以把在张三那里受的气，去治愈型价值的朋友那消化掉（不用感到不好意思，对方如果能和你长期保持关系，他一定也从你身上获取了他要的价值）。还是那句话，情绪价值是一个个体对个体的事情，不要指望有一个人能给你提供所有的价值。
2.简单的炮友关系算什么价值？女神对舔狗来说又算什么价值？
稳定的炮友关系（和包养、买东西都无关的前提下），是陪伴型价值（共度春宵）+自我实现价值（自己的肉体对对方有吸引力，是一件心理层面很有满足的事）+自娱型价值（简单的快乐获得了，对方美or帅就更娱了）
追求女神时，女神的肉体属于物质价值，就像你努力工作是为了拿到工资一样，你提供情绪价值不就是因为馋对方身子嘛？女神爱上你，那就属于自我实现型价值，即你通过努力得到类似狩猎的快感。
但我必须得在这里说一下，我不认可“舔狗”这种词汇，无关性别。我觉得真心喜欢一个人没什么问题，不该被冠以“舔狗”这样的贬义性词汇。被追逐者即使不喜欢对方，也大可不必如此贬低人家；围观群众即使不看好这段感情，也大可不必对追逐所爱的人这么大的恶意。
3.有什么方法能提高情绪价值？
在物质价值支配的前提下，人自然而然就会学会释放情绪价值；在不考虑物质的情况下，人不会长期伪装情绪价值。所以不用学，顺其自然，吸引适合你的人即可。
4.没有物质价值，情绪价值是不是就毫无意义？
能问出这问题的人，我觉得都没必要继续看我的这篇回答了……
就没有过一个真心的朋友，是不建立在金钱权力等等附属条件上交到的？无话可说。
5.和异性聊天，怎么互相调动情绪价值？（聊着聊着，对方就不愿意和我聊天了）
情绪价值的必要性体现在长期。在题干这种情况下，还只是关系的初期，根本不存在“互相调动”情绪价值这回事。
是你想和对方聊天，不是对方想和你聊天，证明可能在对方眼里，你的物质价值+情绪价值并不高。这种情况下，你能做的只有继续努力“释放”自己的情绪价值，让对方多了解你一点，看能不能吸引对方。如果还是吸引不了，请参照并接受上面的那一条“对有些人来说，你就是毫无价值的”。</description>
    </item>
    
    <item>
      <title>温柔的话</title>
      <link>/book/%E6%B8%A9%E6%9F%94%E7%9A%84%E8%AF%9D/</link>
      <pubDate>Thu, 17 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%B8%A9%E6%9F%94%E7%9A%84%E8%AF%9D/</guid>
      <description>如果你半夜醒了，就给我打电话，不要一个人玩手机。 没事，我不听他们说，我听你说。 我不会让你一个人安静的，你的小脑瓜是想不通的，我会哄你到服软，偏向你，让你心安。 不忙，没事，你说，我在 累了就向后倒，我在 其实，每次道别之后，我都有悄悄回头过。 别逞强，你是小朋友，扛不住可以哭。 你负责温柔，我负责守护你的温柔。 我爱你，你可以一遍又一遍地向我确认。 我们会吵架，但是我们会和好，我们会有误会，但是我们都会解释清楚，我们的爱只增不减。 你问我说爱有没有期限，山脚的野草一年又一年，我想陪你的不止是北方的冬天。 我不知道我什么时候开始喜欢你的，我只是记得见到你，我就突然不着急回家了。 你放心，我除了干饭就是想你。 回首亘年漫月里 所有怦然心动 你仍拨得头筹 我不是来讲道理的，我是来给我的小朋友撑腰的。 你什么都愿意和我讲，就是我最有安全感的时候。 如果你感到辛苦，那就转过来到我怀里躲一躲吧， 你在我这里永远有台阶下，永远有纸巾擦泪，永远有最真诚的鼓励和最柔软的怀抱。 你不是我权衡利弊后的选择，而是我怦然心动后，明知不可为而为之的坚定，这是我对这份感情最大的诚意。  这些都是网上看到的
这是我最近记下来的
我以前还记了很多笑话
放在一个文本里
想背下来散步的时候
说给她听
可惜记性太差
每次都在那想半天
挤不出来一个完整的笑话
而我这辈子说过最温柔的话是
不管你能不能理解我做的 也不去想你为什么要这样对我的 我依然会等你到... 就像年轻时候发生过的那样 你如果也还是没来 我就开始新的生活 那是我内心最柔软的时刻</description>
    </item>
    
    <item>
      <title>Sysbench做压力测试</title>
      <link>/ops/sysbench%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Sat, 12 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/ops/sysbench%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95/</guid>
      <description>0.环境 1.安装sysbench  curl -s https://packagecloud.io/install/repositories/akopytov/sysbench/script.rpm.sh | sudo bash yum -y install sysbench 2.执行压测 10.10.0.1 上执行 sysbench /usr/share/sysbench/oltp_read_write.lua --tables=5 --table_size=2000000 --mysql-user=dba --mysql-password******* --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-db=dbatest prepare sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=dba --mysql-password=****** --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-db=dbatest --tables=5 --table_size=2000000 --threads=300 --time=120 --report-interval=60 run &amp;gt;&amp;gt; /root/sb/proxy300.log sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=dba --mysql-password=****** --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-db=dbatest --tables=5 --table_size=2000000 --threads=20 --time=120 --report-interval=10 run sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=dba --mysql-password=****** --mysql-host=127.0.0.1 --mysql-port=3308 --mysql-db=dbatest --tables=5 --table_size=2000000 --threads=20 --time=120 --report-interval=10 run sysbench /usr/share/sysbench/oltp_read_write.lua --mysql-user=dba --mysql-password=****** --mysql-host=127.</description>
    </item>
    
    <item>
      <title>人生就像一列火车</title>
      <link>/book/%E6%A0%BC%E8%A8%80/</link>
      <pubDate>Thu, 10 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%A0%BC%E8%A8%80/</guid>
      <description> 人生就像一列火车 有人上车 有人下车 没有人会陪你走到最后 碰到了即便是有缘 即使到了要下车的时候 也要心存感激地告别 生命只是如此前行 不必说给别人听 只在心里最幽微的地方 时时点着一盏灯 —— 林清玄《心无挂碍 无有恐惧》  </description>
    </item>
    
    <item>
      <title>MySQL中show processlist详细解释</title>
      <link>/mysql/mysql%E4%B8%ADshow-processlist%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E4%B8%ADshow-processlist%E8%AF%A6%E7%BB%86%E8%A7%A3%E9%87%8A/</guid>
      <description>0.概况 MySQL执行SHOW PROCESSLIST命令后，显示的各个字段的含义如下所示：
   列 说明     id 一个标识，要kill一个语句的时候有用。   user 显示当前用户   host 这条语句是从那个服务器的哪个端口上发出的，可以用来追踪出问题语句的用户   db 当前连接连接使用哪个数据库   command 显示当前连接的执行的命令，一般就是休眠（sleep），查询（query），连接（connect）   time 状态持续的时间，单位是秒。   state 当前连接的sql语句的状态，很重要的列   info sql语句。    1.详细说明 id 一个标识，要kill一个语句的时候有用。
user 显示当前用户
host 这条语句是从那个服务器的哪个端口上发出的，可以用来追踪出问题语句的用户
db 当前连接连接使用哪个数据库
command (重要) 共有以下几种状态
 Command 说明 Sleep 休眠，但是连接保持，可以通过设置超时时间的变量，使得连接在超时时间之后，断开连接。 Query 查询。Query的范围包括Create、Insert、Select、Delete、Drop语句类型。 Connect 连接。建立连接过程。  time 状态持续的时间，单位是秒
state (重要)  Checking table 正在检查数据表。 Closing tables 正在将表中修改的数据刷新到磁盘中，同时正在关闭已经用完的表。 Connect Out 复制从服务器正在连接主服务器 Copying to tmp table on disk 由于临时结果集大于 tmp_table_size，正在将临时表从内存存储转为磁盘存储以此节省内存。 Creating tmp table 正在创建临时表以存放部分查询结果。 deleting from main table 服务器正在执行多表删除中的第一部分，刚删除第一个表。 deleting from reference tables 服务器正在执行多表删除中的第二部分，正在删除其他表的记录。 Flushing tables 正在执行 FLUSH TABLES，等待其他线程关闭数据表。 Killed 发送了一个kill请求给某线程，那么这个线程将会检查kill标志位，同时会放弃下一个kill请求。MySQL会在每次的主循环中检查kill标志位，不过有些情况下该线程可能会过一段时间才能死掉。如果该线程被其他线程锁住了，那么kill请求会在锁释放时马上生效。 Locked 被其他查询锁住了。 Sending data 正在处理 SELECT 查询的记录，同时正在把结果发送给客户端。 Sorting for group 正在为 GROUP BY 做排序。 Sorting for order 正在为 ORDER BY 做排序。 Opening tables 正尝试打开一个表。 Removing duplicates 正在执行一个 SELECT DISTINCT 方式的查询，但是MySQL无法在前一个阶段优化掉那些重复的记录。因此，MySQL需要再次去掉重复的记录，然后再把结果发送给客户端。 Reopen table 获得了对一个表的锁，但是必须在表结构修改之后才能获得这个锁。已经释放锁，关闭数据表，正尝试重新打开数据表。 Repair by sorting 修复指令正在排序以创建索引。 Repair with keycache 修复指令正在利用索引缓存一个一个地创建新索引。 Searching rows for update 正在将符合条件的记录找出来以备更新。必须在 UPDATE 要修改相关的记录之前就完成。 Sleeping 正在等待客户端发送新请求。 System lock 正在等待取得一个外部的系统锁。 Upgrading lock INSERT DELAYED 正在尝试取得一个锁表以插入新记录。 Updating 正在搜索匹配的记录，并且修改它们 User Lock 正在等待GET_LOCK()。 Waiting for tables 该线程得到通知，数据表结构已经被修改了，需要重新打开数据表以取得新的结构。然后，为了能的重新打开数据表，必须等到所有其他线程关闭这个表。 waiting for handler insert INSERT DELAYED 已经处理完了所有待处理的插入操作，正在等待新的请求。  info SQL语句</description>
    </item>
    
    <item>
      <title>DBA的工作评价标准</title>
      <link>/dba/dba%E5%B7%A5%E4%BD%9C%E8%83%BD%E5%8A%9B/</link>
      <pubDate>Tue, 22 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/dba%E5%B7%A5%E4%BD%9C%E8%83%BD%E5%8A%9B/</guid>
      <description>每大项有4-5个评价标准，分别给分，卓越(4.5),优秀(4),一般(3.5),待改进(3),较差(2.5) 综合得分计为工作能力得分
 这里不区分级别，资深DBA,高级DBA,DBA 按同一标准打分。
01.责任感和主动性 1:设定工作目标  卓越:有高度的责任感，自己设立挑战目标，并实现目标；在工作责任和任务上挑战自己的极限，为公司创造最大价值 优秀:以极大的责任心去 设立非常挑战性目标；主动要求承担更多责任和工作 一般:给自己设立挑战性目标；严格要求自己；主动要求承担责任和工作 待改进:给自己设立容易达到的目标，不严格要求自己；不主动要求工作和责任 较差:不设立目标，不要求自己；推托工作和责任  2:工作效率效果  卓越:效率非常高，紧迫感非常强，今日事今日毕。全力以赴保证完成任务。树立标准，积极地影响他人 优秀:效率非常高，紧迫感非常强，今日事今日毕。全力以赴保证完成任务 一般:效率高，紧迫感强，在限定时间能完成任务 待改进:做事效率不高，紧迫感不足，有时不能在限定时间能完成，偶尔出错 较差:做事拖沓、无紧迫感、无责任心，不能在限定时间能完成工作，时常出错  3:承担责任  卓越:有自我批评，自我激励的能力；追求卓越，在责任感/主动性上是员工典范 优秀:勇于承认错误而不找借口，积极改正错误；正面积极地影响他人 一般:承认错误不找借口，积极改正错误 待改进:有时不承认错误，有时责怪别人 较差:不承认错误，抱怨和指责别人；态度、言行消极地影响他人  4:奉献精神  卓越:愿意为公司/团队利益而牺牲个人利益 优秀:在任何情况下，关注公司/团队利益多于个人利益 一般:在大多数情况下，关注公司/团队利益多于个人利益 待改进:能平衡个人和公司/团队利益 较差:关注个人利益多于公司和团队  02.解决问题能力 1:解决问题  卓越:洞察工作和流程中的潜在问题，预先就可能出现的问题提出解决方案，避免问题的出现 优秀:解决用户问题时经常能让用户非常满意 一般:解决用户问题时经常能让用户满意 待改进:解决用户问题时有时能让用户满意 较差:不能够解决用户问题，不能让用户满意  2:履行工作职责  卓越:超出职责范围的解决公司面临的困难，并超额完成任务 优秀:能够履行职责并超额完成任务 一般:能够履行职责并完成任务 待改进:有时能够履行职责并完成任务 较差:不能够履行职责并完成任务  3:处理复杂问题  卓越:综合运用逻辑和直觉以获得最好的解决办法；快速和正确地解决问题；认识到产生问题的根本原因,一劳永逸的从系统或流程或人员上彻底解决 优秀:通过分析各种复杂数据，找出关键问题，得出理性的结论并付诸于行动，取得预期目标 一般:能够分析和解决复杂问题；能就新出现的问题提出合理解决办法 待改进:面临复杂问题时所提出方案不是充分必要；但会处理比较简单的问题 较差:在面对复杂问题时需要帮助；做事无轻重缓急  4:态度信心和持续改进  卓越:是某一领域的专家，有必胜信心，永远追求卓越 优秀:把持续改进和提高作为工作和人生态度；面对困难和挑战总是有办法 一般:有不解决问题不罢休的、积极的、有办法态度 待改进:碰到自己无法解决的问题积极向上反映；遇到挫折有时不能快速调解恢复积极心态 较差:对所碰到的问题无动于衷，不向上反映，不去寻求解决方案；遇到挫折容易丧失信心  03.</description>
    </item>
    
    <item>
      <title>MySQL复制故障修复_无主键表大事务卡住</title>
      <link>/mysql/mysql%E5%A4%8D%E5%88%B6%E6%95%85%E9%9A%9C%E8%A7%A3%E5%86%B3_%E5%A4%A7%E4%BA%8B%E5%8A%A1%E5%8D%A1%E4%BD%8F/</link>
      <pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%A4%8D%E5%88%B6%E6%95%85%E9%9A%9C%E8%A7%A3%E5%86%B3_%E5%A4%A7%E4%BA%8B%E5%8A%A1%E5%8D%A1%E4%BD%8F/</guid>
      <description>0.故障现象 生产环境MySQL复制报警，现象
 从库复制延时越来越大,gtid一直停留在固定的地方不变 从库的relaylog越来越大，1G以上，但是增长不明显。 从库当前没有业务访问，不存在资源紧张 主库上最近一段时间没有明显的大批量写入  1.原因定位  从上面的现象卡，基本上可以推断是大事务卡住了， 我看的时候法爷已经把relaylog解出来了，也很明显的看到有很多的delete。 根据以上推断我们去主库上查时间点的日志，发现了： 一个SQL是 delete from t1 where c1=&#39;&#39; 删除了65万行数据 于是问题定位：生产环境的windows机器上有同事用navicat删除了线上MySQL的数据。  简单的一个SQL ，但是因为一些原因综合在一起引起了雪崩
 不幸的是：这张表是个没主键的表，导致从库追日志进程卡住，无法正常执行 幸运的是：这些从库没有业务访问，没有造成实际影响  2.安全规范 首先：生产环境的windows机器安装navicat访问数据库这种行为，肯定是不被允许的，
但是因为“历史原因”我们依然有少量同学（不超过10人）有这种特殊需求。
原计划是3月底推动消除的，
经过此事以后，DBA会加快推进禁止在生产环境安装数据库客户端连接数据库这个规范。
有时候就是这样，觉得这个地方可能有风险，我们排个期来解决，通常就会没等到期限就先暴出来了
 问：为什么我们不用限制账号访问来源的方法？ 答：因为一些原因,加ip限制代价太大，且不利于未来的docker虚拟化。
 3.问题修复 共有3个从节点，我和另外两个DBA用了三种不同的方式来修复
方法一：我用的方法，就很暴力的在从库上reset master 再set 跳过这个事务 use db_test; truncate table t1; stop slave ; reset master; set @@GLOBAL.GTID_PURGED=&#39;59939d78-de2d-11eb-ac46-e43d1a074d20:16020676&#39; start slave; 方法二：法爷用了相对温和的方法，模拟一个事务的方法。 use db_test; stop slave; SET gtid_next=&#39;59939d78-de2d-11eb-ac46-e43d1a074d20:16020676&#39; truncate table t1; SET gtid_next=&#39;automatic&#39;; start slave; 我和法爷讨论了一下，相对来说这个是更安全的方法。保证了事务的连续，偷换了一个事务的内容</description>
    </item>
    
    <item>
      <title>MySQL5.7升级到8.0(一):SQL语法变化</title>
      <link>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B41sql%E8%AF%AD%E6%B3%95/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql5.7%E5%8D%87%E7%BA%A7%E5%88%B08.0%E7%9A%84%E5%8F%98%E6%9B%B41sql%E8%AF%AD%E6%B3%95/</guid>
      <description>研发：MySQL5.7升级到8.0(一):SQL语法变化
  DBA：MySQL5.7升级到8.0(二):配置和参数
   Note：这里面是升级到8.0,需要开发人员参与修改部分或注意部分
   不常用 废弃了 GROUP BY 分组的排序 ASC 和 DESC, 存储过程中包含此语法的无法正常执行;
  不常用 最新版可能不支持 &amp;amp;&amp;amp;, ||, ! 的语法, 需要使用标准 SQL 的 AND, OR, NOT 进行替换;
  不常用 外键的名字在整个 schema 中必须唯一;
  常用 支持公共表表达式cte, 窗口函数 不再支持5.6，5.7 的土方法实现递规这种写法废了！不能再用了
 SELECT * FROM(SELECT @rn:= CASE WHEN @id = id THEN @rn + 1 ELSE 1 END AS rownum,@id:= id as id, volume, dateFROM(SELECT * from table_001 WHERE fdate &amp;lt;= &#39;2022-02-16&#39; ORDER BY id, date DESC) a ,(SELECT @rn=0, @id=0) b )aWHERE rownum &amp;lt;= 5   不常用 支持备份锁(backup lock)</description>
    </item>
    
    <item>
      <title>MySQL的7种日志(一):概况</title>
      <link>/mysql/mysql7%E7%A7%8D%E6%97%A5%E5%BF%97/</link>
      <pubDate>Mon, 14 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql7%E7%A7%8D%E6%97%A5%E5%BF%97/</guid>
      <description>0.前言 和其他关系型数据库一样
MySQL也重度依靠数据库日志来完成一系统的功能。
总结了以下7种重要的日志。今天先简单给这7兄弟做个整体介绍。
接下来会分三个章节分别说清楚（binlog/redolog/undolog 这三位不省心的兄弟在做什么).
1.日志的分类 Binlog:  MySQL最重要的日志（没有之一），记录了所有的DDL和DML语句(除了数据查询语句select、show等)，以事件形式记录 DBA依赖它做：高可用方案，异构数据迁移，备份和恢复，误更新回滚。。。。等等 ，是整个MySQL的灵魂 研发/大数据人员依赖Binlog做数据订阅，数据同步  relaylog:  它是依赖于binlog的日志，格式也和binlog一样。 是MySQL复制进程把“别的实例的binlog&amp;quot;复制到本地后，就叫做relaylog. 作用是为了MySQL高可用复制服务的一种日志。  Slowlog:  慢查询日志用来记录在MySQL中响应时间超过阀值的语句，则会被记录到慢查询日志中。由long_query_time参数控制，默认值10秒， 一般线上环境，我们设置为:0.2秒 或0.5秒 两种标准 一般DBA通常会用脚本将日志收集归类，分析后对部分规则产生报警。 这个日志是文本类型的，打开就能看到，比较简单，很容易理解，也很有用  genlog:  一般轻易不开启，开启以后，会将所有经过的SQL都记录到日志里，非常费资源。 定位奇怪的问题时会用到。审计的时候也能用到。 不建议打开，定位问题后，要及时关闭  errorlog:  数据库产生warning,error时会打印的日志，实例启动失败了，或者实例崩溃了必看的日志。平时做好监控。建议开启死锁print，在errorlog中也能看到死锁信息。  redolog：  可能是最难理解的一个日志了，不同于上面的那些日志，redolog是innodb存储引擎的日志，不是MySQL自身的日志 redolog经常会和binlog/undolog搞混 记住最重要的一点：redolog是为了数据库突然关机或崩溃的时候用的。它的作用是：为了不丢失修改。 redolog通常是物理日志，记录的是数据页的物理修改（区别于binlog的逻辑修改)，而不是某一行或某几行修改成怎样怎样，它用来恢复提交后的物理数据页(恢复数据页，且只能恢复到最后一次提交的位置)。  undolog:  undo和redonlog一样也是是innodb存储引擎的日志，用来回滚行记录到某个版本。undo log一般是逻辑日志，根据每行记录进行记录。 它的作用除了和redolog一起保证数据库突然关机或崩溃的时候，数据不丢失，不混乱。它还是MVCC事务特性的重要组成部分。  小结： 简单介绍完7种日志，其中的三个日志（binlog/redolog/undolog) 涉及知识点非常多，会分别写一篇，慢慢聊。
&amp;ndash; done</description>
    </item>
    
    <item>
      <title>Oracle实时慢日志收集和展示</title>
      <link>/oracle/oracle%E6%80%8E%E6%A0%B7%E5%AE%9E%E6%97%B6%E6%94%B6%E9%9B%86%E5%B1%95%E7%A4%BA%E6%85%A2%E6%9F%A5%E8%AF%A2/</link>
      <pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/oracle/oracle%E6%80%8E%E6%A0%B7%E5%AE%9E%E6%97%B6%E6%94%B6%E9%9B%86%E5%B1%95%E7%A4%BA%E6%85%A2%E6%9F%A5%E8%AF%A2/</guid>
      <description>Oracle怎样实时收集展示慢查询？ 当网上搜相关问题或问一个身边的OracleDBA ，通常会有两种类型的答案
 看awr报告 扔过来一段SQL脚本。   类似于MySQL的慢日志方案：Slowlog--logstash--&amp;gt;es--&amp;gt;MySQL--&amp;gt;web页
但Oracle没有现成的slowlog可以用。
 所以我们先后采用尝试了以下三种方法。
方法一：定时收集awr报告 我们去年和黄老师一起尝试了定时生成awr报告，获取topSQL入库后，再收集展示的方法，这个方案需要对OracleAWR的缓存表理解得很深入，黄老师哼哼哧哧花了一个月时间，最终完成了统一收集，并通过web页展示，但总体效果不好。 主要缺点在于
 不能做到实时，甚至不能准实时（取决于多长时间生成一次awr报告） 只能每个实例取topSQL 方案呆板且不理性 总结：实现难度大，效果差  方法二：Oracle中间件记录慢查询 去年年底的时候我们在写一个Oracle中件间，业务访问Oracle数据库需要连接到Proxy上，然后在Proxy上记录查询日志，这种方法理论上是完全可以的，但是当我去尝试去实现这个功能的时候发现很困难，因为我们采用了端口中转的方式，可以抓到客户端和服务端的通信包，但是不能对应上这些通信包的对应关系。这就导致计算SQL执行时间这一步没法实现。 然后我们架构组也在做jdbc层的中件间，这个是100%可以轻松实现的，但只适用于java程序，对于非java代码访问数据库就无能为力了 总结：收集得很准，但开发工作量大，需要很长的时间
方法三：定时直接读取正在执行的SQL 这个方案是我们年前做了一个Oracle长时间无响应SQL的报警功能（超过60秒没执行完的SQL会发钉钉报警），然后一次故障处理时，法爷说这个功能改一下刚好可以做慢查询收集整理。
于是我们快速的试了下这个方案（代价很小，花了半个小时就完成了，任务配置和报表展示）
 新建存放慢SQL信息的表一个字典表/一个慢SQL表（5分钟） 配置一个每分钟去所有Oracle实例上收集的任务（15分钟） 配合已有CMDB信息，完成慢SQL和研发负责人的对应，写一个SQL （5分钟） 用这个SQL配置出来一张可展示的报表，设置成全体研发可见（5分钟） 总结：零开发工作，只要建个表，配置一个定时job和报表，就可以完成，效果不错  得到这样的一张这样的：实时展示Oracle慢查询的报表 &amp;ndash; done</description>
    </item>
    
    <item>
      <title>2021年终总结:在一切变好之前</title>
      <link>/book/2021%E5%B9%B4%E6%80%BB%E7%BB%93%E5%9C%A8%E4%B8%80%E5%88%87%E5%8F%98%E5%A5%BD%E4%B9%8B%E5%89%8D/</link>
      <pubDate>Mon, 31 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/book/2021%E5%B9%B4%E6%80%BB%E7%BB%93%E5%9C%A8%E4%B8%80%E5%88%87%E5%8F%98%E5%A5%BD%E4%B9%8B%E5%89%8D/</guid>
      <description>在一切变好之前  农历腊月二十九除夕夜 晚上17:30 家家户户团圆的日子 躺在出租屋里 中午点的外卖吃完了在桌子上还没收拾 打了一天的王者 头有点晕 屋里有点黑有点安静 带着点冷清和荒凉 这一年经历了很多事 很努力的去完成了一些计划 岁末年关 想一个人安静一下 享受这独处的时间和空间 这一年 太不容易了 但我努力的去做到了 终于可以放心的去追求 得到了我想要的自由 虽然自由的代价有点大 但都是值得的 这是对逝去的青春的一次回头 此时此刻 坐在黑暗的屋里 让仅有的一点电脑屏幕亮起的光 照着一张憔脆又满是期待的脸 在等着那一时刻的到来 还有3个星期 一切都会变好的  工作  换工作了 离开了工作10几年的旅游行业 病毒太厉害了 旅游业短时间看不到希望 来了不算熟的货运卡车行业 开始带领一个人很多的部门 不喜欢做管理 觉得管人是件很麻烦的事 但是还是接受了挑战 目前为止 做得不错 牺牲了大量的周末时间 快速的完成了新的DBA运维平台搭建工作 几乎是立杆见影的解决了团队困局 新同事们也很给力 一起快刀砍乱麻的整了好几个大活 团队中有些同学非常给力 也有些同学负责带动氛围 都觉得还不错 整个冬天因为个人原因 情绪不佳 感觉和大家有些隔阂了 直观的感觉是信任感在下降 需要改变一下  生活  终于还是迈出了艰难的一步 恢复了单身 不知道这算不算是对彼此的解脱 终究是辜负了信任 年轻时许下的诺言 也真的就没有做到 分开的过程很平淡 因为心怀愧疚 没有去争 她要的都给了 付出的代价是 我现在一个人漂在外面 连租房子的钱都是借的 欠了很多钱 未来可能要3，4年才能还清 当然这些并不足以弥补 还是会觉得亏欠 但不知道如何弥补 生活有时候就是这么无奈  风景  前天去了趟北京环球影城 人山人海不像是疫情该有的样子 据说这还是人少的时候 里面的项目设计得确实比较刺激 对恐高的我更是震憾无比 但是吧 还是少了很重要的东西 玩得并不是很开心 今年还去过其他很多地方 看过很多风景 比如无锡三国城里的吴国婚礼 比如华谊兄弟影视城里的通天帝国 比如江南水乡乌镇的乌篷船 但今年最美的风景 是西京湾的花海 各花入各眼 那片花海是最美的 以后每年的春夏之交 会再去看看 去吹吹那湖边的风 就像09年夏天所期待的那样 而现在最期待的是今年春天 去趟黄山 去新安江边看看满天的油菜花 去看看青山绿水最美的样子 去看最美的风景 时间它滴滴答答的走着 同行的人啊 你准备好了吗  2022-01-31 18:00 石门苑</description>
    </item>
    
    <item>
      <title>分布式一致性协议:raft协议</title>
      <link>/dba/raft%E5%8D%8F%E8%AE%AE/</link>
      <pubDate>Mon, 24 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/raft%E5%8D%8F%E8%AE%AE/</guid>
      <description>关于raft的起源和历史  raft协议是一种分布式强一致性协议  为什么要有一致性？  1.数据不能存在单个节点（主机）上，否则可能出现单点故障 2.多个节点（主机）需要保证具有相同的数据。   都有哪些一致性协议（算法）  Paxos ：强一致性，由Lamport出品，例如：腾讯的PhxSQL，阿里的OceanBase数据库 Raft ：强一致性，由Paxos改进而来，例如：redis的sentinel,etcd数据库 用的是raft协议 ZAB ：强一致性，由Paxos改进而来，例如：ZooKeeper Gossip协议：弱一致性或者叫最终一致性，例如：rediscluster协议     2014年，由斯坦福大学Diego的一篇200多页的博士论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》提出的一种全新的一致性协议。英文好的可以去看看。网上也有好多翻译成中文的 虽然核心协议上基本都是师继Paxos协议，基于多数派的协议。但是 Raft 一致性协议的贡献在于，定义了可易于实现的一致性协议的事实标准。模块化拆分以及设计简化。使分布式协议更加容易理解和实现  名词一： 复制状态机(Replicated state machines)  为了简化和便于理解，raft协议提出了复制状态机的概念，将集群中的节点都当成一个复制状态机，每个状态机只有三种状态。 复制状态机(Replicated state machines) ： 将集群中的每个服务器看做一个状态机, 它们接收外部的指令, 进行状态的改变, 所谓保持分布式一致性即是保证集群中所有状态机的状态一致性。 在任何时候，每个服务器都处于以下三种状态中的一种: 领导人(leader): 处理所有客户端的交互，日志复制同步，任何时候最多有一个领导人 跟随者(follower): 完全被动（不发出RPC，响应传入的RPC） 候选人(candidate): 用于选举新领导者 我们用图来解释这三种状态的变化关系  名词二： 任期和选举 为了判断过时的信息，过时的leader，raft协议提出了任期(term)的概念
 时序被分割为多个领导者任期 每个任期最多1个领导者 有些任期没有领导者（如：上图上的第2个阶段选举失败，但是任期值还是会加1） 每个服务器维护当前任期的值  1.什么时候开始选举  一个正常运行的系统，领导者必须不断的发送心跳（AppendEntries RPC）以保持其领导者的地位 如果在electionTimeout时间内（一般是100-300ms)，跟随都未收到RPC: 跟随者假设领导者已经崩溃，开始新的选举  2.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（四）:性能测试和改进</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B44/</link>
      <pubDate>Tue, 18 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B44/</guid>
      <description>续： 从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》 从零写一个兼容MySQL/Oracle的Proxy中件间（二）:SQL捕获和改写 从零写一个兼容MySQL/Oracle的Proxy中件间（三）:MySQL协议捕获和转发
1.过去的三个文章我们实现了以下功能]  Oracle登录捕获：捕获了Oracle通信协议中的用户登录包 Oracle用户解析：抓到了用户传用户名和密码的内容（密码是加密串） SQL请求包：同时通过对比，确定了用户发送SQL请求的通信包 OracleSQL日志：分析这些包，把SQL语句拿出来，记到日志里。 OracleSQL改写：用户发起的SQL 经过中间层改写到了服务端收到的是另一个SQL执行返回结果。 MySQL兼容：增加配置文件，使中间件可以支持两种数据库 MySQL协议解析：将经过proxy的MySQL包里的SQL语句解析出来，记录到日志  在没更新的这几天里我又偷偷完成了配置变更等小功能。现在中件间其实已经在理论上可以发布使用了
在投入使用前，在测试环境对这个半成品的中件间做了些基准测试。
在测试环境上生成了5张表，每张表200万行数据，对其进行直连和proxy模式压测。
以下是测试报告： 结论是：加了Proxy，性能下降了14% ，在情理之中，一般的SQL中间层因为多了层中转，响应时间会降低20ms左右。tps/qps在不做连接池的情况下会下降10%。 分析性能下降的原因：
因为在proxy存把经过的网络包都拆开来分析其中的内容，且把SQL语句存在日志里，这些步骤是比较费资源和时间的。
为了提升Proxy性能，降低中间层的性能影响，我们加了个配置参数
cat /data/proxy/conf/proxy3308.cnf [basic] logfile = /data/proxy/log/3308.log daemon = true [proxy] proxytype = mysql bind = 0.0.0.0:3308 server = 127.0.0.2:3308 isssl = false iscatchquery = false #增加是否“拆包” false时，Proxy进入高性能模式 iscatchlogin = false maxsquerylsize = 16384 当 iscatchquery=false时，Proxy进入高性能模式
if Iscatchquery { #只有iscatchquery为true时才解析包。 switch ProxyType { case &amp;quot;mysql&amp;quot;: log.Printf(&amp;quot;mysql:sqlPipeMySQL\n&amp;quot;) sqlPipeMySQL(srcCon, dstCon) case &amp;quot;oracle&amp;quot;: log.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（三）:MySQL协议捕获和转发</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B43/</link>
      <pubDate>Mon, 10 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B43/</guid>
      <description>续： 从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》 从零写一个兼容MySQL/Oracle的Proxy中件间（二）:SQL捕获和改写
1.过去的两天我们实现了以下功能]  Oracle登录捕获：捕获了Oracle通信协议中的用户登录包 Oracle用户解析：抓到了用户传用户名和密码的内容（密码是加密串） SQL请求包：同时通过对比，确定了用户发送SQL请求的通信包 OracleSQL日志：分析这些包，把SQL语句拿出来，记到日志里。 OracleSQL改写：用户发起的SQL 经过中间层改写到了服务端收到的是另一个SQL执行返回结果。   MySQL兼容：增加配置文件，使中件间可以支持两种数据库 MySQL协议解析：将经过proxy的MySQL包里的SQL语句解析出来，记录到日志  开始动手：
步骤一：中件间可以同时支持MySQL和Oracle 中件间的配置应该放在哪，理论上是想放在MySQL或zk里，当配置有变更的时候，中件间获得变更，但这个实现有点麻烦，可能得写好久，就先一个本地的配置文件
准备一个配置文件
proxy] proxytype = mysql bind = 0.0.0.0:1106 server = 10.26.*.*:3307 isssl = false iscatchquery = true iscatchlogin = false maxsquerysize = 4096 [proxybak] #proxytype = oracle #bind = 0.0.0.0:1106 #server = 10.26.*.*:1521 #isssl = false #iscatchquery = true #iscatchlogin = false #maxsquerylsize = 4096 然后在通信进程中收到包时处理
func (t *Proxy) pipeSend(dstCon, srcCon *Conn, chSend chan int64) { defer pipeClose(dstCon) switch ProxyType { case &amp;quot;mysql&amp;quot;: log.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（二）:SQL捕获和改写</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B42/</link>
      <pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B42/</guid>
      <description>续上一篇： 从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》
0.前言 昨天的文字里写开发这个中间件的原由和要解决的问题，有朋友留言
网上有现成的开源中间件为啥不用。
 答：网上有很多MySQL的中件间，Oralce目前还没有可以免费使用的中件间. 这可能就是开源和闭源的差别。
 Oracle自带的功能已经可以实现想要的功能（高可用/审计日志）
 答：
 Oracle官方的高可用方案RAC，无疑是非常非常非常优秀的,但我们现有的硬件不支持做跨机房RAC,以及我们迁移时需要proxy中间层来降低业务中断时间。 Oracle的审计日志太笨重/不支持慢日志/不支持SQL黑名单。   1.昨天我们实现了以下功能]  捕获了Oracle通信协议中的用户登录包 抓到了用户传用户名和密码的内容（密码是加密串） 同时通过对比，确定了用户发送SQL请求的通信包   SQL日志：分析这些包，把SQL语句拿出来，记到日志里。 SQL改写：用户发起的SQL 经过中间层改写到了服务端收到的是另一个SQL执行返回结果。  开始动手：
步骤一：从Oracle通信包中分解出SQL语句 已知有以下两种head的包是在传递SQL
0x1 0xf 0x0 0x0 0x6 0x0 0x0 0x0 0x0 0x0 0x11 0x6b 0x4 0xa5 0x10 0x0 0x0 0x35 0x1c 0x0 0x0 0x1 0x0 0x0 0x0 0x3 0x5e 0x5 0x61 0x80 0x0 0x0 0x0 0x0 0x0 0x0 0xfe 0xff 0xff 0xff 0x1 0x0 0x0 0x0 0x6 0x0 0x0 0x0 0x0 0x0 0x3 0x5e 0x6 0x61 0x80 0x0 0x0 0x0 0x0 0x0 0x0 0xfe 0xff 0xff 0xff 0xff 0xff 0xff 0xff 0x24 0x0 0x0 0x0 0xfe 0xff 0xff 0xff 0xff 0xff 0xff 1.</description>
    </item>
    
    <item>
      <title>从零写一个兼容MySQL/Oracle的Proxy中件间（一）《初识Oracle的通信协议》</title>
      <link>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B41/</link>
      <pubDate>Wed, 05 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/proxy/%E4%BB%8E%E9%9B%B6%E5%86%99%E4%B8%80%E4%B8%AA%E5%85%BC%E5%AE%B9mysqloracle%E7%9A%84proxy%E4%B8%AD%E4%BB%B6%E9%97%B41/</guid>
      <description>0.前言 MySQL由于开源的原因，有各式各样的中件间Proxy ，极大的丰富了做高可用或迁移的方案，习惯了MySQL生态圈的灵活和便利，Oracle官方不开源代码和协议，没有中间件proxy，显得很笨重。
比如以下的方案就会很不好办：
 实时抓取Oralce的访问SQL日志 慢日志捕获和收集 高可用中件间Proxy在故障时自动切换 SQL访问黑名单。  基于以上的一些困难，打算自己从头写一个兼容MySQL/Oracle的中件间，希望从中件间层同时支持两种数据库。方便我们做数据库的高可用管理和从Oracle到MySQL的迁移。
这个计划是在年前的2021年最后一次组内会议上提出来的构想。元旦放假期间我就一直在想这事怎么搞
问题的难点在于：Oracle的client/server端通信没有文档的说明，没人能说清楚Oracle是怎么交互的。
这两天用最原始的方法抓包，一个包一个包的去看，找到包的规律，分析它的通信协议。竟然发现这个方法可行
1.步骤 1.写一个Python脚本去连接（192.168.1.1:1521）上的Oracle  #!/usr/bin/env python ## coding: utf-8 import cx_Oracle conn = cx_Oracle.connect(&#39;dboopreader/dbooppassword@192.168.1.1:1521/testdb&#39;) print(&amp;quot;连接成功&amp;quot;) conn.close() print(&amp;quot;连接关闭&amp;quot;) 通过wireshark抓包，发现一次简单的连接，有38个通信包。
2.捕获这些包，发现它的规律 挨个点开这些包，发现了一些有用的信息，然后发现wireshark的包看起来不方便， 本地模拟一个端口1522端口，劫持这些请求，打印出来，得到如下这种的tcp包
抓到:127.0.0.1到192.168.1.1的包 二进制展示如下: 0.0x7 0xaf 0x0 0x0 0x6 0x0 0x0 0x0 0x0 0x0 0x2 0x54 0x3 0x54 0x3 0x3 0x2a 0x6 0x1 0x1 20.0x1 0x6f 0x1 0x1 0xc 0x1 0x1 0x1 0x1 0x1 0x1 0x1 0x7f 0xff 0x3 0xe 0x3 0x3 0x1 0x1 40.</description>
    </item>
    
    <item>
      <title>分布式数据库</title>
      <link>/dba/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/</link>
      <pubDate>Mon, 03 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%BA%93/</guid>
      <description>分布式CAP原则 CAP定理是NOSQL数据库的基石，指的是在一个分布式系统中，Consistency(一致性)、Availability(可用性)、Partition tolerance(分区容错性)这三者只能满足其中两个，不能全部满足，这个结论是实践的总结，但是理论上，如果网络很好的情况下，三个特性可以同时满足。现实中的分布式系统因为网络的原因，很有可能会出现延迟或丢包等问题，因此必须要实现分区容忍性，一致性和可用性之间只能二选一。 对于传统的数据库来说，需要强一致性；而NoSQL系统一般注重性能和扩展性，而非强一致性。
 Consistency(一致性)：分布式数据系统中的所有的数据备份，在同一时刻值都相同。也就是同一时刻所有机器上的数据保持一致。 Availability(可用性)：当集群中的一部分节点故障后，集群整体还能响应客户端的请求。也就是说，每个用户的请求都能收到正常的响应，并且响应时间在用户可接受范围内。 Partition tolerance(分区容错性)：尽管节点之间网络通信丢失或延迟了任意数量的消息，但是整个系统仍然在正常运行。  可以有三种组合：
 CA：指的是单点集群。 CP：舍弃了可用性，可用性指的是高性能，所以性能不是很高。 AP：是弱一致性，一般的NoSQL数据库对一致性的要求不是很高  分布式的一致性  为什么要有一致性？  1.数据不能存在单个节点（主机）上，否则可能出现单点故障 2.多个节点（主机）需要保证具有相同的数据。   都有哪些一致性协议（算法）  Paxos ：强一致性，由Lamport出品，例如：腾讯的PhxSQL，阿里的OceanBase数据库 Raft ：强一致性，由Paxos改进而来，例如：redis的sentinel,etcd数据库 用的是raft协议 ZAB ：强一致性，由Paxos改进而来，例如：ZooKeeper Gossip协议：弱一致性或者叫最终一致性，例如：rediscluster协议    数据分片 当数据量过于庞大，单机难以支撑时，会面临扩展瓶颈，那么就需要将数据进行拆分，分散在多个数据库实例上。
数据分片是指将数据全局划分为相关的逻辑片段，有水平切分、垂直切分、混合切分三种类型。
 水平切分:可以简单地理解为按照数据行进行切分，即一部分行放在某数据库，另外一部分放在另外的数据库实例。比如可以按照时间、地区拆分，亦或是根据hash进行拆分。 垂直切分：垂直拆分可以简单理解为按照表进行分类，将表分布在不同的节点上,基本目标是将使用频繁的属性聚集在一起 混合切分：水平切分与垂直切合的结合。  数据分片的基本原则
 完备性条件 可重构性条件 不相交性条件  垂直分片后将数据组合起来需要执行连接运算，比水平分片后的数据组合要困难一些。
分布式查询处理及优化 分布式数据库需要考虑查询问题，其需要做的就是把一个分布式数据库上的高层次查询映射为本地数据库上的操作，最后通过网络通信，将操作结果汇聚起来。
相对于集中式数据库，分布式数据库还要考虑额外的几个问题：
 选择最优站点查询 数据传送方式 站点之间交换数据的问题 相对于集中式的查询目标，分布式需要多考虑一项 “通信开销代价”  对水平分片的优化
 尽量把选择条件下移到分片的限定关系处，再把分片的限定关系与选择条件进行比较，去掉它们之间存在矛盾的相应片断。 如果最后剩下一个水平片断，则在重构全局关系的操作中，就可去掉“并”操作.  对垂直分片的优化
 把垂直分片所用到的属性集，与查询条件中的投影操作所涉及的属性相比较，去掉无关的垂直片断。 如果最后只剩下一个垂直片断与查询有关时，去掉重构全局关系的**“连接”**操作（至少可以减少“连接”操作的次数）  基于半连接算法的查询优化</description>
    </item>
    
    <item>
      <title>从网上抄来的一些人生道理</title>
      <link>/book/%E4%BB%8E%E7%BD%91%E4%B8%8A%E6%8A%84%E6%9D%A5%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%BA%E7%94%9F%E9%81%93%E7%90%86/</link>
      <pubDate>Mon, 22 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/book/%E4%BB%8E%E7%BD%91%E4%B8%8A%E6%8A%84%E6%9D%A5%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%BA%E7%94%9F%E9%81%93%E7%90%86/</guid>
      <description> 是骄傲、虚荣、嫉妒和报复，支撑你走到今天。你的成长依赖的是这些负能量，而非天生的善良。 赚钱就是最大的正能量，赚钱也是最大的厚道。任何人赚钱的姿势都很难看，但努力养家糊口的样子，值得每个人去欣赏。 大喜大悲，看清自己；大起大落，看清朋友。 一定要珍惜自己的低谷期，那时候什么都是真的。 人因受到的伤害而变得成熟，而不是因为年龄的增加。 真正坚持到最后的人靠的不是激情，而是恰到好处的喜欢和投入。 自身价值才是一切，所有关系破裂的本质，都是期望落空。 平静的湖面孕育不出精悍的水手，安逸的环境创造不出时代的伟人。 莽撞地开始，拙劣地完成，也好过因为心怀完美主义而迟迟不动手去做。 一个人的生命质量，就等于这个人的认知质量，并且一分一毫都不会差。 在成事的道路上别怕拥挤，因为大多数人连基本功都不过关，而且也不够勤奋。 成功带来的只是一种被包裹的虚幻，只有从失败和错误中，人们才能得到真正的经验和教训。 重新开始永远都不晚，就算是周六的凌晨2点，也可以成为你新的起点，时间是一种幻觉。 人生最幸福的事，就是你所爱的、你所擅长的、世界所需要的，全都重叠，并且有人愿意给你付钱。 主动探索，主动发现，对自己的经历像公司档案那样记录，并定期复盘反思总结，你的人生会进步飞快。 能长久走下去的人，一定是志同道合的良师益友。除此之外的朋友，只是不同人生阶段的同路人而已。好聚好散。 凡是听不得残酷真相的人，一般都活在婴儿阶段。人的成熟，一半是对美好事物的追求，一半是对残酷真相的接纳。 人先要自保，才有更多力量做其他事。人先对自己负责，对家人负责，才可能做其他事，而不是做一个热心肠的人。 要么努力到出类拔萃，要么就懒得乐知天命。最怕你见识打开了，可努力又跟不上，骨子里清高至极，性格上又软弱无比。 如果你和自己没有什么深仇大恨的话，请整理一下你那脏乱的屋子，远离那些不思进取的小群体，忘掉得不到的旧人，请你好好爱自己。 当你所处的群体，普遍在智力、道德、经验、思考等等任何一个方面都没办法赋予你一个榜样和目标的时候，合群的意义，也就不存在了。 真正阻碍我们学习的，不是那些我们未知的知识，而是那些我们已知的东西。因为已知的东西会构成我们的思维定势，而思维定势会让我们排斥新的东西。 你并不需要做第一，也不需要做第二，你甚至不需要跟他们竞争。只需要超过绝大部分又懒又笨的人，你就可以过上相对较好的生活，然后在这个基础上迭代改进。 无论是你原地升职，还是跳槽高就，都需要成绩和作品的支撑。保饭碗是小家子气，不会成事的，只能是越混越差。不要怕被利用，不要怕被摘桃子，在提升核心能力的前提下，大胆去冲吧。 生活中大多数人是成不了事的，如果你下定决心要做出一些成就，就必须时刻警惕，不要被身边人同化。想有所成就，注定要走一条孤独的路，注定要面对多数人的质疑与反对。你必须直面这种痛苦，承受它，没有其他办法。 你在塑造环境，环境也在塑造你。当一个人要改变时，会被环境视为异类，出现排斥反应。这时你需要强大的定力：他们说他们的，你做你的。不用解释，也不用争论，把时间用在正事上。等你做成了，怎么说都对；做不成，怎么说都错。 在体力和精力耗尽之后，人太容易沉迷于丝毫不需要动脑的消费内容了。这个时候，人没有创造力、没有思考状态、无法关注长远的事、更不可能跳出此刻的局面探索更好的生活选项。世界又有这么多聪明的人想方设法地设计出各种速食垃圾引诱你，想不投降都是很难的。 我们需要的越少，我们就越近似于神。面对美女，不想和她恋爱，你就不会成为舔狗；面对成功人士，你不渴望什么人脉或虚荣，即使他再有钱有权，你也一样不卑不亢。 放弃向他人证明自己，放弃向自己证明自己。专注忘我地去做你应该做的事情，心无旁骛地去解决问题。当你脚踏实地的走自己的路时，那种拼命想要证明什么的冲动就会越来越少。你也会因此变得轻松、自由。 你今天怕走冤枉路，明天怕被割韭菜……小账算的精，大账不会算，不去摸清规则和积累经验，只会在重要的节点错失机会。 你不跳出舒适区，不为知识付费，把薪酬都用在吃喝玩乐的陷阱里，日复一日地随波逐流，突破口只会离你越来越远。  </description>
    </item>
    
    <item>
      <title>服务器过保日期批量查询python</title>
      <link>/ops/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%87%E4%BF%9D%E6%89%B9%E9%87%8F%E6%9F%A5%E8%AF%A2python%E8%84%9A%E6%9C%AC/</link>
      <pubDate>Sat, 09 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%87%E4%BF%9D%E6%89%B9%E9%87%8F%E6%9F%A5%E8%AF%A2python%E8%84%9A%E6%9C%AC/</guid>
      <description>浪潮服务器过保查询  import json import urllib import requests # def chenck_hardware_info(sn): url = &#39;https://www.inspur.com/eportal/ui&#39; sn_file = &amp;quot;/tm/sn.text&amp;quot; def request_datatime(sn): params = { &amp;quot;struts.portlet.action&amp;quot;: &amp;quot;/portlet/download-front!serverConfig.action&amp;quot;, &amp;quot;sn&amp;quot;: sn, &amp;quot;src&amp;quot;: &amp;quot;inspur&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;CN&amp;quot;, &amp;quot;pageId&amp;quot;: &amp;quot;2317460&amp;quot;, &amp;quot;moduleId&amp;quot;: &amp;quot;82efecfc33da48b4a66567cb3dcbe5f3&amp;quot; } headers = { &amp;quot;Referer&amp;quot;: &amp;quot;https://www.inspur.com/eportal/ui?pageId=2317460&amp;quot;, &amp;quot;Cookie&amp;quot;: &amp;quot;JSESSIONID=****; ........(这里写cookie地址)&amp;quot; } r = requests.post(url, headers=headers, params=urllib.parse.urlencode(params)) resp = r.text[1:len(r.text)-1] resp = json.loads(resp) #print(resp[&#39;Date&#39;]) return resp[&#39;warranty1&#39;] def post_info(sn): r1 = request_datatime(sn) url = &amp;quot;http://cmdbbackend.dev.tujia.com/api/inventoryitem/sn/update/life&amp;quot; headers = { &amp;quot;OPS-Token&amp;quot;:&amp;quot;IHmioqYhb0XgBAsEiHeK_guibinw&amp;quot;, &amp;quot;Content-Type&amp;quot;:&amp;quot;application/json&amp;quot; } data = [{ &amp;quot;serialNo&amp;quot;: sn, &amp;quot;contractPeriod&amp;quot;: r1 }] r = requests.</description>
    </item>
    
    <item>
      <title>数据库备份管理制度</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E7%AE%A1%E7%90%86%E5%88%B6%E5%BA%A6/</link>
      <pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%87%E4%BB%BD%E7%AE%A1%E7%90%86%E5%88%B6%E5%BA%A6/</guid>
      <description>备份策略  数据库备库节点上启用定时任务，每天0:10～6:10 全备（或增备)文件 至本地sas盘，每个实例压缩成一个备份文件。 生成的备份文件调用接口上传状态并断点续传止主备份备机 数据库备份在本地sas盘上保留1-3天的备份文件 辅备份机在0点-22点保持当天的备份文件夹和主备份机的同步 主备份机每天22:00 将当天的接收到的备份文件移至 yyyyMMdd 目录下 所有备份机每天23:00删除30天以上的过期备份文件  每个月的第一次数据库全备，永不过期删除（例如：如果db每天一次全备，则每月的1号备份永久保留）    备份周期  MySQL：每天1次全备，15分钟同步一次binlog日志，全备和日志保留30天 Oracle:每周1次全备,其余天数增备，实时保存日志，备份和日志保留30天 MongoDB:每天1次全备，全备保留30天  备份脚本（源机） 传输脚本（辅备份机） </description>
    </item>
    
    <item>
      <title>孤岛备份机和勒索病毒</title>
      <link>/dba/%E5%AD%A4%E5%B2%9B%E5%A4%87%E4%BB%BD%E6%9C%BA%E5%92%8C%E5%8B%92%E7%B4%A2%E7%97%85%E6%AF%92/</link>
      <pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E5%AD%A4%E5%B2%9B%E5%A4%87%E4%BB%BD%E6%9C%BA%E5%92%8C%E5%8B%92%E7%B4%A2%E7%97%85%E6%AF%92/</guid>
      <description>勒索病毒 什么是勒索病毒？  勒索病毒就是那种中毒后 加密你的文件（通常是aes加密算法) 提示你去支付一些电子货币才能解开文件的一种病毒 通常是要求支付一定数量的比特币 像下面这种  怎么写一个勒索病毒？  如果让我写一个简单的勒索病毒 我可能会这样写 0.像指定的服务器（控制机）请求一个aes公钥 1.用这个公钥挨个给本地文件加密  1.1 遍历本地所有文件 1.2 给每个文件头加上一个特殊标记（不用多，10来个字节就行） 1.3 挨个用公钥加密所有文件   2.提示用户文件加密了，要求给钱 3.如果收到钱了就给他一个解密的代码 4.解密代码这样写  4.1 遍历本地所有文件 4.2 判断是否有特殊标记 4.3 如果有，则是加密文件 4.4 用私钥去解开这个文件   当然真实的勒索病毒会更加严谨，我只是描述一下思路 我也从来没写过  中了勒索病毒怎么办？  不差钱方案：给钱，然后寄希望于对方的人品。 运气好方案：这是个常见的普通勒索病毒，网上有很多的工具可以尝试解一下 报警：造成重大损失的可以公开报警，交给安全部门处理，当然这个破案的难度有点大，数据可能还是找不回来 补救方案：用备份来救命。  如果有备份，可以恢复文件，那这时候就基本上可以依靠本身的备份体系来恢复大部分损失（还是会有不可挽回的损失）    勒索病毒和备份体系的攻防  聪明的勒索病毒会攻击备份体系 1.本机备份：中了勒索病毒以后，本机备份几乎是99%也会中毒，几乎没啥用了 2.异机备份：如果是个人电脑中毒，很难会感染到备份机，但是如果是机房里的服务器中毒了，那么病毒极有可能会感染备份机。 3.异机房备份：同异机备份，主要还是一个服务器内网环境。 如何防止勒索病毒攻击备份体系呢？ 这就是我们接下来下说的孤岛备份机方案  孤岛备份机 什么是孤岛备份机？  它是一个特殊的备份机 1.它不和普通的服务器连网 2.本地不开任何端口，任何其他服务器不能请求它的任何服务 3.只和指定的一台机器直连（通常这台机器是个普通的备份机） 4.它只以视为“这台普通备份机”的备份机 5.它会定时拉取普通备份机上的指定目录 6.</description>
    </item>
    
    <item>
      <title>MacOS里的grep,sed,awk等命令不好用怎么办</title>
      <link>/ops/macos%E9%87%8C%E7%9A%84grepawksed%E7%AD%89%E5%91%BD%E4%BB%A4%E4%B8%8D%E5%A5%BD%E7%94%A8%E6%80%8E%E4%B9%88%E5%8A%9E/</link>
      <pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/macos%E9%87%8C%E7%9A%84grepawksed%E7%AD%89%E5%91%BD%E4%BB%A4%E4%B8%8D%E5%A5%BD%E7%94%A8%E6%80%8E%E4%B9%88%E5%8A%9E/</guid>
      <description>问题  工作用的电脑是mac 经常发现linux上常用的sed,grep命令用不起来，各种报错 后来查了一下发现mac里的sed和linux的不是一个版本的软件   Mac OS X uses BSD sed and not GNU sed. When you use a GNU sed extension with Mac OS X sed, you get different results, or failures. Classically, sed does not support numeric offsets, forwards or backwards. You&amp;rsquo;ll need to revise your script to work on Mac OS X.
 解决 使用以下命令安装GNU命令套件： brew install coreutils 使用以下命令安装gnu-sed： brew install gnu-sed --with-default-names 以上命令安装的gnu套件的命令都是带有g前缀的  gcat &amp;ndash;&amp;gt; linux里的cat gsed &amp;ndash;&amp;gt; linux里的sed ggrep &amp;ndash;&amp;gt; linux里的grep gawk &amp;ndash;&amp;gt; linux里的awk &amp;hellip;  如果想直接代替，不输入g前缀则 vim .</description>
    </item>
    
    <item>
      <title>常用脚本_grep,sed,awk,find命令</title>
      <link>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_grepsedawkfind%E5%91%BD%E4%BB%A4/</link>
      <pubDate>Wed, 07 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_grepsedawkfind%E5%91%BD%E4%BB%A4/</guid>
      <description>grep ##zcat 是用于查看压缩的文件 ##gzip 套件包含许多可以 “在原地” 处理压缩文件的实用程序。zcat、zgrep、zless、zdiff 等实用程序的作用分别与 cat、grep、less 和 diff 相同，但是它们操作压缩的文件。 zcat web.log.gz | grep aqzt.com | head ###Grep &#39;OR&#39; 或操作 grep &amp;quot;pattern1\|pattern2&amp;quot; file.txt grep -E &amp;quot;pattern1|pattern2&amp;quot; file.txt grep -e pattern1 -e pattern2 file.txt egrep &amp;quot;pattern1|pattern2&amp;quot; file.txt awk &#39;/pattern1|pattern2/&#39; file.txt sed -e &#39;/pattern1/b&#39; -e &#39;/pattern2/b&#39; -e d file.txt #找出文件（filename）中包含123或者包含abc的行 grep -E &#39;123|abc&#39; filename #用egrep同样可以实现 egrep &#39;123|abc&#39; filename #awk 的实现方式 awk &#39;/123|abc/&#39; filename ###Grep &#39;AND&#39; 与操作 grep -E &#39;pattern1.*pattern2&#39; file.txt # in that order grep -E &#39;pattern1.</description>
    </item>
    
    <item>
      <title>常用脚本_scp和rsync</title>
      <link>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_scp%E5%92%8Crsync/</link>
      <pubDate>Sat, 15 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_scp%E5%92%8Crsync/</guid>
      <description>scp 从本地复制到远程 在本地服务器上将/data/mysqlbackup目录下所有的文件传输到服务器10.0.0.2的/data/mysqlbackup目录下，命令为：
scp -r /data/mysqlbackup root@10.0.0.2:/data/mysqlbackup
从远程复制到本地 在本地服务器上操作，将服务器10.0.0.2上/data/mysqlbackup/目录下所有的文件全部复制到本地的/root目录下，命令为： scp -r root@10.0.0.2:/data/mysqlbackup /root
rsync  time=$(date &#39;+%Y-%m-%d-%H-test1_binlog.0001&#39;) time1=$(date &#39;+%Y-%m-%d-%H-test2_binlog.0001&#39;) #cp /data/mysqlbackup/log/test1_binlog.0001 /data/mysqlbackup/log/$time #cp /data/mysqlbackup/log/test2_binlog.0001 /data/mysqlbackup/log/$time1 /usr/bin/rsync -av --bwlimit=10000 /data/mysqlbackup/log/test1_binlog.0001 /data/mysqlbackup/log/$time /usr/bin/rsync -av --bwlimit=10000 /data/mysqlbackup/log/test2_binlog.0001 /data/mysqlbackup/log/$time1 cat /dev/null &amp;gt; /data/mysqlbackup/log/test1_binlog.0001 cat /dev/null &amp;gt; /data/mysqlbackup/log/test2_binlog.0001 find /data/mysqlbackup/log/ -ctime +5 -exec rm -f {} \; ##剪切参考，IO速度限制的cp和mv(限速1024 KB/s) ##cp: rsync --bwlimit=1024 {src} {dest} ##mv: rsync --bwlimit=1024 --remove-source-files {src} {dest} ##使用ssh方式rsync，不用服务端，简单方便，SSH需要认证，就不用每次输入密码 #ssh-keygen -t rsa -N &#39;&#39; -f ~/.</description>
    </item>
    
    <item>
      <title>常用脚本_ssh无密码认证(linux互信)</title>
      <link>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_ssh%E6%97%A0%E5%AF%86%E7%A0%81%E8%AE%A4%E8%AF%81/</link>
      <pubDate>Fri, 14 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_ssh%E6%97%A0%E5%AF%86%E7%A0%81%E8%AE%A4%E8%AF%81/</guid>
      <description>A机添加B机ssh信任 # B机执行 ssh-keygen -t rsa 一路回车 cat /root/.ssh/idrsa.pub # A机执行 vim /root/.ssh/authorized_keys 将B机的pub信息加入 ssh ssh-keygen -t rsa -N &amp;quot;&amp;quot; -f ~/.ssh/id_rsa -q -b 2048 -C &amp;quot;test@ppabc.cn&amp;quot; ssh无密码认证 RSA ssh-keygen -t rsa cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys ssh无密码认证 DSA ssh-keygen -t dsa -P &#39;&#39; -f ~/.ssh/id_dsa cat ~/.ssh/id_dsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys chmod 600 ~/.ssh/authorized_keys 查看生产的密匙 cat ~/.ssh/id_dsa
用ssh -v 显示详细的登陆信息查找原因： ssh -v localhost 拷贝本地生产的key到远程服务器端（两种方法）  方法1  cat ~/.ssh/id_dsa.pub | ssh 远程用户名@远程服务器ip &#39;cat - &amp;gt;&amp;gt; ~/.</description>
    </item>
    
    <item>
      <title>常用脚本_iptables</title>
      <link>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_iptables/</link>
      <pubDate>Fri, 07 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_iptables/</guid>
      <description>安装和启动 systemctl status iptables yum install iptables-services systemctl start iptables #启动 systemctl status iptables #查看运行状态 systemctl restart iptables.service #重启 systemctl stop iptables.service #停止 systemctl enable iptables.service #设置开机启动 systemctl disable iptables.service #禁止开机启动 常用命令 iptables -h #查询帮助 iptables -L -n #列出（filter表）所有规则 iptables -L -n --line-number #列出（filter表）所有规则，带编号 iptables -L -n -t nat #列出（nat表）所有规则 iptables -F #清除（filter表）中所有规则 iptables -F -t nat #清除（nat表）中所有规则 service iptables save #保存配置（保存配置后必须重启iptables） systemctl restart iptables.service #重启 查看： iptables -L -n --line-number iptables -t nat -A PREROUTING -p tcp -d 127.</description>
    </item>
    
    <item>
      <title>常用脚本_date和time</title>
      <link>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_time/</link>
      <pubDate>Thu, 06 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_time/</guid>
      <description>日期加减  ####昨天日期（1天以前） date -d last-day +%Y-%m-%d date -d &amp;quot;1 days ago&amp;quot; +%Y-%m-%d date -d &#39;-1 days&#39; +%Y-%m-%d ####下周一日期 date -d &#39;next monday&#39; +%Y-%m-%d ####明天日期 date -d next-day +%Y-%m-%d date -d &#39;1 days&#39; +%Y-%m-%d ####前天 （2天以前） ‘n days ago&#39; 表示n天前的那一天 date -d &amp;quot;2 days ago&amp;quot; +%Y-%m-%d ####上个月的今天 date -d last-month +%Y-%m-%d ####下个月的今天 date -d next-month +%Y-%m-%d ###当前时间 date +%Y-%m-%d_%H_%M_%S ###注意 : 当你不希望出现无意义的 0 时(比如说 1999/03/07)，则可以在标记中插入 – 符号 ###比如说 date +%-H:%-M:%-S 会把时分秒中无意义的 0 给去掉，像是原本的 08:09:04 会变为 8:9:4。 date +%Y%-m%-d ##201766 日期方面 : %a : 星期几 (Sun.</description>
    </item>
    
    <item>
      <title>常用脚本_python</title>
      <link>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_python/</link>
      <pubDate>Wed, 05 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_python/</guid>
      <description>python安装 yum install -y gcc zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel yum install -y libffi-devel cd /opt/ wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tar.xz tar -xvJf Python-3.7.0.tar.xz mkdir -p /usr/local/python3 cd Python-3.7.4 ./configure --prefix=/usr/local/python3 make &amp;amp;&amp;amp; make install ln -s /usr/local/python3/bin/python3 /usr/local/bin/python3 ln -s /usr/local/python3/bin/pip3 /usr/local/bin/pip3 python3 -V pip3 -V </description>
    </item>
    
    <item>
      <title>常用脚本_linux性能</title>
      <link>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E6%80%A7%E8%83%BD%E6%9F%A5%E7%9C%8B/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E6%80%A7%E8%83%BD%E6%9F%A5%E7%9C%8B/</guid>
      <description>System_info wget https://www.dboop.com/download/System_info.sh &amp;amp;&amp;amp; sh System_info.sh cpu wget https://www.dboop.com/download/cpu.sh &amp;amp;&amp;amp; sh cpu.sh disk wget https://www.dboop.com/download/disk.sh &amp;amp;&amp;amp; sh disk.sh iostat  wget https://www.dboop.com/download/iostat.sh &amp;amp;&amp;amp; sh iostat.sh loadavg  wget https://www.dboop.com/download/loadavg.sh &amp;amp;&amp;amp; sh loadavg.sh memory wget https://www.dboop.com/download/memory.sh &amp;amp;&amp;amp; sh memory.sh show_disks  wget https://www.dboop.com/download/show_disks.sh &amp;amp;&amp;amp; sh show_disks.sh swap  wget https://www.dboop.com/download/swap.sh &amp;amp;&amp;amp; sh swap.sh vmstat wget https://www.dboop.com/download/vmstat.sh &amp;amp;&amp;amp; sh vmstat.sh 内存 # 查内存占用情况 ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; # 其中rsz是是实际内存 ps -e -o &#39;pid,comm,args,pcpu,rsz,vsz,stime,user,uid&#39; | grep java | sort -nrk5 # 其中rsz为实际内存，上例实现按内存排序，由大到小 # 查内存占用情况 ps -aux | sort -k4nr | head -n 10 # 使用指令查看占用的物理内存， ps aux|awk &#39;{sum+=$6} END {print sum/1024}&#39; # 使用指令，核实进程的最大使用内存量 ps -eo pid,rss,pmem,pcpu,vsz,args --sort=rss # 查内存命令 ps p 916 -L -o pcpu,pmem,pid,tid,time,tname,cmd # 排查高CPU占用介绍的PS命令 ps -mp 9004 -o THREAD,tid,time,rss,size,%mem # 分析具体的对象数目和占用内存大小 jmap -histo:live [pid] # 利用MAT工具分析是否存在内存泄漏等等。 jmap -dump:live,format=b,file=xxx.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_AlterTable</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_altertable/</link>
      <pubDate>Mon, 12 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_altertable/</guid>
      <description>速记  当修改表结构的时候有三种选项: ALTER TABLE t1 ALTER COLUMN ... ALTER TABLE t1 CHANGE COLUMN ... ALTER TABLE t1 MODIFY COLUMN ... 容易记混 一般我们这样记 Change column 无所不能 Modify column 不能给列改名,其他都行 Alter Column 最弱,改点默认值什么的(但是他也有好处,非常快和安全)  增  增加列(单列)  ALTER TABLE t1 ADD col-name col-type comment &#39;xxx&#39;;  增加列(多列)  ALTER TABLE t1 ADD col-name col-type comment &#39;xxx&#39;, ADD col-name col-type(col-length) comment &#39;xxx&#39;;  增加表字段并指明字段放置为第一列  ALTER TABLE t1 add col-name col-type COMMENT &#39;sss&#39; FIRST;  增加表字段并指明字段放置为特定列后面  ALTER TABLE t1 add col-name col-type after col-name-1; 删  删除列  ALTER TABLE t1 DROP col-name;  删除表中主键  Alter TABLE t1】 drop primary key 改  修改字段类型  - 使用MODIFY修改字段类型 ALTER TABLE t1 modify column col-name col-type; - 使用CHANGE修改字段类型 ALTER TABLE t1 change col-name col-name col-type;  修改列名  使用CHANGE修改字段名称 ALTER TABLE t1 change old-col-name new-col-name col-type;  修改表名  - 重命名表1 ALTER TABLE t1 RENAME 【表新名字】 - 重命名表2 RENAME TABLE t1 to new-table-name;  修改默认值  - 为字段设置NULL和DEFAULT ALTER TABLE t1 modify col-name col-type not null default 100; - 修改字段的默认值 ALTER TABLE t1 alter col-name set default 10000; -字段删除默认值 ALTER TABLE t1 alter col-name drop default; </description>
    </item>
    
    <item>
      <title>MySQL常用脚本_binlog解析和回滚</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_binlog/</link>
      <pubDate>Sat, 10 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_binlog/</guid>
      <description>解析binlog 方法1:mysqlbinlog mysqlbinlog --no-defaults --base64-output=decode-rows -vv --database=dboop --start-datetime=&#39;2020-10-11 00:00:00&#39; --stop-datetime=&#39;2020-10-11 15:00:00&#39; mysql-bin.000075 &amp;gt;75.sql 常用参数 database：只列出该数据库下的binlog数据，但无法过滤由触发器执行的SQL。 base64-output=decode-rows -vv：显示具体 SQL 语句。 skip-gtids=true：忽略 GTID 显示。  输出结果  # at 20001 #201011 12:04:09 server id 1 end_log_pos 20094 CRC32 0x2b305ac Query thread_id=53 exec_time=0 error_code=0 SET TIMESTAMP=1651011012/*!*/; BEGIN /*!*/; 上面输出包括信息：
 position: 位于文件中的位置，即第一行的（# at 20001）,说明该事件记录从文件第20001个字节开始 timestamp: 事件发生的时间戳，即第二行的（#201011 12:04:09） server id: 服务器标识（1） end_log_pos 表示下一个事件开始的位置（即当前事件的结束位置+1） thread_id: 执行该事件的线程id （thread_id=53） exec_time: 事件执行的花费时间 error_code: 错误码，0意味着没有发生错误 type:事件类型Query  方法2:my2sql wget https://www.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_故障定位</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E6%95%85%E9%9A%9C%E5%AE%9A%E4%BD%8D/</guid>
      <description>当前正在运行的SQL select id,user,db,info,Command,Time,State from information_schema.processlist where info is not null and user not in (&#39;dba&#39;,&#39;repl&#39;) order by time desc limit 50; -- 杀连接 select concat(&#39;kill &#39;,id,&#39;;&#39;) as ids from information_schema.processlist where time&amp;gt;50 and info is not null and user like &#39;%&#39; and db like &#39;%&#39; order by time desc ; -- 杀连接shell mysqlw -h 127.0.0.1 -P 3306 -e &amp;quot;select concat(&#39;kill &#39;,id,&#39;;&#39;) as ids from information_schema.processlist where db like &#39;dboop%&#39; and user like &#39;%&#39; &amp;quot; &amp;gt;&amp;gt;3306kill.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_mysqldump</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_mysqldump/</link>
      <pubDate>Sun, 04 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_mysqldump/</guid>
      <description>如果用loginpath的可以用
  mysqldump --login-path=dba 代替 mysqldump -uroot -p123456 常用命令 备份整个实例(dump全实例) mysqldump -uroot -p123456 -h127.0.0.1 -P3306 --single-transaction --column-statistics=0 --skip_add_locks --skip-lock-tables --master-data=2 -A | gzip &amp;gt; /data/mysqlbackup/dboop_dump`date &#39;+%m-%d-%Y&#39;`.sql.gz 备份实例中的用户库(用于实例迁移或升级) mysql -uroot -p123456 -h127.0.0.1 -P3306 -e &amp;quot;show databases&amp;quot; |grep -Ev &amp;quot;Database|information_schema|mysql|performance_schema&amp;quot; | xargs mysqldump -uroot -p123456 -h127.0.0.1 -P3306 --single-transaction --column-statistics=0 --skip_add_locks --skip-lock-tables --master-data=2 --databases &amp;gt; /data/mysqlbackup/dboop_dump0401.sql -- 此时mysql.user用户也没有迁移过来,如果需要迁移用户,参考: https://www.dboop.com/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E7%94%A8%E6%88%B7%E7%9B%B8%E5%85%B3/ 导出db1、db2两个数据库的所有数据 mysqldump -uroot -p123456 --set-gtid-purged=OFF --skip_add_locks --skip-lock-tables --databases db1 db2 &amp;gt; /data/mysqlbackup/dboop_dump0401.sql 导出db1中的a1、a2表 mysqldump -uroot -p123456 --set-gtid-purged=OFF --skip_add_locks --skip-lock-tables --databases db1 --tables a1 a2 &amp;gt; /data/mysqlbackup/dboop_dump0401.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_用户相关</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E7%94%A8%E6%88%B7%E7%9B%B8%E5%85%B3/</link>
      <pubDate>Fri, 02 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E7%94%A8%E6%88%B7%E7%9B%B8%E5%85%B3/</guid>
      <description>生成随机密码 select substring(md5(rand()), 1, 15); select left(replace(uuid(), &#39;-&#39;, &#39;.&#39;),15); 创建用户及赋权 MySQL5.6及以前 grant select on 库名.* to `用户名`@`主机名` identified by &#39;密码&#39;; MySQL5.7+ create user `用户名`@`主机名` identified by &#39;密码&#39;; grant select on 库名.* to `用户名`@`主机名`; MySQL8.0 create user `用户名`@`主机名`identified with mysql_native_password by &#39;密码&#39;; GRANT select on 库名.* TO `用户名`@`主机名`; 常用语句 -- 创建一个管理员帐号 create user &#39;dba&#39;@&#39;%&#39; IDENTIFIED BY &#39;********&#39;; GRANT ALL PRIVILEGES ON *.* TO &#39;dba&#39;@&#39;%&#39; WITH GRANT OPTION; -- 创建一个复制帐号 create user repl@&#39;%&#39; identified with mysql_native_password by &#39;********&#39;; GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.</description>
    </item>
    
    <item>
      <title>MySQL常用脚本_免密登录login-path设置</title>
      <link>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95loginpath/</link>
      <pubDate>Thu, 01 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%B8%B8%E7%94%A8%E8%84%9A%E6%9C%AC_%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95loginpath/</guid>
      <description>保存账号信息 mysql_config_editor set --login-path=dba --user=root --host=127.0.0.1 --password 其中可配置项
-h,–host=name 添加host到登陆文件中 -G,–login-path=name 在登录文件中为login path添加名字（默认为client） -p,–password 在登陆文件中添加密码（该密码会被mysql_config_editor自动加密） -u,–user 添加用户名到登陆文件中 -S,–socket=name 添加sock文件路径到登陆文件中 -P,–port=name 添加登陆端口到登陆文件中 查看配置 mysql_config_editor print -all 删除配置
mysql_config_editor remove --login-path=dba 登陆数据库
mysql --login-path=dba </description>
    </item>
    
    <item>
      <title>git常用命令整理</title>
      <link>/ops/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/</link>
      <pubDate>Thu, 25 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/git%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%95%B4%E7%90%86/</guid>
      <description>重新初始化，删掉历史版本记录  1、创建并切换到lastest_branch分支 git checkout --orphan latest_branch 2、添加所有文件 git add -A 3、提交更改 git commit -am &amp;quot;删除历史版本记录，初始化仓库&amp;quot; 4、删除分支 git branch -D master 5、将当前分支重命名 git branch -m master 6、强制更新存储库 git push -f origin master 新建 Git 仓库 # 把当前目录变更成一个 Git 仓库 $ git init # 新建一个目录，将其初始化为 Git 仓库 $ git init [project-name] # 克隆远程仓库 $ git clone [url] Git 配置信息 # 显示当前的 Git 配置 $ git config --list # 编辑 Git 配置文件 $ git config -e [--global] # 设置提交代码时的用户信息 $ git config [--global] user.</description>
    </item>
    
    <item>
      <title>Linux服务器共享目录Centos7</title>
      <link>/ops/linux%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB/</link>
      <pubDate>Sun, 21 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/ops/linux%E6%96%87%E4%BB%B6%E5%85%B1%E4%BA%AB/</guid>
      <description>0.环境 服务器：
 10.10.0.1 源机器 10.10.0.2 目标机器1 10.10.0.3 目标机器2 目标： 将10.10.0.1机器上的 /public/downloadnew 文件夹共享给10.10.0.2/3两台机器  三台机器共享写入 /data/www/dboop/static/download目录
1.安装nfs yum -y install nfs-utils rpcbind # 开机启动 systemctl enable rpcbind.service systemctl enable nfs-server.service # 重启服务 systemctl restart rpcbind.service systemctl restart nfs-server.service 2.共享设置 10.10.0.1 上执行 mkdir /public/downloadnew ln -s /public/downloadnew /data/www/dboop/static/download -f vim /etc/exports #输入 /public 10.10.0.2(insecure,rw,sync,no_root_squash) 10.10.0.3(insecure,rw,sync,no_root_squash) exportfs -rv 10.10.0.2/3 上执行 vim /etc/fstab #输入 10.10.0.1:/public /mnt/public nfs defaults 0 0 mkdir /mnt/public mount -a df -h ln -s /mnt/public/downloadnew /data/www/dboop/static/download -f 到此 10.</description>
    </item>
    
    <item>
      <title>MySQL的锁:innodb锁粒度详解</title>
      <link>/mysql/mysql%E9%94%81_innodb%E9%94%81%E7%B2%92%E5%BA%A6/</link>
      <pubDate>Sat, 13 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E9%94%81_innodb%E9%94%81%E7%B2%92%E5%BA%A6/</guid>
      <description>锁定义 lock_rec_not_gap锁  Record Locks
 A record lock is a lock on an index record. For example, SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; prevents any other transaction from inserting, updating, or deleting rows where the value of t.c1 is 10. Record locks always lock index records, even if a table is defined with no indexes. For such cases, InnoDB creates a hidden clustered index and uses this index for record locking.</description>
    </item>
    
    <item>
      <title>2020年终总结</title>
      <link>/book/2020%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 09 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/book/2020%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/</guid>
      <description> 农历腊月二十八 同事们走了大半 平时热闹的办公室里 安静了不少 用同一个姿势靠在椅背上 盯着电脑打着字 可能也没什么活吧 祈祷我们的DB不要再出故障了 每到快放假的时候 故障总是特别的多 希望今年最后这两天能消停点 今年过得非常难受 想提起情绪 总是会被无情的打落 岁末年关 回首这一年 哎，总算是过去了  疫情  疫情还是在反复 看起来像是快被控制住了样子 比起去年的长时间封在家里不出门 今年好歹是能正常上班了 虽然偶尔也传出来几个阳的 好在控制得及时 算是见到阳光了 疫情深深的伤了我们的行业 动了民宿行业的根本 今年我们公司几乎是倒闭了 被去哪儿网合并了 我也跟着被合并到去哪儿网的团队 新团队的工作压力轻了 但是焦虑感反而更强了 会经常想 这样下去旅游行业还能不能行了 去哪儿是不是也快撑不下去了 上周开了年会 又多了点信心 应该还能撑两年吧 疫情眼瞅着要过去了 行业复苏在即  意外  比起疫情 今年的意外更加伤人 从2020年8月9日9:13分的一个陌生人电话开始 老张因为一起意外 在医院里躺了好几个月 几乎是我难熬的一段时间 尤其是一开始的那段时间 守在icu门口 不知道该何去何从 老张能不能活着从里面出来 意外是怎么发生的 后面该找什么人能帮上忙 白天我在各处跑着 办各种奇怪的手续 和亲戚朋友同学们联系着 晚上会一个人坐在icu病房门口 也没什么事 也帮不上什么忙 就在那坐到很晚才回附近的酒店 像个神经病一样孤独而无助 后来老张出了icu 进了普通病房 又开始爆炸的忙 从来没想过照顾一个病人需要有这么多的事 到最后准备回北京的时候 看着镜子里的自己 觉得憔悴到有点枯萎了 这是我最艰难的一段时间 好在老张是挺了过来 2021年1月11日10:34 老张出院回家 开始新的恢复阶段 这段时间对老张，我，姐 对我们这个一家人来说 是非常非常不容易的 感谢老张的坚强 感谢家里的亲戚朋友们的无私帮忙 当然感谢在医院里的高中同学 让我在那段时间里 能知道下一步怎么办 还要感谢从远方赶回来帮忙的一位老友 提供了额外的医院里的信息支撑 那段时间里 是我赖以依靠的力量 越是在困难的时候 身边有可以信任和提供帮助的人 越能体会到温暖 这一年过得很艰难 但是这一年也过得很温暖 当这一切都已过去 那些支撑老张和我的人 留在记忆中很深刻的一笔 这一年，你们很重要  </description>
    </item>
    
    <item>
      <title>MySQL binlog 问答</title>
      <link>/dba/mysqlbinlogquestion/</link>
      <pubDate>Fri, 15 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/dba/mysqlbinlogquestion/</guid>
      <description>binlog 是什么?  MySQL 的二进制日志 ,不是纯文本类的 记录的是数据库变动的日志(insert,update,delete,create,replace,grant &amp;hellip;.) 不包括 select,set 等  binlog 重要吗？是不是一定要开？  几乎是最重要的MySQL日志，严谨点说是最重要的之一 如果没有特殊情况，一定要开！为什么？你再往后看&amp;hellip;)  binlog 有什么作用？  高可用同步，经常用它来同步主库和从库的数据。 它本身就是记录了数据库变化的日志，放在那让你看也是它作为“日志”的作用 恢复指定时间点的数据, 想把数据库恢复到指定时间眯，得靠它 回滚数据 ，误删除数据时用到 审计 变更捕获到其他平台(kafka,es或其他数据库)  binlog 怎么开启，放在哪，怎么存储的？ binlog组提交  MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数 引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程： flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）； sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）； commit 阶段：各个事务按顺序做 InnoDB commit 操作； 上面的每个阶段都有一个队列，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。 对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率。  主从复制是怎么实现？ MySQL 集群的主从复制过程梳理成 3 个阶段：  写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引擎中的数据。  具体详细过程如下：  MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。  24、什么时候 binlog cache 会写到 binlog 文件？ 在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。</description>
    </item>
    
    <item>
      <title>MySQL的innodb中Next-Key锁的解析</title>
      <link>/dba/innodb_lock_2020/</link>
      <pubDate>Tue, 05 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/dba/innodb_lock_2020/</guid>
      <description>去年的某个时候，一个朋友在微信上问我MySQL间隙锁的案例，当时正在赶一个项目，没来得及看那个CASE，后来找不到了。昨天看到这篇jahfer写的博客: https://jahfer.com/posts/innodb-locks/ 觉得在介绍Next-Key锁的这方面很有创意的使用了自制的动画（非常简陋的动画 没啥用，我换成了截图做标记了)，不管是创意还是内容都值得一看
   作者:jahfer 翻译:51ak   &amp;ndash;翻译全文如下：
最近，我在调试MySQL高并发问题时有机会深入理解了InnoDB的锁定机制，这篇文章是我学习innodb锁行为的一个总结。
0.概念介绍 InnoDB只有几个锁定概念,但是它们的使用和行为取决于当前连接正在使用的事务隔离级别
 …the isolation level is the setting that fine-tunes the balance between performance and reliability, consistency, and reproducibility of results when multiple transactions are making changes and performing queries at the same time. 引自MySQL官方文档 https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html
 InnoDB一共有4种隔离级别（按最严格到最宽松的顺序）
 SERIALIZABLE 序列化 REPEATABLE READ (default) 可重复读 READ COMMITTED 读已提交 READ UNCOMMITTED 读未提交  每种隔离级别下的锁行为差异非常大，而我们现在只分析前两种隔离级别（SERIALIZABLE，REPEATABLE READ),首先让我们创建一个book 表。</description>
    </item>
    
    <item>
      <title>数据库团队DBA的OKR和KPI指标</title>
      <link>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9B%A2%E9%98%9Fdba%E7%9A%84okr%E5%92%8Ckpi%E6%8C%87%E6%A0%87/</link>
      <pubDate>Thu, 10 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/dba/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9B%A2%E9%98%9Fdba%E7%9A%84okr%E5%92%8Ckpi%E6%8C%87%E6%A0%87/</guid>
      <description>数据库团队的OKR（目标关键结果）可能包括：  数据存储容量优化：通过数据库管理和数据存储优化技术，提高数据存储的效率和容量。 数据安全性提升：通过数据库安全技术和安全控制，保护数据的安全性。 数据恢复能力提高：通过数据备份和恢复技术，提高数据恢复的效率和能力。 数据库性能优化：通过数据库性能调优和技术优化，提高数据库的性能。 数据查询效率提高：通过数据查询优化技术，提高数据查询的效率。 数据可视化开发：通过数据可视化技术，提高数据的可读性和可视性。 数据分析能力提高：通过数据分析技术，提高数据分析的效率和能力。 数据处理效率提高：通过数据处理优化技术，提高数据处理的效率。 数据交换效率提高：通过数据交换优化技术，提高数据交换的效率。 数据管理流程改进：通过对数据管理流程的优化和改进，提高数据管理的效率和质量。 团队技能培训：通过为团队提供技能培训，提高团队的专业水平和能力。 协同工作效率提高：通过改进团队间的协同工作，提高团队的协同工作效率。  这些OKR可能根据公司的具体情况进行调整和定制，以适应公司的特殊需求。
数据库团队有哪些可量化的KPI指标 数据库团队可以使用以下可量化的指标来评估其进展和成果：
 数据库响应时间：测量数据库对请求的响应时间，以评估数据库的性能。 数据库可用性：通过计算数据库在一段时间内的可用率，评估数据库的可用性。 数据库容量：通过监控数据库的容量使用情况，评估数据库的容量需求。 数据完整性：通过测量数据的完整性，评估数据的质量。 数据备份和恢复：通过测量数据备份和恢复的时间和准确性，评估数据的安全性。 团队效率：通过测量团队完成任务的效率，评估团队的工作效率。 研发团队满意度：通过调查研发对数据库团队的满意程度，评估团队的服务质量。  这些指标可以帮助数据库团队评估其OKR的实现情况，并对团队的工作进行评估和改进。</description>
    </item>
    
    <item>
      <title>丐帮PK赛</title>
      <link>/dba/gaibangpk/</link>
      <pubDate>Mon, 07 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/dba/gaibangpk/</guid>
      <description>[ 丐帮pk赛：帮派里选出来最睿智的两个人，1vs1，互相出题，答题时长不限，方法不限,结果正确了，得1分，否则对方得一份。统计最终得分。 ]
当前得分 p:3 / k:8  第一轮.地铁站走楼梯问题 题目:从知春路地铁站走楼梯上来，共有30级台阶，阿怂每次上台阶要么是1层，要么是2层，问：有多少种方法阿怂可以爬完这30级台阶？
 出题人:P 答题人:K 答题时间:用时17分钟 答题结果:正确 正确答案:1346269 解答:每一层楼梯的方法不是他决定的,是他上一级台阶有几种方法和上上一级台阶有几种方法,加一块就是他有几种方法. 图解： 代码：https://github.com/51ak/golearn/blob/master/day11_%E7%88%AC%E6%A5%BC%E6%A2%AF/main.go   第二轮.病毒感染率问题 题目:假设新冠病毒检测covid-19的感染率为1%，而新冠病毒检测检测准确率为99%,如果阿怂做了一次检查，被通知结果为阳性，那么阿怂真正感染的概率是多少？
 出题人:K 答题人:P 答题时间:用时2分钟 答题结果:失败! 答成了99% 正确答案:50% 解答:假设有10000人，感染率为1%,那么其中有100为感染，9900位健康,其中100感染者里，有99位真感染，1人误诊，9900位健康者里，有99位因为误诊是感染的，9801位是健康的。于是：被告知感染的人里，有99位真正的感染者和99位健康的人。你感染了被诊断出来的几率是0.01*（1-0.01）=0.0099，假如你没感染被诊断出来的几率是0.99*0.01=0.0099所以实际上依然健康的概率为0.0099/（0.0099+0.0099）=0.5 贝叶斯理论   第三轮.收保护费问题 题目:阿怂要到一条街上收保护费，老大交待这次不能同时收相邻的两个捕子的保护费，收了第1号铺子的就不能收第2号，可以跳过去收3或者4号。假如从1号铺子开始每个铺子能收到的保护费分别是4, 3, 5, 4, 5, 23, 7, 8, 9, 7, 7, 8, 12, 15, 17, 13, 4, 9, 12, 17, 13, 5, 0, 7, 14, 7, 9, 8, 9, 30 元，问：阿怂最多能收到多少元钱？
 出题人:P 答题人:K 答题时间:用时18分钟 答题结果:166算错了一个数 正确答案:167 解答:当收到第N个铺子时，可以选择收和不收，  选择收，那么这个铺子时的最大金额就是N-2间铺子的最大金额+第N间铺子 选择不收，那么收到这个铺子时的最大金额就是第N-1间铺子的最大金额 所以在N号铺子时，怂哥最多能拿到的钱就是上面两个值取个最大值，把这个值人做个记号，然后接着去下个铺子做同样的事，结果就一路推出来了。 在第N号铺子，只需要回头看看N-2，N-1号铺子门口的最大金额标记和这个铺子的钱就知道这个铺子的记号。 图解： 代码：https://github.</description>
    </item>
    
    <item>
      <title>MySQL的drop/truncate Table影响分析和最佳实践</title>
      <link>/dba/droptable/</link>
      <pubDate>Thu, 26 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/dba/droptable/</guid>
      <description>0.前言 MySQL上直接Drop张大表,会有什么影响，能否直接写个 drop table ; 或者 truncate table ; 甚至是delete * from 如果这张表足够大，比如1亿行记录，drop 的时间需要多久，期间我的MySQL是否能正常访问？
首先明确一点，现在讨论的是要删掉的大表一定是没人访问的表，否则如果这张表仍然还有被高频的访问，你敢直接删那基本上就是茅坑里点灯，找死！ 如果MySQL版本是5.5.23以下，直接DROP一张大表，也是守着茅坑睡觉，离死不远。 好，现在明确了这张表肯定没人访问了，你的MySQL版本也足够新，并不表示你就远离了茅坑，但如果这张表足够大，仍然有被崩到的风险。
大表：我们定义为5000万行以上，或者单表文件大于100G
我们要讨论的是innodb存储引擎,myisam等存储引擎，DROP 表又快又安全
1.drop table 的风险和避免方法 Drop table 要做的主要有3件事：  把硬盘上的这个文件删了 把内存中的这个库已经加载加来的Page删了，腾出空间 把MySQL元数据字典中这张表关联信息删了  可能会引起的风险有3种：  MySQL长时间阻塞其他事务执行，大量请求堆积，实例假死。(锁) 磁盘IO被短时间大量占用，数据库性能明显下降(IO) 内存里的page大量置换，引起线程阻塞，实例假死（内存)  解决和避免的方法3种：  io占用的问题，对这个表建一个硬链，使Drop table 表的时候并没有真的去磁盘上删那个巨大的ibd文件，事后再用truncate的方式慢慢的删除这个文件，如果是SSD盘和卡,drop table后再直接rm文件也没问题 内存和IO占用的问题，升级MySQL版本   MySQL 5.5.23 引入了 lazy drop table 来优化改进了drop 操作影响(改进，改进，并没有说完全消除!!!拐杖敲黑板3次)
  MySQL5.7.8 拆分了AHI共用一个全局的锁结构 btr_search_latch
  MySQL8.0 解决了truncate table 的风险
   道路千万行，低峰第一条。选择低峰时间段，找个夜深人静，月黑风高的时候是更好的选择。  2.</description>
    </item>
    
    <item>
      <title>MySQL的行格式(Compact、Redundant、Dynamic和Compressed)</title>
      <link>/mysql/mysql%E7%9A%84%E8%A1%8C%E6%A0%BC%E5%BC%8Fcompact_redundant_dynamic_compressed/</link>
      <pubDate>Thu, 18 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E8%A1%8C%E6%A0%BC%E5%BC%8Fcompact_redundant_dynamic_compressed/</guid>
      <description>前言  MySQL的默认存储引擎innodb是按16k大小的page来组织存储数据的 MySQL的*.ibd 数据文件，大小一定是能被16kB整除的 在逻辑上innodb是按btree来组织数据存储的 针对每一行具体的数据，共有4种存储方式可供选择：Compact、Redundant、Dynamic和Compressed 其中：Redundant 已经被淘汰了，不建议使用 Compact/Dynamic/Compressed 用的是同一个原理，只在细节上有点变化，不影响其实现逻辑 所以我们说行格式的时候，就可以从compact格式来分析，后两种是compact格式的变种   以下原理部分，都只说compact行格式。(?因为compact是基础，后两种都是基于它衍生出来的)
 行格式在哪里看，怎么修改行格式 查看 mysql&amp;gt; show table status like &#39;%dbooptest%&#39; \G *************************** 1. row *************************** Name: dbooptest Engine: InnoDB Version: 10 Row_format: Dynamic Rows: 9 Avg_row_length: 1820 Data_length: 16384 Max_data_length: 0 Index_length: 0 Data_free: 0 Auto_increment: NULL Create_time: 2020-06-10 20:22:49 Update_time: 2020-06-10 20:22:49 Check_time: NULL Collation: utf8mb4_unicode_ci Checksum: NULL Create_options: Comment: 测试 1 row in set (0.</description>
    </item>
    
    <item>
      <title>MySQL的事务id:trx_id</title>
      <link>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1id_trx_id/</link>
      <pubDate>Sat, 02 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E7%9A%84%E4%BA%8B%E5%8A%A1id_trx_id/</guid>
      <description>什么是事务id(trx_id)  可以理解为MySQL官方存储引擎innodb维护的一个全局自增变量:max_trx_id, 一个6字节长度的整数。(max_trx_id如果一直增长，理论上也是有溢出的可能性的，超过2的48次方后，会重新从0开始，这时候会破坏事务的顺序规则) 每当一个事务开始时，需要申请一个新的trx_id值时，就获取max_trx_id的最新值，然后将max_trx_id值加1。  事务id的作用  主要是用来记录事务开始的顺序 会用在各种事务冲突和mvcc中  如何查看事务id 查看当前事务的trx_id  select TRX_ID from INFORMATION_SCHEMA.INNODB_TRX where TRX_MYSQL_THREAD_ID = CONNECTION_ID() 查看当前的事务id列表（活动)  select TRX_ID from INFORMATION_SCHEMA.INNODB_TRX 查看当前的事务id列表（活动+非活动) 看innodb status 的TRANSACTIONS 部分
show engine innodb status \G # 找到这一部分 TRANSACTIONS 部分 TRANSACTIONS Trx id counter 2419 -- 当前最大事务 ID Purge done for trx&#39;s n:o &amp;lt; 2419 undo n:o &amp;lt; 0 state: running but idle History list length 0 LIST OF TRANSACTIONS FOR EACH SESSION: ---TRANSACTION 421658589187480, not started 0 lock struct(s), heap size 1136, 0 row lock(s) ---TRANSACTION 421658589186624, not started 0 lock struct(s), heap size 1136, 0 row lock(s) ---TRANSACTION 421658589185768, not started 0 lock struct(s), heap size 1136, 0 row lock(s) 上面是我搭的测试环境，所以没有活跃事务， 需要注意的是几个事务id都非常大（例：421658589187480） 这个后面会解释说明</description>
    </item>
    
    <item>
      <title>MySQL原理_innodb存储格式详解(二)</title>
      <link>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A32/</link>
      <pubDate>Wed, 11 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A32/</guid>
      <description>MySQL用page页来管理和存储数据文件，这些page页是如何被组织起来的，真实的数据(data,index) 真实是怎么存放在ibd文件和内存中的呢，了解存储格式，将帮助我们更好的理解MySQL是如何工作的，从而更好的理解其他数据库知识点（索引，MVCC,等等），本视频中我将利用Python脚本把这些难以理解的数据页解析出来并图形化展示给大家，望大家知其然而知其所以然
视频较长，分两段录制，录制时音量较小，注意控制下声音大小。
  下面是分析.idb文件的脚本，执行结果的明细部分</description>
    </item>
    
    <item>
      <title>MySQL原理_innodb存储格式详解(一)</title>
      <link>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/mysql/mysql%E5%8E%9F%E7%90%86_innodb%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%E8%AF%A6%E8%A7%A3/</guid>
      <description>MySQL用page页来管理和存储数据文件，这些page页是如何被组织起来的，真实的数据(data,index) 真实是怎么存放在ibd文件和内存中的呢，了解存储格式，将帮助我们更好的理解MySQL是如何工作的，从而更好的理解其他数据库知识点（索引，MVCC,等等），本视频中我将利用Python脚本把这些难以理解的数据页解析出来并图形化展示给大家，望大家知其然而知其所以然
视频较长，分两段录制，录制时音量较小，注意控制下声音大小。
  核心问题：MySQL（*innodb)是如何组织，存储表数据的？
innodb单表最大能到多少，为什么？
为什么innodb数据文件的大小始终可以被16384整除？
int 和bigint 差别有多大?
varchar(10) 和varchar(100) 差别有多大？varchar(1000)呢? TEXT 呢?
1页(page)=基本单位，存储和读取的核心，每页大小默认：16k
1区(extend)=64页,64*16k=1M
1组(space)=256区, 256*1M=256M
每个page都有个编号，整型最大 2^32 x 16k = 64T （单表大小上限）
下面是分析.idb文件的脚本，执行结果截图 </description>
    </item>
    
    <item>
      <title>SQLServer联机重建或组织索引</title>
      <link>/sqlserver/sqlserver%E8%81%94%E6%9C%BA%E9%87%8D%E5%BB%BA%E6%88%96%E7%BB%84%E7%BB%87%E7%B4%A2%E5%BC%95/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/sqlserver/sqlserver%E8%81%94%E6%9C%BA%E9%87%8D%E5%BB%BA%E6%88%96%E7%BB%84%E7%BB%87%E7%B4%A2%E5%BC%95/</guid>
      <description>索引维护 联机重建或组织索引 CREATE procedure [pr_auto_indexdefrag_online] as begin set nocount on declare @Db_name nvarchar(256) ,@SchemaName nvarchar(256) ,@TableName Nvarchar(256) ,@IndexName Nvarchar(512) ,@PctFrag decimal ,@Defrag nvarchar(max) if exists(select 1 from sys.objects where object_id =object_id(N&#39;#tmp&#39;)) Drop table #tmp; if exists(select 1 from sys.objects where object_id =object_id(N&#39;#tmp_sub&#39;)) Drop table #tmp_sub; create table #tmp_sub(database_id int,dbname nvarchar(32),tablename nvarchar(128),index_type_desc nvarchar(128)) create table #tmp(database_id int,dbname nvarchar(256),tablename nvarchar(256),indexname nvarchar(256),type_desc nvarchar(128),schemaname nvarchar(256),avgfragment decimal) ------找出 text、ntext、image、varchar(max)、nvarchar(max)、varbinary(max)、xml 或大型 CLR 类型的列 exec sp_MSforeachdb &#39;insert into #tmp_sub(database_id,dbname,tablename,index_type_desc) select distinct c.</description>
    </item>
    
    <item>
      <title>SQLServer清除执行计划缓存</title>
      <link>/sqlserver/sqlserver%E6%B8%85%E9%99%A4%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E7%BC%93%E5%AD%98/</link>
      <pubDate>Wed, 05 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/sqlserver/sqlserver%E6%B8%85%E9%99%A4%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E7%BC%93%E5%AD%98/</guid>
      <description>清除执行计划缓存  DBCC FREEPROCCACHE --清除执行计划缓存 查看执行缓存 	select * from sys.dm_exec_cached_plans 如何利用好执行计划缓存    使用存储过程，或者 sp_executesql 的方式调用会被重复使用的语句，而不要直接用 ad-hoc 语句或者 dynamic SQL 。    在语句里引用对象（表、视图、存储过程等），到带上它的 schema 名字，而不光是对象自己的名字。    将 数据库 Parameterization 属性设置成 Forced   这个属性是开启数据库强制参数化。也就是说，对于在这个数据库下运行的大部分语句，SQL Server 都会先参数化，再运行。如果应用经常用 adhoc 的方式调用一样的语句，强制参数化可能会有所帮助    统计信息更新   统计信息手工或者自动更新后，对和它有关的执行计划都不再能重用，而会产生重编译。    Create Procedure &amp;hellip; with Recompile 选项 和 Exce &amp;hellip; with Recomplie 选项 在重建或者调用存储过程的时候使用 &amp;ldquo;with Recomplie&amp;rdquo;，会强制 Sql Server 在调用这个存储过程的时候，永远都要先编译，再运行。    用户使用了 sp_recomplie    用户在调用语句的时候，使用了 &amp;ldquo;Keep Plan&amp;rdquo; 或者 &amp;ldquo;KeepFixed Plan&amp;rdquo; 这样的查询提示    定时任务  注意对一些复杂的存储过程，定时清理一下（凌晨）  </description>
    </item>
    
    <item>
      <title>SQLServer索引相关的DMV</title>
      <link>/sqlserver/sqlserver%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E7%9A%84dmv/</link>
      <pubDate>Tue, 04 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/sqlserver/sqlserver%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%E7%9A%84dmv/</guid>
      <description>动态管理视图：   sys.dm_db_missing_index_details –返回关于缺失索引的详细信息。
  sys.dm_db_missing_index_group_stats - 返回缺失索引组的摘要信息
  sys.dm_db_missing_index_groups – 返回一个具体组的缺失索引的信息。
  sys.dm_db_missing_index_columns(index_handle) – 返回在一个索引中缺失的数据库表列的信息。这是一个函数，它要求传递index_handle。
  和大多数动态管理视图的跟踪统计数据一样，当SQL Server实例重启，这些数据被完全清除时，
  　1.被大量更新，却很少被使用的索引 SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED SELECT DB_NAME() AS DatabaseName , SCHEMA_NAME(o.Schema_ID) AS SchemaName , OBJECT_NAME(s.[object_id]) AS TableName , i.name AS IndexName , s.user_updates , s.system_seeks + s.system_scans + s.system_lookups AS [System usage] INTO #TempUnusedIndexes FROM sys.dm_db_index_usage_stats s INNER JOIN sys.indexes i ON s.</description>
    </item>
    
    <item>
      <title>Oracle新建A用户默认访问B用户的表和视图</title>
      <link>/oracle/oracle%E6%96%B0%E5%BB%BAa%E7%94%A8%E6%88%B7%E9%BB%98%E8%AE%A4%E8%AE%BF%E9%97%AEb%E7%94%A8%E6%88%B7%E7%9A%84%E8%A1%A8%E5%92%8C%E8%A7%86%E5%9B%BE/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/oracle/oracle%E6%96%B0%E5%BB%BAa%E7%94%A8%E6%88%B7%E9%BB%98%E8%AE%A4%E8%AE%BF%E9%97%AEb%E7%94%A8%E6%88%B7%E7%9A%84%E8%A1%A8%E5%92%8C%E8%A7%86%E5%9B%BE/</guid>
      <description>原因  在oracle 中schema与用户是一一对应的关系 A用户默认访问的是Aschema下的表 业务需求建立用户A可以查询B用户建立的表；会报错：表和视图不存在 以下是解决方法  方案一：改写SQL  A用户查询时带上B用户的schema  select * from A.table 方案二：手动指定current_schema  临时修改A用户的schema，执行下面SQL事件  alter session set current_schema=B; select * from table; 方案三：触发器  增加A用户的登录trigger  create or replace trigger {triggername} after logon on A.schema begin execute immediate &#39;alter session set current_schema=B&#39;; 方案四：增加同义词  增加A用户的同义词  # 用管理账号执行 grant create synonym to A # 用A账号执行 CREATE SYNONYM TEST FOR B.TEST; .... #把所有的表和视图都加上 总结  方法1.</description>
    </item>
    
    <item>
      <title>SQLServer的资源等待</title>
      <link>/sqlserver/sqlserver%E7%9A%84%E8%B5%84%E6%BA%90%E7%AD%89%E5%BE%85/</link>
      <pubDate>Mon, 03 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/sqlserver/sqlserver%E7%9A%84%E8%B5%84%E6%BA%90%E7%AD%89%E5%BE%85/</guid>
      <description>清除执行计划缓存 DBCC SQLPERF (&#39;sys.dm_os_wait_stats&#39;, CLEAR);  开始重新统计  统计SQL  select a.[RowNum] ,a.[WaitType] ,a.[Wait_S]-b.[Wait_S] as [Wait_S] ,a.[Resource_S]-b.[Resource_S] as [Resource_S] ,a.[Signal_S]-b.[Signal_S] as [Signal_S] ,a.[WaitCount]-b.[WaitCount] as [WaitCount] ,a.[Percentage]-b.[Percentage] as [Percentage] ,a.[AvgWait_S]-b.[AvgWait_S] as [AvgWait_S] ,a.[AvgRes_S]-b.[AvgRes_S] as [AvgRes_S] ,a.[AvgSig_S]-b.[AvgSig_S] as [AvgSig_S] from ( SELECT [RowNum] ,[WaitType] ,[Wait_S] ,[Resource_S] ,[Signal_S] ,[WaitCount] ,[Percentage] ,[AvgWait_S] ,[AvgRes_S] ,[AvgSig_S] FROM [system].[dbo].[dba_WaitType_log] where addtime=&#39;2015-03-26 17:04:04.683&#39; ) a left join ( SELECT [RowNum] ,[WaitType] ,[Wait_S] ,[Resource_S] ,[Signal_S] ,[WaitCount] ,[Percentage] ,[AvgWait_S] ,[AvgRes_S] ,[AvgSig_S] FROM [system].</description>
    </item>
    
    <item>
      <title>SQLServer移动ALWASYON副本文件的方法和脚本</title>
      <link>/sqlserver/sqlserver%E7%A7%BB%E5%8A%A8alwasyon%E5%89%AF%E6%9C%AC%E6%96%87%E4%BB%B6%E7%9A%84%E6%96%B9%E6%B3%95%E5%92%8C%E8%84%9A%E6%9C%AC/</link>
      <pubDate>Sun, 02 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/sqlserver/sqlserver%E7%A7%BB%E5%8A%A8alwasyon%E5%89%AF%E6%9C%AC%E6%96%87%E4%BB%B6%E7%9A%84%E6%96%B9%E6%B3%95%E5%92%8C%E8%84%9A%E6%9C%AC/</guid>
      <description>1.暂停ALWAYSON数据传送 ALTER DATABASE [db1] SET HADR SUSPEND; ALTER DATABASE [db2] SET HADR SUSPEND; ALTER DATABASE [db3] SET HADR SUSPEND; 2.生成脚本：  SELECT database_id, (sum(size)*8/1024/1024) as usedGb FROM sys.master_files WHERE database_id in ( select database_id from sys.databases where NAME NOT IN (&#39;....&#39;) ) AND physical_name LIKE &#39;E:\%&#39; group by database_id order by 2 desc 3.生成脚本2 SELECT name , physical_name AS CurrentLocation , state_desc,(size*8/1024/1024) as usedGb ,&#39;ALTER DATABASE [&#39;+DB_NAME(database_id)+&#39;] MODIFY FILE ( NAME = &#39;+name+&#39; , FILENAME = &#39;&#39;&#39;+REPLACE(physical_name,&#39;D:\&#39;,&#39;F:\&#39;)+&#39;&#39;&#39; )&#39; FROM sys.</description>
    </item>
    
    <item>
      <title>我收藏的一些安全(黑科技)硬件</title>
      <link>/book/%E6%88%91%E6%94%B6%E8%97%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%89%E5%85%A8%E7%A1%AC%E4%BB%B6/</link>
      <pubDate>Thu, 30 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/book/%E6%88%91%E6%94%B6%E8%97%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AE%89%E5%85%A8%E7%A1%AC%E4%BB%B6/</guid>
      <description>2020年春节 冠状病毒肆虐
憋在家里觉得是个机会有大把时间
重构一个以前自己写的一个项目代码
战斗的第二天
笔记本因为长时间不关机
硬盘烧了
无法补救的那种
虽然有备用电脑
但离开了个人熟悉的软件和环境
好多事情还是没法做
比如
每周四都会在公众号上连载的MySQL视频
就被迫中断了
昨晚我仰望星空
在想宇宙的奥秘和人类&amp;hellip;
以及软件和硬件的依赖
再牛的软件还是靠硬件支撑的
而我对硬件基本是白痴
日常工作中凡是硬件相关的
都尽量交给更合适的同事完成
只有个人电脑和数码产品
才会花精力去折腾
但作为一个硬件白痴
也收藏了一些有趣的硬件
以前周末会经常去中关村的鼎好，E世界
瞎逛
买了很多无用的小东西
类似于女人对口红的追求
迷恋板卡和黑科技
今天跟大家分享3个有趣的跟安全有关的硬件
提前声明
不可以学坏了，做坏事，否则后果自负
1.PM3 呃，这是git上一个很酷炫项目的硬件载体
华强北给焊出来了
众所周知街边配钥匙的会配一些小区门禁卡，电梯卡，员工卡,甚至一些加密卡
那么pm3能做的会更多
对应的卡片种类极大丰富
最重要的是破解加密卡，你知道插上usb连上电脑以后那个小卡片就是个裸着的
2.摩托罗拉c118 上古神器
2g网络的缺陷魔鬼被这个摩托罗拉这个低端手机给释放了
连上电脑，关联上附近的信号塔，开始短信嗅探
浓浓的黑科技感觉
当然现在是4G，5G时代了
3.大功率WiFi信号接收器 呃，这个是wifi信号定向（发送）接收器
但是因为wifi协议的缺陷
尤其是早期路由器的wps_pin 码漏洞
介绍下这个Pin漏洞（基本上几个小时一定会破解一个无线wifi密码）
有定向信号接收器外挂在室外（物理外挂）
周边1公里的wifi都能破
好在这几年pin漏洞都补上了
但抓包暴力依旧还在
Wifi密码照样不经打
Wifi信号放大器会增加这种能力的范围和强度
所以Wifi密码一定要很复杂
Wifi爆破成功就进内网（然后&amp;hellip;）
然后，
再说这个硬件
它不止是接收信号放大
也可以把发送信号放大
想想这又可以怎么搞事
发散一下思维
不可以学坏了，做坏事，否则后果自负
不可以学坏了，做坏事，否则后果自负
不可以学坏了，做坏事，否则后果自负
2020-1-30 晨 北京顺义</description>
    </item>
    
    <item>
      <title>windowsCluster集群脑裂问题最佳实践</title>
      <link>/sqlserver/windowscluster%E9%9B%86%E7%BE%A4%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/sqlserver/windowscluster%E9%9B%86%E7%BE%A4%E8%84%91%E8%A3%82%E9%97%AE%E9%A2%98%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</guid>
      <description>故障描述  WinSvr2008R2/Windows 故障转移集群 意外中断1分钟左右后自行恢复 导致SQLServer alwayson集群 中断访问，连接卡住无法提供访问  错误日志 从日志上来看，当时应该在这个时段节点dboopawo91和dboopawo92两台服务器发生了网络错误，我们看到在日志中显示，两台服务器互相联不通对方了，所以在他们的日志中显示，由于这些机器都无法联通，所以他们都被从群集中踢出。
------------------------------------------------------------------------------ 节点dboopawo91报由于网络问题联系上不dboopawo92。 --------------------- 11/06/2015 12:13:02 AM Critical dboopawo91.server.dboop.com 1135 Microsoft-Windows-FailoverClustering Node Mgr NT AUTHORITY\SYSTEM Cluster node &#39;dboopawo92&#39; was removed from the active failover cluster membership. The Cluster service on this node may have stopped. This could also be due to the node having lost communication with other active nodes in the failover cluster. Run the Validate a Configuration wizard to check your network configuration.</description>
    </item>
    
    <item>
      <title>快速用python,uv安装一个mcp服务</title>
      <link>/ops/%E5%BF%AB%E9%80%9F%E7%94%A8pythonuv%E5%AE%89%E8%A3%85%E4%B8%80%E4%B8%AAmcp%E6%9C%8D%E5%8A%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/ops/%E5%BF%AB%E9%80%9F%E7%94%A8pythonuv%E5%AE%89%E8%A3%85%E4%B8%80%E4%B8%AAmcp%E6%9C%8D%E5%8A%A1/</guid>
      <description>安装uv  sudo -s curl &amp;quot;https://dba.corp.shiqiao.com/static/download/install_uv.sh&amp;quot; -o install_uv.sh sh install_uv.sh ln -s /Users/kouko/.local/bin/* /usr/local/bin exit 配置uv环境  uv python install 3.12 uv venv --python 3.12 uv pip install mcp uv python pin 3.12 制作简单mcp服务  from mcp.server.fastmcp import FastMCP # 创建一个启用调试模式的 MCP 服务器 mcp = FastMCP(&amp;quot;Demo&amp;quot;, debug=True) # 定义加法工具 @mcp.tool() def add(a: int, b: int) -&amp;gt; int: &amp;quot;&amp;quot;&amp;quot;Add two numbers&amp;quot;&amp;quot;&amp;quot; return a + b # 运行服务器 if __name__ == &amp;quot;__main__&amp;quot;: print(&amp;quot;Starting MCP server in stdio mode&amp;quot;) mcp.</description>
    </item>
    
  </channel>
</rss>
